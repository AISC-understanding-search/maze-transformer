{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTConfig\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from muutils.logger import Logger, TimerContext\n",
    "from muutils.json_serialize import json_serialize, dataclass_serializer_factory\n",
    "from muutils.tensor_utils import ATensor, NDArray\n",
    "from muutils.statcounter import StatCounter\n",
    "from muutils.misc import shorten_numerical_to_str\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from maze_transformer.generation.latticemaze import LatticeMaze\n",
    "from maze_transformer.generation.generators import LatticeMazeGenerators\n",
    "from maze_transformer.training.tokenizer import MazeTokenizer, SPECIAL_TOKENS\n",
    "from maze_transformer.training.mazedataset import MazeDatasetConfig, MazeDataset\n",
    "from maze_transformer.evaluation.plot_maze import plot_multi_paths, PathFormat\n",
    "from maze_transformer.training.dataset import GPTDatasetConfig\n",
    "from maze_transformer.training.config import TrainConfig\n",
    "from maze_transformer.training.training import TRAIN_SAVE_FILES\n",
    "from maze_transformer.evaluation.eval_model import generate_plot_predicted_path, MazePath, load_model_with_configs, LoadedModelConfigs, predict_maze_path\n",
    "from maze_transformer.evaluation.pathdist import MazeEvalFunction, ArrMazeEvalFunction, MazeEvalFuncs, ArrMazeEvalFuncs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot example mazes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup consts\n",
    "def testdata_plot_predicted_path(\n",
    "\t\tmodel_path: str = \"data/g4-n4K/g4-n4K_tiny-v1_2022-10-05-02-03-44/model.final.pt\",\n",
    "\t\tn_mazes: int = 10,\n",
    "\t\tn_tokens_pred: int = 8,\n",
    "\t\tfilename: str = \"data/g4-n1K-test/maze_tokens.jsonl\",\n",
    "\t):\n",
    "\tmazes_tokens: list[list[str]] = list()\n",
    "\n",
    "\t# load model and configs\n",
    "\tloaded_model_and_configs: LoadedModelConfigs = load_model_with_configs(model_path, MazeDatasetConfig)\n",
    "\tdata_cfg: MazeDatasetConfig; train_cfg: TrainConfig\n",
    "\tmodel_cfg: OpenAIGPTConfig; model: OpenAIGPTLMHeadModel\n",
    "\tdata_cfg, train_cfg, model_cfg, model = loaded_model_and_configs\n",
    "\tmodel.config.max_length = model_cfg.n_positions + 1\n",
    "\n",
    "\t# load maze test data\n",
    "\twith open(filename, \"r\") as f:\n",
    "\t\tfor line in f:\n",
    "\t\t\tmazes_tokens.append(line.split())\n",
    "\n",
    "\tmazes_tokens = mazes_tokens[:n_mazes]\n",
    "\n",
    "\tmazes_solved: list[tuple[LatticeMaze, MazePath, MazePath]] = [\n",
    "\t\tpredict_maze_path(\n",
    "\t\t\ttokens = tokens,\n",
    "\t\t\tdata_cfg = data_cfg,\n",
    "\t\t\tmodel = model,\n",
    "\t\t\tn_tokens_pred = n_tokens_pred,\n",
    "\t\t)\n",
    "\t\tfor tokens in mazes_tokens\n",
    "\t]\n",
    "\n",
    "\n",
    "\t# plot\n",
    "\tfor maze, path, path_pred in mazes_solved:\n",
    "\t\tplot_multi_paths(\n",
    "\t\t\tmaze = maze,\n",
    "\t\t\tpaths = [\n",
    "\t\t\t\tPathFormat(path, \"true\", color = \"red\", quiver_kwargs = {'width': 0.015}),\n",
    "\t\t\t\tPathFormat(path_pred, \"predicted\", color = \"blue\", quiver_kwargs = {}),\n",
    "\t\t\t],\n",
    "\t\t)\n",
    "\n",
    "# testdata_plot_predicted_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run path dist eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvalFuncTuple = tuple[typing.Literal[\"arr\", \"list\"], MazeEvalFunction|ArrMazeEvalFunction]\n",
    "\n",
    "ALL_PATHDIST_FUNCS: dict[str, EvalFuncTuple] = {\n",
    "\t**{\n",
    "\t\tname: (\"arr\", func)\n",
    "\t\tfor name, func in ArrMazeEvalFuncs.__dict__.items()\n",
    "\t\tif not name.startswith(\"_\")\n",
    "\t},\n",
    "\t**{\n",
    "\t\tname: (\"list\", func)\n",
    "\t\tfor name, func in MazeEvalFuncs.__dict__.items()\n",
    "\t\tif not name.startswith(\"_\")\n",
    "\t},\n",
    "}\n",
    "\n",
    "print(ALL_PATHDIST_FUNCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from maze_transformer.evaluation.eval_model import ArrMazePath\n",
    "\n",
    "\n",
    "def evaluate_model_pathdist_scores(\n",
    "\t\tmodel_path: str,\n",
    "\t\tpathdist_functions: dict[str, EvalFuncTuple]|None = None,\n",
    "\t\tn_tokens_pred: int = 8,\n",
    "\t\ttest_data: str = \"data/g4-n1K-test/maze_tokens.jsonl\",\n",
    "\t\tn_mazes: int = 10,\n",
    "\t\tverbose: bool = False,\n",
    "\t) -> dict[str, StatCounter]:\n",
    "\n",
    "\tif pathdist_functions is None:\n",
    "\t\tpathdist_functions = ALL_PATHDIST_FUNCS\n",
    "\n",
    "\tmazes_tokens: list[list[str]] = list()\n",
    "\n",
    "\t# load model and configs\n",
    "\tloaded_model_and_configs: LoadedModelConfigs = load_model_with_configs(model_path, MazeDatasetConfig)\n",
    "\tdata_cfg: MazeDatasetConfig; train_cfg: TrainConfig\n",
    "\tmodel_cfg: OpenAIGPTConfig; model: OpenAIGPTLMHeadModel\n",
    "\tdata_cfg, train_cfg, model_cfg, model = loaded_model_and_configs\n",
    "\tmodel.config.max_length = model_cfg.n_positions + 1\n",
    "\n",
    "\t# load maze test data\n",
    "\twith open(test_data, \"r\") as f:\n",
    "\t\tfor idx, line in enumerate(f):\n",
    "\t\t\tmazes_tokens.append(line.split())\n",
    "\t\t\tif idx >= n_mazes:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t# predict paths\n",
    "\tmazes_solved: list[tuple[LatticeMaze, MazePath, MazePath]] = list()\n",
    "\tfor tokens in mazes_tokens:\n",
    "\t\tmaze, p_true, p_pred = predict_maze_path(\n",
    "\t\t\ttokens = tokens,\n",
    "\t\t\tdata_cfg = data_cfg,\n",
    "\t\t\tmodel = model,\n",
    "\t\t\tn_tokens_pred = n_tokens_pred,\n",
    "\t\t\tverbose = verbose,\n",
    "\t\t)\n",
    "\n",
    "\t\tmazes_solved.append((maze, p_true, p_pred))\n",
    "\n",
    "\t\tif verbose:\n",
    "\t\t\tprint(f\"{p_true = }\")\n",
    "\t\t\tprint(f\"{p_pred = }\")\n",
    "\n",
    "\t# convert paths\n",
    "\tmazes_solved_arrpath: list[tuple[LatticeMaze, ArrMazePath, ArrMazePath]] = [\n",
    "\t\t(maze, np.array(p_true), np.array(p_pred))\n",
    "\t\tfor maze, p_true, p_pred in mazes_solved\n",
    "\t]\n",
    "\n",
    "\n",
    "\t# evaluate\n",
    "\tpathdist_scores: dict[str, StatCounter] = dict()\n",
    "\tfor name, (pathdist_type, pathdist_func) in pathdist_functions.items():\n",
    "\t\tif pathdist_type == \"list\":\n",
    "\t\t\tpathdist_scores[name] = StatCounter(\n",
    "\t\t\t\tpathdist_func(maze, p_true, p_pred)\n",
    "\t\t\t\tfor maze, p_true, p_pred in mazes_solved\n",
    "\t\t\t)\n",
    "\t\telif pathdist_type == \"arr\":\n",
    "\t\t\tpathdist_scores[name] = StatCounter(\n",
    "\t\t\t\tpathdist_func(maze, p_true, p_pred)\n",
    "\t\t\t\tfor maze, p_true, p_pred in mazes_solved_arrpath\n",
    "\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(f\"Invalid pathdist_type: {pathdist_type}\")\n",
    "\n",
    "\treturn pathdist_scores\n",
    "\n",
    "# evaluate_model_pathdist_scores(\"data/g4-n4K/g4-n4K_tiny-v1_2022-10-05-02-03-44/model.final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pathdist_scores_checkpoints(\n",
    "\t\tpath_base: str,\n",
    "\t\tcheckpoint_idxs: list[int]|None = None,\n",
    "\t\tpathdist_functions: dict[str, EvalFuncTuple]|None = None,\n",
    "\t\tn_tokens_pred: int = 8,\n",
    "\t\ttest_data: str = \"data/g4-n1K-test/maze_tokens.jsonl\",\n",
    "\t\tn_mazes: int = 10,\n",
    "\t\tverbose: bool = False,\n",
    "\t) -> dict[str, dict[int, StatCounter]]:\n",
    "\n",
    "\tmodel_checkpoints: list[tuple[int,str]]\n",
    "\n",
    "\tif checkpoint_idxs is not None:\n",
    "\t\tmodel_checkpoints = list()\n",
    "\t\tfor idx in checkpoint_idxs:\n",
    "\t\t\tmdl_path: Path = Path(path_base) / f\"model.iter_{idx}.pt\"\n",
    "\t\t\tif not mdl_path.exists():\n",
    "\t\t\t\traise ValueError(f\"Checkpoint file {mdl_path} does not exist\")\n",
    "\t\t\tmodel_checkpoints.append((idx, str(mdl_path)))\n",
    "\telse:\n",
    "\t\tmodel_checkpoints = [\n",
    "\t\t\t(int(mdl_path.stem.split(\"_\")[-1].split(\".\")[0]), str(mdl_path))\n",
    "\t\t\tfor mdl_path in Path(path_base).glob(\"model.iter_*.pt\")\n",
    "\t\t]\n",
    "\t\n",
    "\tprint(f\"Found {len(model_checkpoints)} checkpoints:\\n\\t{model_checkpoints = }\")\n",
    "\n",
    "\tpathdist_scores_idx: dict[int, dict[str, StatCounter]] = dict()\n",
    "\n",
    "\tfor idx, mdl_path in model_checkpoints:\n",
    "\t\tprint(f\"# Evaluating checkpoint {idx} at {mdl_path}\")\n",
    "\t\tpathdist_scores_idx[idx] = evaluate_model_pathdist_scores(\n",
    "\t\t\tmodel_path = mdl_path,\n",
    "\t\t\tpathdist_functions = pathdist_functions,\n",
    "\t\t\tn_tokens_pred = n_tokens_pred,\n",
    "\t\t\ttest_data = test_data,\n",
    "\t\t\tn_mazes = n_mazes,\n",
    "\t\t\tverbose = verbose,\n",
    "\t\t)\n",
    "\n",
    "\treturn {\n",
    "\t\tname: {\n",
    "\t\t\tidx: scores[name]\n",
    "\t\t\tfor idx, scores in pathdist_scores_idx.items()\n",
    "\t\t}\n",
    "\t\tfor name in pathdist_scores_idx[0]\n",
    "\t}\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pathdist_scores(\n",
    "\t\tdata: dict[str, dict[int, StatCounter]],\n",
    "\t\tcolors: dict[str, str]|None = None,\n",
    "\t\tpercentile_bounds: tuple[float, float] = (0.4, 0.6),\n",
    "\t):\n",
    "\n",
    "\tif colors is None:\n",
    "\t\tcolors = {\n",
    "\t\t\tfunc_name: f\"C{i}\"\n",
    "\t\t\tfor i, func_name in enumerate(data.keys())\n",
    "\t\t}\n",
    "\n",
    "\tfig, ax = plt.subplots(len(data), 1, figsize = (8, 4 * len(data)))\n",
    "\tfig.subplots_adjust(hspace = 0.5)\n",
    "\t\t\n",
    "\tfor i, (name, scores_idxed) in enumerate(data.items()):\n",
    "\t\tx = list(scores_idxed.keys())\n",
    "\t\ty = [\n",
    "\t\t\tscores_idxed[idx].median()\n",
    "\t\t\tfor idx in x\n",
    "\t\t]\n",
    "\t\tax[i].plot(x, y, label=name, color=colors[name])\n",
    "\t\t# plot shaded error bars\n",
    "\t\ty_ub = [\n",
    "\t\t\tscores_idxed[idx].percentile(percentile_bounds[1])\n",
    "\t\t\tfor idx in x\n",
    "\t\t]\n",
    "\t\ty_lb = [\n",
    "\t\t\tscores_idxed[idx].percentile(percentile_bounds[0])\n",
    "\t\t\tfor idx in x\n",
    "\t\t]\n",
    "\t\tax[i].fill_between(\n",
    "\t\t\tx, y_lb, y_ub,\n",
    "\t    \talpha=0.5, \n",
    "\t\t\tedgecolor=colors[name], facecolor=colors[name],\n",
    "\t\t)\n",
    "\n",
    "\t\tax[i].set_title(f\"{name}, {percentile_bounds = }\")\n",
    "\t\tax[i].set_xlabel(\"Checkpoint\")\n",
    "\t\tax[i].set_ylabel(\"score\")\n",
    "\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = evaluate_pathdist_scores_checkpoints(\n",
    "\tpath_base = \"data/g4-n4K/g4-n4K_tiny-v1_2022-10-05-02-03-44/checkpoints/\",\n",
    "\tcheckpoint_idxs = list(range(0, 9829, 156*16)),\n",
    "\ttest_data = \"data/g4-n1K-test/maze_tokens.jsonl\",\n",
    "\tn_mazes = 8,\n",
    "\t# verbose = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pathdist_scores(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# generate_plot_predicted_path(\"data/g4-n4K/g4-n4K_tiny-v1_2022-09-10-02-02-12/checkpoints/model.iter_3120.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "826cdf1d6a1d995932fcc4b02bd7049699ce423053098b308e34496c9b855014"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
