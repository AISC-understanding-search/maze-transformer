{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we want to:\n",
    "\n",
    "- investigate the spatial structure of the residual stream\n",
    "- see which tokens the different directions in the residual stream map to"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic\n",
    "import os\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import typing\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "# import torch.nn.functional as F\n",
    "from fancy_einsum import einsum\n",
    "import einops\n",
    "from jaxtyping import Float, Int, Bool\n",
    "\n",
    "# scipy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import pearsonr\n",
    "# from scipy.spatial.distance import cosine\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets\n",
    "\n",
    "\n",
    "# muutils\n",
    "from muutils.misc import shorten_numerical_to_str\n",
    "from muutils.nbutils.configure_notebook import configure_notebook\n",
    "# TransformerLens imports\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "\n",
    "# Our Code\n",
    "from maze_dataset import MazeDataset, MazeDatasetConfig, SolvedMaze, LatticeMaze, SPECIAL_TOKENS\n",
    "from maze_dataset.tokenization import MazeTokenizer, TokenizationMode\n",
    "from maze_dataset.plotting.print_tokens import color_maze_tokens_AOTP\n",
    "from maze_dataset.tokenization.token_utils import strings_to_coords, coords_to_strings\n",
    "from maze_dataset.constants import _SPECIAL_TOKENS_ABBREVIATIONS\n",
    "\n",
    "from maze_transformer.training.config import ConfigHolder, ZanjHookedTransformer, BaseGPTConfig\n",
    "from muutils.tensor_utils import string_dict_shapes\n",
    "from maze_transformer.mechinterp.plot_weights import plot_embeddings\n",
    "from maze_transformer.mechinterp.residual_stream_structure import (\n",
    "    TokenPlottingInfo, process_tokens_for_pca, EmbeddingsPCAResult, compute_pca, plot_pca_colored, \n",
    "    compute_distances_and_correlation, compute_grid_distances,\n",
    "    plot_distances_matrix, plot_distance_grid, plot_distance_correlation,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup (we won't be training any models)\n",
    "DEVICE: torch.device = configure_notebook(seed=42, dark_mode=False)\n",
    "print(f\"{DEVICE = }\")\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to load the model from\n",
    "MODEL_PATH: str = \"../examples/hallway-medium_2023-06-16-03-40-47.iter_26554.zanj\"\n",
    "# MODEL_PATH: str = \"../examples/wandb.jerpkipj.pt.zanj\"\n",
    "# load the model and tokenizer\n",
    "MODEL: ZanjHookedTransformer|HookedTransformer = ZanjHookedTransformer.read(MODEL_PATH)\n",
    "num_params: int = MODEL.num_params()\n",
    "print(f\"loaded model with {shorten_numerical_to_str(num_params)} params ({num_params = }) from\\n{MODEL_PATH}\")\n",
    "TOKENIZER: MazeTokenizer = MODEL.zanj_model_config.maze_tokenizer\n",
    "print(f\"using tokenizer with {TOKENIZER = }\")\n",
    "print(f\"{TOKENIZER.vocab_size = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tokenizer and embedding info\n",
    "print(f\"{TOKENIZER.token_arr = }\")\n",
    "d_model: int = MODEL.config.d_model\n",
    "print(f\"{MODEL.W_E.shape = }\")\n",
    "assert MODEL.W_E.shape == (TOKENIZER.vocab_size, d_model)\n",
    "VOCAB_TOKENS: Int[torch.Tensor, \"vocab_size\"] = torch.arange(TOKENIZER.vocab_size, device=DEVICE)\n",
    "assert VOCAB_TOKENS.tolist() == TOKENIZER.encode(TOKENIZER.token_arr)\n",
    "\n",
    "# maps token to (index, coordinate)\n",
    "COORDINATE_TOKENS_INFO: dict[str, (int, tuple[int,int])] = {\n",
    "    tok: coord\n",
    "    for idx, (tok, coord) in enumerate(zip(TOKENIZER.token_arr, TOKENIZER.strings_to_coords(TOKENIZER.token_arr, when_noncoord=\"include\")))\n",
    "    if not isinstance(coord, str)\n",
    "}\n",
    "print(f\"{COORDINATE_TOKENS_INFO = }\")\n",
    "\n",
    "# information for how to plot the tokens\n",
    "VOCAB_PLOT_INFO: list[TokenPlottingInfo] = process_tokens_for_pca(TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL.process_weights_(fold_ln=False, center_writing_weights=False, center_unembed=False)\n",
    "# MODEL.process_weights_(fold_ln=False, center_writing_weights=False, center_unembed=True)\n",
    "# MODEL.process_weights_(fold_ln=False, center_writing_weights=True, center_unembed=False)\n",
    "# MODEL.process_weights_(fold_ln=False, center_writing_weights=True, center_unembed=True)\n",
    "# MODEL.process_weights_(fold_ln=True, center_writing_weights=False, center_unembed=False)\n",
    "# MODEL.process_weights_(fold_ln=True, center_writing_weights=False, center_unembed=True)\n",
    "# MODEL.process_weights_(fold_ln=True, center_writing_weights=True, center_unembed=False)\n",
    "# MODEL.process_weights_(fold_ln=True, center_writing_weights=True, center_unembed=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible distance functions:\n",
    "\"\"\"\n",
    "'braycurtis', 'canberra', 'chebyshev', 'cityblock',\n",
    "'correlation', 'cosine', 'dice', 'euclidean', 'hamming',\n",
    "'jaccard', 'jensenshannon', 'kulczynski1',\n",
    "'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto',\n",
    "'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath',\n",
    "'sqeuclidean', 'yule'\n",
    "\"\"\"\n",
    "\n",
    "MODEL_EMBEDDING_MATRIX: Float[np.ndarray, \"d_vocab d_model\"] = MODEL.W_E.detach().cpu().numpy()\n",
    "\n",
    "distances_result: dict = compute_distances_and_correlation(\n",
    "    embedding_matrix=MODEL_EMBEDDING_MATRIX,\n",
    "    tokenizer=TOKENIZER,\n",
    "    embedding_metric=\"cosine\",\n",
    "    coordinate_metric=\"cityblock\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print info about 'correlation', 'corr_pval', 'tokenizer', 'embedding_metric', 'coordinate_metric'\n",
    "print(\n",
    "    f\"\"\"Correlation between embedding (via {distances_result['embedding_metric']}) and coordinate (via {distances_result['coordinate_metric']}) distances:\n",
    "{distances_result['correlation']:.3f} (p={distances_result['corr_pval']:.3f})\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distances_matrix(**distances_result, show=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_grid: Float[np.ndarray, \"n n n n\"] = compute_grid_distances(\n",
    "    embedding_distances_matrix=distances_result[\"embedding_distances_matrix\"],\n",
    "    tokenizer=TOKENIZER,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distance_correlation(\n",
    "    distance_grid,\n",
    "    embedding_metric=distances_result[\"embedding_metric\"],\n",
    "    coordinate_metric=distances_result[\"coordinate_metric\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distance_grid(distance_grid, embedding_metric=distances_result[\"embedding_metric\"], show=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA of token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"dont run the rest for now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the PCA\n",
    "PCA_RESULTS: dict[str, EmbeddingsPCAResult] = compute_pca(\n",
    "    model=MODEL,\n",
    "    token_plotting_info=VOCAB_PLOT_INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipywidgets.interact(\n",
    "    plot_pca_colored, \n",
    "    pca_results_options=ipywidgets.fixed(PCA_RESULTS),\n",
    "    pca_results_key=ipywidgets.Dropdown(\n",
    "        options=list(PCA_RESULTS.keys()),\n",
    "        value='all',\n",
    "        description='PCA Results:',\n",
    "    ),\n",
    "    vocab_colors=ipywidgets.fixed(VOCAB_PLOT_INFO), \n",
    "    dim1=ipywidgets.IntText(\n",
    "        value=1,\n",
    "        description='Dim 1:',\n",
    "    ),\n",
    "    dim2=ipywidgets.IntText(\n",
    "        value=2,\n",
    "        description='Dim 1:',\n",
    "    ),\n",
    "    lattice_connections=ipywidgets.Dropdown(\n",
    "        options=[True, False],\n",
    "        value=True,\n",
    "        description='Show Lattice:',\n",
    "    ),\n",
    "    # float or None (entry box of float or text)\n",
    "    symlog_scale=ipywidgets.FloatText(\n",
    "        value=-1,\n",
    "        description='Symlog Scale:',\n",
    "    ),\n",
    "    axes_and_centered=ipywidgets.Dropdown(\n",
    "        options=[True, False],\n",
    "        value=True,\n",
    "        description='Center and show axes:',\n",
    "    ),\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
