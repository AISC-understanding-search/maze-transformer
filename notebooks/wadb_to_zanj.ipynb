{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mivan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-transformer-2cGx2R0F-py3.10\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "from muutils.misc import shorten_numerical_to_str\n",
    "\n",
    "from maze_transformer.training.config import BaseGPTConfig, TrainConfig, ConfigHolder\n",
    "from maze_dataset import MazeDataset, MazeDatasetConfig\n",
    "from maze_transformer.training.config import ZanjHookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download wandb artifact\n",
    "\n",
    "\n",
    "def get_step(artifact):\n",
    "    # Find the alias beginning with \"step=\"\n",
    "    step_alias = [alias for alias in artifact.aliases if alias.startswith(\"step=\")]\n",
    "    if len(step_alias) != 1: # if we have multiple, skip as well\n",
    "        return -1\n",
    "    return int(step_alias[0].split('=')[-1])\n",
    "\n",
    "def load_model(config_holder, model_path, fold_ln=True):\n",
    "    model = config_holder.create_model()\n",
    "    state_dict = torch.load(model_path, map_location=model.cfg.device)\n",
    "    model.load_and_process_state_dict(\n",
    "        state_dict,\n",
    "        fold_ln=False,\n",
    "        center_writing_weights=True,\n",
    "        center_unembed=True,\n",
    "        refactor_factored_attn_matrices=True,\n",
    "    )\n",
    "    model.process_weights_(fold_ln=fold_ln)\n",
    "    model.setup()  # Re-attach layernorm hooks by calling setup\n",
    "    model.eval()\n",
    "    return model \n",
    "\n",
    "def load_wandb_run(\n",
    "        project=\"aisc-search/alex\", \n",
    "\t\trun_id=\"sa973hyn\", \n",
    "\t\toutput_path='./downloaded_models',\n",
    "\t\tcheckpoint=None,\n",
    "\t):\n",
    "    api = wandb.Api()\n",
    "\n",
    "    artifact_name = f\"{project.rstrip('/')}/{run_id}\"\n",
    "\n",
    "    run = api.run(artifact_name)\n",
    "    wandb_cfg = run.config  # Get run configuration\n",
    "\n",
    "    # -- Get / Match checkpoint --\n",
    "    if checkpoint is not None:\n",
    "        # Match checkpoint \n",
    "        available_checkpoints = [artifact for artifact in run.logged_artifacts() if artifact.type == 'model']\n",
    "        available_checkpoints = list(run.logged_artifacts())\n",
    "        artifact = [artifact for artifact in available_checkpoints if get_step(artifact) == checkpoint]\n",
    "        if len(artifact) != 1:\n",
    "            print(f\"Could not find checkpoint {checkpoint} in {artifact_name}\")\n",
    "            print(\"Available checkpoints:\")\n",
    "            [print(artifact.name, '| Steps: ', get_step(artifact)) for artifact in available_checkpoints]\n",
    "            return\n",
    "\n",
    "        artifact = artifact[0]\n",
    "        print('Loading checkpoint', checkpoint)\n",
    "    else:\n",
    "        # Get latest checkpoint\n",
    "        print('Loading latest checkpoint')\n",
    "        artifact_name = f\"{artifact_name}:latest\"\n",
    "        artifact = api.artifact(artifact_name)\n",
    "        checkpoint = get_step(artifact)    \n",
    "\n",
    "    # -- Initalize configurations --\n",
    "    # Model cfg\n",
    "    model_properties = {k:wandb_cfg[k] for k in ['act_fn', 'd_model', 'd_head', 'n_layers']}\n",
    "    model_cfg = BaseGPTConfig(name=f\"model {run_id}\", weight_processing={'are_layernorms_folded': True, 'are_weights_processed': True}, **model_properties)\n",
    "\n",
    "    # Dataset cfg\n",
    "    grid_n = math.sqrt(wandb_cfg['d_vocab'] - 11) #! Jank \n",
    "    assert grid_n == int(grid_n), \"grid_n must be a perfect square + 11\"  # check integer\n",
    "    ds_cfg = MazeDatasetConfig(name=wandb_cfg.get('dataset_name','no_name'), grid_n=int(grid_n), n_mazes=1)\n",
    "\n",
    "\n",
    "    cfg = ConfigHolder(model_cfg=model_cfg, dataset_cfg=ds_cfg, train_cfg=TrainConfig(name=f\"artifact '{artifact_name}', checkpoint '{checkpoint}'\"))\n",
    "\n",
    "    download_path = Path(output_path)/f'{artifact.name.split(\":\")[0]}'/f'model.iter_{checkpoint}.pt'\n",
    "    #! Account for final checkpoint\n",
    "    if not download_path.exists():\n",
    "        artifact.download(root=download_path.parent)\n",
    "        print(f'Downloaded model to {download_path}')\n",
    "    else:\n",
    "        print(f'Model already downloaded to {download_path}')\n",
    "\n",
    "    print('Loading model')\n",
    "    model = load_model(cfg, download_path, fold_ln=True)\n",
    "    return  model, cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading latest checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded model to downloaded_models\\sa973hyn\\model.iter_29000000.pt\n",
      "Loading model\n",
      "type(MODEL) = <class 'transformer_lens.HookedTransformer.HookedTransformer'> type(CFG) = <class 'maze_transformer.training.config.ConfigHolder'>\n"
     ]
    }
   ],
   "source": [
    "MODEL_KWARGS: dict = dict(\n",
    "    project=\"aisc-search/alex\", \n",
    "\trun_id=\"jerpkipj\", \n",
    "\tcheckpoint=None,\n",
    ")\n",
    "MODEL, CFG = load_wandb_run(**MODEL_KWARGS)\n",
    "\n",
    "print(f\"{type(MODEL) = } {type(CFG) = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model with 9.6M parameters\n",
      "{'load_wandb_run_kwargs': {'project': 'aisc-search/alex', 'run_id': 'sa973hyn', 'checkpoint': None}, 'train_cfg.name': \"artifact 'aisc-search/alex/sa973hyn:latest', checkpoint '29000000'\"}\n"
     ]
    }
   ],
   "source": [
    "MODEL_ZANJ: ZanjHookedTransformer = ZanjHookedTransformer(CFG)\n",
    "MODEL_ZANJ.load_state_dict(MODEL.state_dict())\n",
    "MODEL_ZANJ.training_records = {\n",
    "    \"load_wandb_run_kwargs\": MODEL_KWARGS,\n",
    "    \"train_cfg.name\": CFG.train_cfg.name,\n",
    "}\n",
    "print(f\"loaded model with {shorten_numerical_to_str(MODEL_ZANJ.num_params())} parameters\")\n",
    "print(MODEL_ZANJ.training_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ZANJ.save(f\"examples/wandb.{MODEL_KWARGS['run_id']}.zanj\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maze-transformer",
   "language": "python",
   "name": "maze-transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
