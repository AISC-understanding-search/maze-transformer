{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# print(os.environ.get('CUDA_LAUNCH_BLOCKING', None))\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# print(os.environ.get('CUDA_LAUNCH_BLOCKING', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic\n",
    "from pathlib import Path\n",
    "import typing\n",
    "import itertools\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from jaxtyping import Bool, Int, Float\n",
    "from muutils.nbutils.configure_notebook import configure_notebook\n",
    "\n",
    "# Our Code\n",
    "# dataset stuff\n",
    "from maze_dataset import MazeDataset, MazeDatasetConfig, SolvedMaze, LatticeMaze, SPECIAL_TOKENS, LatticeMazeGenerators, CoordArray\n",
    "from maze_dataset.tokenization import MazeTokenizer, TokenizationMode\n",
    "\n",
    "# model stuff\n",
    "from maze_transformer.training.config import ZanjHookedTransformer\n",
    "\n",
    "# mechinterp stuff\n",
    "from maze_transformer.mechinterp.logit_attrib_task import LOGIT_ATTRIB_TASKS\n",
    "from maze_transformer.evaluation.load_model_testdata import load_model_with_test_data\n",
    "from maze_transformer.evaluation.eval_single_token_tasks import TaskPrompt, TaskEvalResult, get_task_prompts_targets, eval_model_across_tasks\n",
    "from maze_transformer.mechinterp.logit_attrib_task import LOGIT_ATTRIB_TASKS\n",
    "from maze_transformer.evaluation.eval_model import predict_maze_paths\n",
    "from maze_transformer.evaluation.path_evals import rollout_evals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup (we won't be training any models)\n",
    "DEVICE: torch.device = configure_notebook(seed=42, dark_mode=False)\n",
    "print(f\"{DEVICE = }\")\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "which models to evaluate, how many mazes to use for evaluation, and which datasets to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATHS: list[str] = [\n",
    "\t\"../examples/model.hallway-jvq.final.zanj\",\n",
    "\t# \"../examples/wandb.jerpkipj.zanj\",\n",
    "]\n",
    "\n",
    "N_MAZES: int = 8\n",
    "GRID_N_VALS: list[int] = [6, 7]\n",
    "SEED=46\n",
    "\n",
    "DATASET_CFG_SOURCES: list[MazeDatasetConfig] = list(itertools.chain.from_iterable([\n",
    "\t[\n",
    "\t\tMazeDatasetConfig(\n",
    "\t\t\tname=\"forkless\",\n",
    "\t\t\tgrid_n=grid_n,\n",
    "\t\t\tn_mazes=N_MAZES,\n",
    "\t\t\tmaze_ctor=LatticeMazeGenerators.gen_dfs,\n",
    "\t\t\tmaze_ctor_kwargs=dict(do_forks=False),\n",
    "\t\t\tapplied_filters=[{'name': 'path_length', 'args': (), 'kwargs': {\"min_length\": 6}}],\n",
    "\t\t\tseed=SEED,\n",
    "\t\t),\n",
    "\t\tMazeDatasetConfig(\n",
    "\t\t\tname=\"RDFS\",\n",
    "\t\t\tgrid_n=grid_n,\n",
    "\t\t\tn_mazes=N_MAZES,\n",
    "\t\t\tmaze_ctor=LatticeMazeGenerators.gen_dfs,\n",
    "\t\t\tmaze_ctor_kwargs=dict(do_forks=True),\n",
    "\t\t\tapplied_filters=[{'name': 'path_length', 'args': (), 'kwargs': {\"min_length\": 6}}],\n",
    "\t\t\tseed=SEED,\n",
    "\t\t),\n",
    "\t\tMazeDatasetConfig(\n",
    "\t\t\tname=\"pRDFS\",\n",
    "\t\t\tgrid_n=grid_n,\n",
    "\t\t\tn_mazes=N_MAZES,\n",
    "\t\t\tmaze_ctor=LatticeMazeGenerators.gen_dfs_percolation,\n",
    "\t\t\tmaze_ctor_kwargs=dict(p=0.1),\n",
    "\t\t\tapplied_filters=[{'name': 'path_length', 'args': (), 'kwargs': {\"min_length\": 6}}],\n",
    "\t\t\tseed=SEED,\n",
    "\t\t),\n",
    "\t]\n",
    "\tfor grid_n in GRID_N_VALS\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create and inspect datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS: list[MazeDataset] = [\n",
    "\tMazeDataset.from_config(dcs)\n",
    "\tfor dcs in DATASET_CFG_SOURCES\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the distribution of path lengths for each dataset\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(len(DATASETS), 2, figsize=(8, 4), sharex=\"col\")\n",
    "placeholder_tokenizer: MazeTokenizer = MazeTokenizer(tokenization_mode=TokenizationMode.AOTP_UT_uniform, max_grid_size=16)\n",
    "for i, ds in enumerate(DATASETS):\n",
    "\ttotal_lengths = [len(maze.as_tokens(placeholder_tokenizer)) for maze in ds]\n",
    "\tpath_lengths = [len(maze.solution) for maze in ds]\n",
    "\tax[i, 0].hist(total_lengths, bins=16, alpha=0.5, label=\"total lengths\")\n",
    "\tax[i, 1].hist(path_lengths, bins=16, alpha=0.5, label=\"path lengths\")\n",
    "\tax[i, 0].set_title(f\"{ds.cfg.to_fname()}, max = {max(total_lengths)}\")\n",
    "\tax[i, 1].set_title(f\"max = {max(path_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eval_table(\n",
    "\tmodels_paths: list[str],\n",
    "\tdatasets: list[MazeDataset],\n",
    ") -> pd.DataFrame:\n",
    "\t\n",
    "\tfrom maze_dataset.plotting import MazePlot, PathFormat\n",
    "\tfrom maze_transformer.evaluation.plotting import plot_predicted_paths\n",
    "\n",
    "\tfor model_path in models_paths:\n",
    "\t\tprint(\"-\"*50)\n",
    "\t\tmodel, _ = load_model_with_test_data(\n",
    "\t\t\tmodel_path=model_path,\n",
    "\t\t\tdataset_cfg_source=None,\n",
    "\t\t\tn_examples=None,\n",
    "\t\t)\n",
    "\n",
    "\t\t# iter over datasets\n",
    "\t\tfor dataset in datasets:\n",
    "\t\t\tprint(f\"rollouts for {model_path} on {dataset.cfg.to_fname()}\")\n",
    "\n",
    "\t\t\tplot_predicted_paths(\n",
    "\t\t\t\tmodel, \n",
    "\t\t\t\tdataset, \n",
    "\t\t\t\tn_mazes=3,\n",
    "\t\t\t\tmax_new_tokens=50, \n",
    "\t\t\t\trow_length=3, \n",
    "\t\t\t\tfigsize_scale=5,\n",
    "\t\t\t\tpredicted_path_fmt=PathFormat(cmap=\"winter\"),\n",
    "\t\t\t\tshow=False,\n",
    "\t\t\t\tprint_generations=False,\n",
    "\t\t\t)\n",
    "\t\t\tplt.suptitle(f\"model: {model_path.split('/')[-1]}\\ndataset: {dataset.cfg.to_fname()}\")\n",
    "\t\t\t# plt.savefig(f\"rollouts/{model_path.split('/')[-1]}.{dataset.cfg.to_fname()}.pdf\")\n",
    "\t\t\tplt.show()\n",
    "\n",
    "generate_eval_table(\n",
    "\tmodels_paths=MODELS_PATHS,\n",
    "\tdatasets=DATASETS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eval_table(\n",
    "\tmodels_paths: list[str],\n",
    "\tdatasets: list[MazeDataset],\n",
    "\trollouts_acc: int|None = None,\n",
    "\trollout_temperature: float = 0.0,\n",
    "\tprint_completions: bool = False,\n",
    "\tbatch_size: int|None = 16,\n",
    ") -> pd.DataFrame:\n",
    "\t\n",
    "\toutput: list[dict] = list()\n",
    "\t\n",
    "\t# iter over models\n",
    "\tfor model_path in models_paths:\n",
    "\t\tprint(\"-\"*50)\n",
    "\t\tmodel, _ = load_model_with_test_data(\n",
    "\t\t\tmodel_path=model_path,\n",
    "\t\t\tdataset_cfg_source=None,\n",
    "\t\t\tn_examples=None,\n",
    "\t\t)\n",
    "\n",
    "\t\t# iter over datasets\n",
    "\t\tfor dataset in datasets:\n",
    "\t\t\tprint(f\"evaluating {model_path} on {dataset.cfg.to_fname()}\")\n",
    "\n",
    "\t\t\ttokenizer: MazeTokenizer = model.zanj_model_config.maze_tokenizer\n",
    "\t\t\ttask_prompt_targets: dict[str, TaskPrompt] = get_task_prompts_targets(\n",
    "\t\t\t\tdataset=dataset,\n",
    "\t\t\t\tmaze_tokenizer=tokenizer,\n",
    "\t\t\t\ttasks=LOGIT_ATTRIB_TASKS,\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tprint(f\"\\trunning task evals\")\n",
    "\t\t\ttask_results: dict[str, TaskEvalResult] = eval_model_across_tasks(\n",
    "\t\t\t\tmodel = model,\n",
    "\t\t\t\ttask_prompts = task_prompt_targets,\n",
    "\t\t\t\tdo_cache=False,\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# TODO: store whole array of predictions, not just mean -- use this for correlations between task accuracies?\n",
    "\t\t\toutput_item: dict = dict(\n",
    "\t\t\t\tmodel=model.config.name,\n",
    "\t\t\t\tdataset=dataset.cfg.to_fname(),\n",
    "\t\t\t\t**{\n",
    "\t\t\t\t\ttask_name: task_result.predicted_correct.cpu().float().mean().item()\n",
    "\t\t\t\t\tfor task_name, task_result in task_results.items()\n",
    "\t\t\t\t},\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tif rollouts_acc is not None:\n",
    "\t\t\t\tprint(f\"\\trunning rollout evals\")\n",
    "\t\t\t\tdataset_tokens: list[list[str]] = dataset.as_tokens(maze_tokenizer=tokenizer)[:rollouts_acc]\n",
    "\t\t\t\t# do rollouts\n",
    "\t\t\t\tpredictions: list[list[str|tuple[int, int]]] = predict_maze_paths(\n",
    "\t\t\t\t\ttokens_batch=dataset_tokens,\n",
    "\t\t\t\t\tdata_cfg=dataset.cfg,\n",
    "\t\t\t\t\tmodel=model,\n",
    "\t\t\t\t\tmax_new_tokens=None,\n",
    "\t\t\t\t\ttemperature=rollout_temperature,\n",
    "\t\t\t\t\twhen_noncoord=\"include\",\n",
    "\t\t\t\t\tsmart_max_new_tokens=True,\n",
    "\t\t\t\t\tbatch_size=batch_size,\n",
    "\t\t\t\t)\n",
    "\t\t\t\t\n",
    "\t\t\t\tif print_completions:\n",
    "\t\t\t\t\tfor p, x in zip(dataset_tokens, predictions):\n",
    "\t\t\t\t\t\tp_idx: int = p.index(SPECIAL_TOKENS.PATH_START)\n",
    "\t\t\t\t\t\tp_ctx: list[str] = p[:p_idx]\n",
    "\t\t\t\t\t\tp_path: list[str] = p[p_idx:]\n",
    "\t\t\t\t\t\tprint(\"\\t\\t\", \" \".join(p_ctx))\n",
    "\t\t\t\t\t\tprint(\"\\t\\t\", \" \".join(p_path))\n",
    "\t\t\t\t\t\tprint(\"\\t\\t\", \" \".join([str(t) for t in x]))\n",
    "\n",
    "\t\t\t\tmazes: list[SolvedMaze] = dataset.mazes[:rollouts_acc]\n",
    "\t\t\t\tassert len(predictions) == len(mazes)\n",
    "\n",
    "\t\t\t\t# evaluate rollouts\n",
    "\t\t\t\trollout_eval_results: dict[str, float] = rollout_evals(\n",
    "\t\t\t\t\tpredictions=predictions,\n",
    "\t\t\t\t\tmazes=mazes,\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\t\t\toutput_item.update(rollout_eval_results)\n",
    "\n",
    "\t\t\tprint(f\"\\t{output_item}\")\n",
    "\t\t\toutput.append(output_item)\n",
    "\n",
    "\t\t# delete model to save memory\n",
    "\t\tdel model\n",
    "\n",
    "\treturn pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS: pd.DataFrame = generate_eval_table(\n",
    "\tmodels_paths=MODELS_PATHS,\n",
    "\tdatasets=DATASETS,\n",
    "\trollouts_acc=N_MAZES,\n",
    "\tprint_completions=False,\n",
    "\tbatch_size=128, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# view and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS.to_json(f\"eval_results-n{N_MAZES}.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively, load and processed a saved dataset\n",
    "# import pandas as pd\n",
    "# RESULTS = pd.read_json(f\"eval_results-n256.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS['model/dataset'] = (\n",
    "\tRESULTS['model'].apply(lambda x: x.split('-')[0]) \n",
    "\t+ '/' \n",
    "\t+ RESULTS['dataset']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_df = RESULTS.drop(['model', 'dataset'], axis=1).set_index('model/dataset').T.reset_index()\n",
    "transposed_df.rename(columns={'index': 'Metric'}, inplace=True)\n",
    "# convert all to percentages\n",
    "transposed_df = transposed_df.applymap(lambda x: f\"{x:.1%}\" if isinstance(x, float) else x)\n",
    "transposed_df\n",
    "# columns by what comes after the slash\n",
    "transposed_df = transposed_df[\n",
    "\tsorted(\n",
    "\t\ttransposed_df.columns, \n",
    "\t\tkey = lambda x: (\n",
    "\t\t\t\"0\" if x == 'Metric' \n",
    "\t\t\telse x\n",
    "\t\t\t# else [\"hallway\", \"forkless\", \"RDFS\", \"pRDFS\"].index(x.split('/')[1].split('-')[0])\n",
    "\t\t)\n",
    "\t)\n",
    "]\n",
    "transposed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_df.to_csv(f\"eval_results-n256.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transposed_df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_g6 = pd.read_csv(\"processed/eval_results-n256-g6.csv\")\n",
    "# df_g7 = pd.read_csv(\"processed/eval_results-n256-g7.csv\")\n",
    "# print(df_g6.to_latex(index=False))\n",
    "# print(\"=\"*80)\n",
    "# print(df_g7.to_latex(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maze-transformer-2cGx2R0F-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
