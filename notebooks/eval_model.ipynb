{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generic\n",
    "import typing\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Utilities\n",
    "from muutils.statcounter import StatCounter\n",
    "\n",
    "# Our Code\n",
    "from maze_transformer.utils.notebook_utils import configure_notebook\n",
    "from maze_transformer.generation.latticemaze import LatticeMaze\n",
    "from maze_transformer.evaluation.plot_maze import plot_multi_paths, PathFormat\n",
    "from maze_transformer.evaluation.eval_model import MazePath, ArrMazePath, load_model_with_configs, predict_maze_path\n",
    "from maze_transformer.evaluation.pathdist import MazeEvalFunction, ArrMazeEvalFunction, MazeEvalFuncs, ArrMazeEvalFuncs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setup\n",
    "device = configure_notebook(seed=42, dark_mode=True)\n",
    "# We won't be training any models\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Get latest model\n",
    "# this should point towards a directory containing a run. If you don't have any runs, you can use `poetry run python scripts/create_dataset.py create ./data/maze 10 --grid_n=4`\n",
    "run_path = Path(\"../data/maze/g4-n10\")\n",
    "assert run_path.exists(), f\"Run path {run_path.as_posix()} does not exist\"\n",
    "model_path = list(sorted(run_path.glob(\"**/model.final.pt\"), key=os.path.getmtime))[\n",
    "\t-1\n",
    "].resolve()\n",
    "maze_path = run_path / \"maze_tokens.jsonl\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# plot example mazes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# setup consts\n",
    "def testdata_plot_predicted_path(\n",
    "\t\tmodel_path: Path,\n",
    "\t\tmaze_tokens_path: Path, \n",
    "\t\tn_mazes: int = 10,\n",
    "\t\tn_tokens_pred: int = 8,\n",
    "\t):\n",
    "\t# load model and configs\n",
    "\tmodel, cfg = load_model_with_configs(model_path)\n",
    "\t\n",
    "\t# load maze test data\n",
    "\tmazes_tokens: list[list[str]] = [\n",
    " \t   line.split() for line in maze_tokens_path.read_text().splitlines()\n",
    "\t]\n",
    "\tmazes_tokens = mazes_tokens[:n_mazes]\n",
    "\n",
    "\tmazes_solved: list[tuple[LatticeMaze, MazePath, MazePath]] = [\n",
    "\t\tpredict_maze_path(\n",
    "\t\t\ttokens = tokens,\n",
    "\t\t\tdata_cfg = cfg.dataset_cfg,\n",
    "\t\t\tmodel = model,\n",
    "\t\tn_tokens_pred = n_tokens_pred,\n",
    "\t\t)\n",
    "\t\tfor tokens in mazes_tokens\n",
    "\t]\n",
    "\n",
    "\n",
    "\t# plot\n",
    "\tfor maze, path, path_pred in mazes_solved:\n",
    "\t\tplot_multi_paths(\n",
    "\t\t\tmaze = maze,\n",
    "\t\t\tpaths = [\n",
    "\t\t\t\tPathFormat(path, \"true\", color = \"red\", quiver_kwargs = {'width': 0.015}),\n",
    "\t\t\t\tPathFormat(path_pred, \"predicted\", color = \"blue\", quiver_kwargs = {}),\n",
    "\t\t\t],\n",
    "\t\t)\n",
    "\n",
    "testdata_plot_predicted_path(model_path, maze_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# run path dist eval"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "EvalFuncTuple = tuple[typing.Literal[\"arr\", \"list\"], MazeEvalFunction|ArrMazeEvalFunction]\n",
    "\n",
    "ALL_PATHDIST_FUNCS: dict[str, EvalFuncTuple] = {\n",
    "\t**{\n",
    "\t\tname: (\"arr\", func)\n",
    "\t\tfor name, func in ArrMazeEvalFuncs.__dict__.items()\n",
    "\t\tif not name.startswith(\"_\")\n",
    "\t},\n",
    "\t**{\n",
    "\t\tname: (\"list\", func)\n",
    "\t\tfor name, func in MazeEvalFuncs.__dict__.items()\n",
    "\t\tif not name.startswith(\"_\")\n",
    "\t},\n",
    "}\n",
    "\n",
    "print(ALL_PATHDIST_FUNCS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_model_pathdist_scores(\n",
    "\t\tmodel_path: Path,\n",
    "\t\tmaze_tokens_path: Path,\n",
    "\t\tpathdist_functions: dict[str, EvalFuncTuple] | None = ALL_PATHDIST_FUNCS,\n",
    "\t\tn_tokens_pred: int = 8,\n",
    "\t\tn_mazes: int = 64,\n",
    "\t\tverbose: bool = False,\n",
    "\t) -> dict[str, StatCounter]:\n",
    "\n",
    "\tmazes_tokens: list[list[str]] = list()\n",
    "\n",
    "\t# load model and configs\n",
    "\tmodel, cfg = load_model_with_configs(model_path)\n",
    "\n",
    "\t# load maze test data\n",
    "\tmazes_tokens: list[list[str]] = [\n",
    " \t   line.split() for line in maze_tokens_path.read_text().splitlines()\n",
    "\t]\n",
    "\n",
    "\tmazes_tokens_initial = mazes_tokens\n",
    "\tmazes_tokens_after = []\n",
    "\twith open(maze_tokens_path, \"r\") as f:\n",
    "\t\tfor idx, line in enumerate(f):\n",
    "\t\t\tmazes_tokens_after.append(line.split())\n",
    "\t\t\tif idx >= n_mazes:\n",
    "\t\t\t\tbreak\n",
    "\tassert all([x==y for x,y in zip(mazes_tokens_initial, mazes_tokens_after)])\n",
    "\tprint('passed check')\n",
    "\n",
    "\t# predict paths\n",
    "\tmazes_solved: list[tuple[LatticeMaze, MazePath, MazePath]] = list()\n",
    "\tfor tokens in mazes_tokens:\n",
    "\t\tmaze, p_true, p_pred = predict_maze_path(\n",
    "\t\t\ttokens = tokens,\n",
    "\t\t\tdata_cfg = cfg.dataset_cfg,\n",
    "\t\t\tmodel = model,\n",
    "\t\t\tn_tokens_pred = n_tokens_pred,\n",
    "\t\t\tverbose = verbose,\n",
    "\t\t)\n",
    "\n",
    "\t\tmazes_solved.append((maze, p_true, p_pred))\n",
    "\n",
    "\t\tif verbose:\n",
    "\t\t\tprint(f\"{p_true = }\")\n",
    "\t\t\tprint(f\"{p_pred = }\")\n",
    "\n",
    "\t# convert paths\n",
    "\tmazes_solved_arrpath: list[tuple[LatticeMaze, ArrMazePath, ArrMazePath]] = [\n",
    "\t\t(maze, np.array(p_true), np.array(p_pred))\n",
    "\t\tfor maze, p_true, p_pred in mazes_solved\n",
    "\t]\n",
    "\n",
    "\t# evaluate\n",
    "\tpathdist_scores: dict[str, StatCounter] = dict()\n",
    "\tfor name, (pathdist_type, pathdist_func) in pathdist_functions.items():\n",
    "\t\tif pathdist_type == \"list\":\n",
    "\t\t\tpathdist_scores[name] = StatCounter(\n",
    "\t\t\t\tpathdist_func(maze, p_true, p_pred)\n",
    "\t\t\t\tfor maze, p_true, p_pred in mazes_solved\n",
    "\t\t\t)\n",
    "\t\telif pathdist_type == \"arr\":\n",
    "\t\t\tpathdist_scores[name] = StatCounter(\n",
    "\t\t\t\tpathdist_func(maze, p_true, p_pred)\n",
    "\t\t\t\tfor maze, p_true, p_pred in mazes_solved_arrpath\n",
    "\t\t\t)\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(f\"Invalid pathdist_type: {pathdist_type}\")\n",
    "\n",
    "\treturn pathdist_scores\n",
    "\n",
    "evaluate_model_pathdist_scores(model_path, maze_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_pathdist_scores_checkpoints(\n",
    "\t\trun_path: Path, # Path to run, not model.final.pt or checkpoints\n",
    "\t\tmaze_tokens_path: Path,\n",
    "\t\tcheckpoint_idxs: list[int]|None = None,\n",
    "\t\tpathdist_functions: dict[str, EvalFuncTuple]|None = ALL_PATHDIST_FUNCS,\n",
    "\t\tskip_every_nth: int = 1,\n",
    "\t\tn_tokens_pred: int = 8,\n",
    "\t\tn_mazes: int = 10,\n",
    "\t\tverbose: bool = False,\n",
    "\t) -> dict[str, dict[int, StatCounter]]:\n",
    "\n",
    "\tmodel_checkpoints: list[tuple[int,Path]]\n",
    "\tassert run_path.is_dir(), f\"Model path {run_path} is not a directory (expect run directory, not model files)\"\n",
    "\n",
    "\tif checkpoint_idxs is not None:\n",
    "\t\tmodel_checkpoints = list()\n",
    "\t\tfor idx in checkpoint_idxs:\n",
    "\t\t\tmdl_path: Path = Path(run_path) / f\"checkpoints/model.iter_{idx}.pt\"\n",
    "\t\t\tif not mdl_path.exists():\n",
    "\t\t\t\traise ValueError(f\"Checkpoint file {mdl_path} does not exist\")\n",
    "\t\t\tmodel_checkpoints.append((idx, mdl_path))\n",
    "\telse:\n",
    "\t\tmodel_checkpoints = [\n",
    "\t\t\t(int(mdl_path.stem.split(\"_\")[-1].split(\".\")[0]), mdl_path)\n",
    "\t\t\tfor mdl_path in sorted(Path(run_path).glob(\"checkpoints/model.iter_*.pt\"))\n",
    "\t\t]\n",
    "\t\n",
    "\tprint(f\"Found {len(model_checkpoints)} checkpoints:\\n\\t{model_checkpoints = }\")\n",
    "\n",
    "\tpathdist_scores_idx: dict[int, dict[str, StatCounter]] = dict()\n",
    "\n",
    "\tfor idx, mdl_path in model_checkpoints[::skip_every_nth]:\n",
    "\n",
    "\t\tprint(f\"# Evaluating checkpoint {idx} at {mdl_path}\")\n",
    "\t\tpathdist_scores_idx[idx] = evaluate_model_pathdist_scores(\n",
    "\t\t\tmodel_path = mdl_path,\n",
    "\t\t\tpathdist_functions = pathdist_functions,\n",
    "\t\t\tn_tokens_pred = n_tokens_pred,\n",
    "\t\t\tmaze_tokens_path = maze_tokens_path,\n",
    "\t\t\tn_mazes = n_mazes,\n",
    "\t\t\tverbose = verbose,\n",
    "\t\t)\n",
    "\n",
    "\treturn {\n",
    "\t\tname: {\n",
    "\t\t\tidx: scores[name]\n",
    "\t\t\tfor idx, scores in pathdist_scores_idx.items()\n",
    "\t\t}\n",
    "\t\tfor name in pathdist_scores_idx[0]\n",
    "\t}\n",
    "\n",
    "data = evaluate_pathdist_scores_checkpoints(\n",
    "\trun_path = model_path.parent,\n",
    "\tmaze_tokens_path = maze_path,\n",
    "\tn_mazes = 4,\n",
    "\tskip_every_nth=10,\n",
    "\t# verbose = True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_pathdist_scores(\n",
    "\t\tdata: dict[str, dict[int, StatCounter]],\n",
    "\t\tcolors: dict[str, str]|None = None,\n",
    "\t\tpercentile_bounds: tuple[float, float] = (0.4, 0.6),\n",
    "\t):\n",
    "\n",
    "\tif colors is None:\n",
    "\t\tcolors = {\n",
    "\t\t\tfunc_name: f\"C{i}\"\n",
    "\t\t\tfor i, func_name in enumerate(data.keys())\n",
    "\t\t}\n",
    "\n",
    "\tfig, ax = plt.subplots(len(data), 1, figsize = (8, 4 * len(data)))\n",
    "\tfig.subplots_adjust(hspace = 0.5)\n",
    "\t\t\n",
    "\tfor i, (name, scores_idxed) in enumerate(data.items()):\n",
    "\t\tx = list(scores_idxed.keys())\n",
    "\t\ty = [\n",
    "\t\t\tscores_idxed[idx].median()\n",
    "\t\t\tfor idx in x\n",
    "\t\t]\n",
    "\t\tax[i].plot(x, y, label=name, color=colors[name])\n",
    "\t\t# plot shaded error bars\n",
    "\t\ty_ub = [\n",
    "\t\t\tscores_idxed[idx].percentile(percentile_bounds[1])\n",
    "\t\t\tfor idx in x\n",
    "\t\t]\n",
    "\t\ty_lb = [\n",
    "\t\t\tscores_idxed[idx].percentile(percentile_bounds[0])\n",
    "\t\t\tfor idx in x\n",
    "\t\t]\n",
    "\t\tax[i].fill_between(\n",
    "\t\t\tx, y_lb, y_ub,\n",
    "\t    \talpha=0.5, \n",
    "\t\t\tedgecolor=colors[name], facecolor=colors[name],\n",
    "\t\t)\n",
    "\n",
    "\t\tax[i].set_title(f\"{name}, {percentile_bounds = }\")\n",
    "\t\tax[i].set_xlabel(\"Checkpoint\")\n",
    "\t\tax[i].set_ylabel(\"score\")\n",
    "\n",
    "\tplt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_pathdist_scores(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62905ad46d100ac3585b269e697bed797f99f9269c8442add34dc9bf06a84d9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}