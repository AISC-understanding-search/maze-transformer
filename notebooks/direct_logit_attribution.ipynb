{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logit Lens and Direct Logit Attribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this notebook, we will apply direct logit attribution to our maze-transformer model. \n",
    "\n",
    "This is mostly just getting the data in an appropriate format and using techniques from Neel's explanatory analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mivan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-transformer-2cGx2R0F-py3.10\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "# Generic\n",
    "import os\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import typing\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "import torch\n",
    "# import torch.nn.functional as F\n",
    "from fancy_einsum import einsum\n",
    "import einops\n",
    "from jaxtyping import Float, Int, Bool\n",
    "\n",
    "from muutils.misc import shorten_numerical_to_str\n",
    "from muutils.nbutils.configure_notebook import configure_notebook\n",
    "# TransformerLens imports\n",
    "from transformer_lens import ActivationCache\n",
    "\n",
    "# Our Code\n",
    "from maze_dataset import MazeDataset, MazeDatasetConfig, SolvedMaze, LatticeMaze, SPECIAL_TOKENS\n",
    "from maze_dataset.tokenization import MazeTokenizer, TokenizationMode\n",
    "from maze_dataset.plotting.print_tokens import color_maze_tokens_AOTP\n",
    "\n",
    "from maze_transformer.tokenizer import HuggingMazeTokenizer\n",
    "from maze_transformer.training.config import ConfigHolder, ZanjHookedTransformer, BaseGPTConfig\n",
    "from maze_transformer.evaluation.eval_model import  load_model_with_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = device(type='cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x26f86fd14e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup (we won't be training any models)\n",
    "DEVICE: torch.device = configure_notebook(seed=42, dark_mode=True)\n",
    "print(f\"{DEVICE = }\")\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loading the model in\n",
    "\n",
    "Loading in Alex's successfully trained model, it uses an embedding dimension of 384 with 6 heads and 12 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model with 1.3M params (num_params = 1274699)\n"
     ]
    }
   ],
   "source": [
    "MODEL: ZanjHookedTransformer = ZanjHookedTransformer.read(\"../examples/hallway-medium_2023-06-16-03-40-47.iter_26554.zanj\")\n",
    "num_params: int = MODEL.num_params()\n",
    "print(f\"loaded model with {shorten_numerical_to_str(num_params)} params ({num_params = })\")\n",
    "GPT_CONFIG: BaseGPTConfig = MODEL.zanj_model_config.model_cfg\n",
    "TOKENIZER: MazeTokenizer = MODEL.zanj_model_config.maze_tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset Creation\n",
    "\n",
    "Creating a collection of mazes to have the model predict on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 100 mazes\n",
      "first maze:\n",
      "<ADJLIST_START> (6,4) <--> (6,3) ; (4,7) <--> (3,7) ; (3,4) <--> (3,5) ; (3,3) <--> (4,3) ; (4,4) <--> (4,5) ; (3,7) <--> (3,6) ; (3,3) <--> (3,4) ; (4,7) <--> (4,6) ; (4,2) <--> (4,1) ; (6,5) <--> (5,5) ; (3,5) <--> (3,6) ; (4,6) <--> (4,5) ; (5,1) <--> (4,1) ; (6,4) <--> (6,5) ; (5,1) <--> (5,2) ; (5,2) <--> (5,3) ; (5,3) <--> (5,4) ; (4,3) <--> (4,2) ; (5,4) <--> (5,5) ; <ADJLIST_END> <ORIGIN_START> (3,4) <ORIGIN_END> <TARGET_START> (4,1) <TARGET_END> <PATH_START> (3,4) (3,3) (4,3) (4,2) (4,1) <PATH_END>\n",
      "first maze, colored:\n",
      "\u001b[30m\u001b[48;2;234;209;220m<ADJLIST_START> (6,4) <--> (6,3) ; (4,7) <--> (3,7) ; (3,4) <--> (3,5) ; (3,3) <--> (4,3) ; (4,4) <--> (4,5) ; (3,7) <--> (3,6) ; (3,3) <--> (3,4) ; (4,7) <--> (4,6) ; (4,2) <--> (4,1) ; (6,5) <--> (5,5) ; (3,5) <--> (3,6) ; (4,6) <--> (4,5) ; (5,1) <--> (4,1) ; (6,4) <--> (6,5) ; (5,1) <--> (5,2) ; (5,2) <--> (5,3) ; (5,3) <--> (5,4) ; (4,3) <--> (4,2) ; (5,4) <--> (5,5) ; <ADJLIST_END>\u001b[0m \u001b[30m\u001b[48;2;217;210;233m<ORIGIN_START> (3,4) <ORIGIN_END>\u001b[0m \u001b[30m\u001b[48;2;207;226;243m<TARGET_START> (4,1) <TARGET_END>\u001b[0m \u001b[30m\u001b[48;2;217;234;211m<PATH_START> (3,4) (3,3) (4,3) (4,2) (4,1) <PATH_END>\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get 100 mazes and pass into model, storing logits and cache\n",
    "n_examples: int = 100\n",
    "TEST_DATASET_CFG: MazeDatasetConfig = deepcopy(MODEL.zanj_model_config.dataset_cfg)\n",
    "TEST_DATASET_CFG.n_mazes = n_examples\n",
    "# hacky: adjust things to work\n",
    "for filter in TEST_DATASET_CFG.applied_filters:\n",
    "\tif len(filter[\"args\"]) == 0:\n",
    "\t\tfilter[\"args\"] = tuple()\n",
    "\n",
    "DATASET: MazeDataset = MazeDataset.from_config(TEST_DATASET_CFG)\n",
    "DATASET_TOKENS: list[list[str]] = DATASET.as_tokens(TOKENIZER, join_tokens_individual_maze=False)\n",
    "\n",
    "print(f\"loaded {len(DATASET_TOKENS)} mazes\")\n",
    "print(f\"first maze:\\n{' '.join(DATASET_TOKENS[0])}\")\n",
    "print(f\"first maze, colored:\\n{color_maze_tokens_AOTP(DATASET_TOKENS[0], fmt='terminal')}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the data up to the path start token, as well as the path start tokens themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_token_first_index: typing.Callable[[str, list[str]], int] = lambda search_token, token_list: token_list.index(search_token)\n",
    "def get_token_first_index(search_token: str, token_list: list[str]) -> int:\n",
    "\treturn token_list.index(search_token)\n",
    "\n",
    "TaskSetup = typing.NamedTuple(\"TaskSetup\", [\n",
    "\t(\"prompts\", list[list[str]]),\n",
    "\t(\"targets\", str),\n",
    "])\n",
    "\n",
    "class DLAProtocol(typing.Protocol):\n",
    "\t\"\"\"should take a dataset, and return a tuple of (prompts, targets)\"\"\"\n",
    "\tdef __call__(self, dataset_tokens: list[list[str]], **kwargs) -> TaskSetup:\n",
    "\t\t...\n",
    "\n",
    "\n",
    "def setup_task_starttoken(\n",
    "\tdataset_tokens: list[list[str]], \n",
    "\tstart_token: str,\n",
    ") -> TaskSetup:\n",
    "\n",
    "\tprompts: list[list[str]] = list()\n",
    "\ttargets: list[str] = list()\n",
    "\n",
    "\tfor maze_tokens in DATASET_TOKENS:\n",
    "\t\tpath_start_idx: int = get_token_first_index(SPECIAL_TOKENS.PATH_START, maze_tokens)\n",
    "\t\tprompt_tokens: list[str] = maze_tokens[:path_start_idx + 1]\n",
    "\t\tprompts.append(prompt_tokens)\n",
    "\t\ttargets.append(maze_tokens[path_start_idx + 1])\n",
    "\n",
    "\treturn TaskSetup(prompts=prompts, targets=targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first maze prompt:\n",
      "[...] 5,3) <--> (5,4) ; (4,3) <--> (4,2) ; (5,4) <--> (5,5) ; <ADJLIST_END> <ORIGIN_START> (3,4) <ORIGIN_END> <TARGET_START> (4,1) <TARGET_END> <PATH_START>\n",
      "first maze target:\n",
      "(3,4)\n",
      "first maze target id:\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "DATASET_PROMPTS: list[list[str]]\n",
    "DATASET_PROMPTS_JOINED: list[str]\n",
    "DATASET_TARGETS: list[str]\n",
    "DATASET_TARGET_IDS: Float[torch.Tensor, \"n_mazes\"]\n",
    "\n",
    "DATASET_PROMPTS, DATASET_TARGETS = setup_task_starttoken(DATASET_TOKENS, SPECIAL_TOKENS.PATH_START)\n",
    "DATASET_PROMPTS_JOINED = [\" \".join(prompt) for prompt in DATASET_PROMPTS]\n",
    "DATASET_TARGET_IDS = torch.tensor(TOKENIZER.encode(DATASET_TARGETS), dtype=torch.long)\n",
    "\n",
    "print(f\"first maze prompt:\\n{'[...] ' + DATASET_PROMPTS_JOINED[0][-150:]}\")\n",
    "print(f\"first maze target:\\n{DATASET_TARGETS[0]}\")\n",
    "print(f\"first maze target id:\\n{DATASET_TARGET_IDS[0]}\")\n",
    "\n",
    "n_mazes: int = len(DATASET_TOKENS)\n",
    "d_vocab: int = TOKENIZER.vocab_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGITS: Float[torch.Tensor, \"n_mazes seq_len d_vocab\"]\n",
    "CACHE: ActivationCache\n",
    "\n",
    "LOGITS, CACHE = MODEL.run_with_cache(DATASET_PROMPTS_JOINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_mazes = 100, d_vocab = 75\n",
      "LOGITS.shape = torch.Size([100, 238, 75])\n",
      "cache_shapes = {'hook_embed': torch.Size([100, 238, 128]), 'hook_pos_embed': torch.Size([100, 238, 128]), 'blocks.0.hook_resid_pre': torch.Size([100, 238, 128]), 'blocks.0.ln1.hook_scale': torch.Size([100, 238, 1]), 'blocks.0.ln1.hook_normalized': torch.Size([100, 238, 128]), 'blocks.0.attn.hook_q': torch.Size([100, 238, 4, 32]), 'blocks.0.attn.hook_k': torch.Size([100, 238, 4, 32]), 'blocks.0.attn.hook_v': torch.Size([100, 238, 4, 32]), 'blocks.0.attn.hook_attn_scores': torch.Size([100, 4, 238, 238]), 'blocks.0.attn.hook_pattern': torch.Size([100, 4, 238, 238]), 'blocks.0.attn.hook_z': torch.Size([100, 238, 4, 32]), 'blocks.0.hook_attn_out': torch.Size([100, 238, 128]), 'blocks.0.hook_resid_mid': torch.Size([100, 238, 128]), 'blocks.0.ln2.hook_scale': torch.Size([100, 238, 1]), 'blocks.0.ln2.hook_normalized': torch.Size([100, 238, 128]), 'blocks.0.mlp.hook_pre': torch.Size([100, 238, 512]), 'blocks.0.mlp.hook_post': torch.Size([100, 238, 512]), 'blocks.0.hook_mlp_out': torch.Size([100, 238, 128]), 'blocks.0.hook_resid_post': torch.Size([100, 238, 128]), 'blocks.1.hook_resid_pre': torch.Size([100, 238, 128]), 'blocks.1.ln1.hook_scale': torch.Size([100, 238, 1]), 'blocks.1.ln1.hook_normalized': torch.Size([100, 238, 128]), 'blocks.1.attn.hook_q': torch.Size([100, 238, 4, 32]), 'blocks.1.attn.hook_k': torch.Size([100, 238, 4, 32]), 'blocks.1.attn.hook_v': torch.Size([100, 238, 4, 32]), 'blocks.1.attn.hook_attn_scores': torch.Size([100, 4, 238, 238]), 'blocks.1.attn.hook_pattern': torch.Size([100, 4, 238, 238]), 'blocks.1.attn.hook_z': torch.Size([100, 238, 4, 32]), 'blocks.1.hook_attn_out': torch.Size([100, 238, 128]), 'blocks.1.hook_resid_mid': torch.Size([100, 238, 128]), 'blocks.1.ln2.hook_scale': torch.Size([100, 238, 1]), 'blocks.1.ln2.hook_normalized': torch.Size([100, 238, 128]), 'blocks.1.mlp.hook_pre': torch.Size([100, 238, 512]), 'blocks.1.mlp.hook_post': torch.Size([100, 238, 512]), 'blocks.1.hook_mlp_out': torch.Size([100, 238, 128]), 'blocks.1.hook_resid_post': torch.Size([100, 238, 128]), 'blocks.2.hook_resid_pre': torch.Size([100, 238, 128]), 'blocks.2.ln1.hook_scale': torch.Size([100, 238, 1]), 'blocks.2.ln1.hook_normalized': torch.Size([100, 238, 128]), 'blocks.2.attn.hook_q': torch.Size([100, 238, 4, 32]), 'blocks.2.attn.hook_k': torch.Size([100, 238, 4, 32]), 'blocks.2.attn.hook_v': torch.Size([100, 238, 4, 32]), 'blocks.2.attn.hook_attn_scores': torch.Size([100, 4, 238, 238]), 'blocks.2.attn.hook_pattern': torch.Size([100, 4, 238, 238]), 'blocks.2.attn.hook_z': torch.Size([100, 238, 4, 32]), 'blocks.2.hook_attn_out': torch.Size([100, 238, 128]), 'blocks.2.hook_resid_mid': torch.Size([100, 238, 128]), 'blocks.2.ln2.hook_scale': torch.Size([100, 238, 1]), 'blocks.2.ln2.hook_normalized': torch.Size([100, 238, 128]), 'blocks.2.mlp.hook_pre': torch.Size([100, 238, 512]), 'blocks.2.mlp.hook_post': torch.Size([100, 238, 512]), 'blocks.2.hook_mlp_out': torch.Size([100, 238, 128]), 'blocks.2.hook_resid_post': torch.Size([100, 238, 128]), 'blocks.3.hook_resid_pre': torch.Size([100, 238, 128]), 'blocks.3.ln1.hook_scale': torch.Size([100, 238, 1]), 'blocks.3.ln1.hook_normalized': torch.Size([100, 238, 128]), 'blocks.3.attn.hook_q': torch.Size([100, 238, 4, 32]), 'blocks.3.attn.hook_k': torch.Size([100, 238, 4, 32]), 'blocks.3.attn.hook_v': torch.Size([100, 238, 4, 32]), 'blocks.3.attn.hook_attn_scores': torch.Size([100, 4, 238, 238]), 'blocks.3.attn.hook_pattern': torch.Size([100, 4, 238, 238]), 'blocks.3.attn.hook_z': torch.Size([100, 238, 4, 32]), 'blocks.3.hook_attn_out': torch.Size([100, 238, 128]), 'blocks.3.hook_resid_mid': torch.Size([100, 238, 128]), 'blocks.3.ln2.hook_scale': torch.Size([100, 238, 1]), 'blocks.3.ln2.hook_normalized': torch.Size([100, 238, 128]), 'blocks.3.mlp.hook_pre': torch.Size([100, 238, 512]), 'blocks.3.mlp.hook_post': torch.Size([100, 238, 512]), 'blocks.3.hook_mlp_out': torch.Size([100, 238, 128]), 'blocks.3.hook_resid_post': torch.Size([100, 238, 128]), 'blocks.4.hook_resid_pre': torch.Size([100, 238, 128]), 'blocks.4.ln1.hook_scale': torch.Size([100, 238, 1]), 'blocks.4.ln1.hook_normalized': torch.Size([100, 238, 128]), 'blocks.4.attn.hook_q': torch.Size([100, 238, 4, 32]), 'blocks.4.attn.hook_k': torch.Size([100, 238, 4, 32]), 'blocks.4.attn.hook_v': torch.Size([100, 238, 4, 32]), 'blocks.4.attn.hook_attn_scores': torch.Size([100, 4, 238, 238]), 'blocks.4.attn.hook_pattern': torch.Size([100, 4, 238, 238]), 'blocks.4.attn.hook_z': torch.Size([100, 238, 4, 32]), 'blocks.4.hook_attn_out': torch.Size([100, 238, 128]), 'blocks.4.hook_resid_mid': torch.Size([100, 238, 128]), 'blocks.4.ln2.hook_scale': torch.Size([100, 238, 1]), 'blocks.4.ln2.hook_normalized': torch.Size([100, 238, 128]), 'blocks.4.mlp.hook_pre': torch.Size([100, 238, 512]), 'blocks.4.mlp.hook_post': torch.Size([100, 238, 512]), 'blocks.4.hook_mlp_out': torch.Size([100, 238, 128]), 'blocks.4.hook_resid_post': torch.Size([100, 238, 128]), 'blocks.5.hook_resid_pre': torch.Size([100, 238, 128]), 'blocks.5.ln1.hook_scale': torch.Size([100, 238, 1]), 'blocks.5.ln1.hook_normalized': torch.Size([100, 238, 128]), 'blocks.5.attn.hook_q': torch.Size([100, 238, 4, 32]), 'blocks.5.attn.hook_k': torch.Size([100, 238, 4, 32]), 'blocks.5.attn.hook_v': torch.Size([100, 238, 4, 32]), 'blocks.5.attn.hook_attn_scores': torch.Size([100, 4, 238, 238]), 'blocks.5.attn.hook_pattern': torch.Size([100, 4, 238, 238]), 'blocks.5.attn.hook_z': torch.Size([100, 238, 4, 32]), 'blocks.5.hook_attn_out': torch.Size([100, 238, 128]), 'blocks.5.hook_resid_mid': torch.Size([100, 238, 128]), 'blocks.5.ln2.hook_scale': torch.Size([100, 238, 1]), 'blocks.5.ln2.hook_normalized': torch.Size([100, 238, 128]), 'blocks.5.mlp.hook_pre': torch.Size([100, 238, 512]), 'blocks.5.mlp.hook_post': torch.Size([100, 238, 512]), 'blocks.5.hook_mlp_out': torch.Size([100, 238, 128]), 'blocks.5.hook_resid_post': torch.Size([100, 238, 128]), 'ln_final.hook_scale': torch.Size([100, 238, 1]), 'ln_final.hook_normalized': torch.Size([100, 238, 128])}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{n_mazes = }, {d_vocab = }\")\n",
    "print(f\"{LOGITS.shape = }\")\n",
    "cache_shapes: dict[str, tuple[int, ...]] = {k: v.shape for k, v in CACHE.items()}\n",
    "print(f\"{cache_shapes = }\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we expect the logits to have shape `(n_mazes, n_tokens, n_vocab)`\n",
    "\n",
    "\n",
    "## get and evaluate predictions\n",
    "\n",
    "these should have shape `(n_mazes, n_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST_TOK_LOGITS.shape = torch.Size([100, 75])\n"
     ]
    }
   ],
   "source": [
    "LAST_TOK_LOGITS: Float[torch.Tensor, \"n_mazes d_vocab\"] = LOGITS[:, -1, :]\n",
    "print(f\"{LAST_TOK_LOGITS.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(PREDICTED_TOKENS) = 100\n",
      "DATASET_TARGETS[0] = '(3,4)'\n",
      "PREDICTED_TOKENS[0] = '(3,4)'\n"
     ]
    }
   ],
   "source": [
    "PREDICTED_TOKENS: list[str] = TOKENIZER.decode(LAST_TOK_LOGITS.argmax(dim=-1).tolist())\n",
    "print(f\"{len(PREDICTED_TOKENS) = }\")\n",
    "print(f\"{DATASET_TARGETS[0] = }\")\n",
    "print(f\"{PREDICTED_TOKENS[0] = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_correct.shape = torch.Size([100])\n",
      "prediction_correct.float().mean().item() = 1.0\n"
     ]
    }
   ],
   "source": [
    "prediction_correct: Bool[torch.Tensor, \"n_mazes\"] = torch.tensor([\n",
    "\tpred == target \n",
    "\tfor pred, target in zip(PREDICTED_TOKENS, DATASET_TARGETS)\n",
    "])\n",
    "\n",
    "print(f\"{prediction_correct.shape = }\")\n",
    "print(f\"{prediction_correct.float().mean().item() = }\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## directions in residual stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Neels explanatory notebook: https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/main/Exploratory_Analysis_Demo.ipynb\n",
    "def logits_to_ave_logit_diff(\n",
    "        final_logits: Float[torch.Tensor, \"n_mazes d_vocab\"],\n",
    "        answer_tokens: Int[torch.Tensor, \"n_mazes\"],\n",
    "    ) -> Float[torch.Tensor, \"n_mazes\"]:\n",
    "    print(f\"{final_logits.shape = }\")\n",
    "    print(f\"{answer_tokens.shape = }\")\n",
    "    # logit of the answer token for each sample\n",
    "    # answer_logits = torch.tensor([\n",
    "    #     final_logits[i, answer_tokens[i]]\n",
    "    #     for i in range(final_logits.shape[0])\n",
    "    # ])\n",
    "    answer_logits: Float[torch.Tensor, \"n_mazes\"] = torch.gather(final_logits, 1, answer_tokens.unsqueeze(1)).squeeze(1)\n",
    "    # logits of all other tokens for each sample\n",
    "    other_logits = torch.tensor([\n",
    "        final_logits[i, :answer_tokens[i]].sum() \n",
    "        + final_logits[i, answer_tokens[i] + 1:].sum()\n",
    "        for i in range(final_logits.shape[0])\n",
    "    ])\n",
    "    other_logits_sum: Float[torch.Tensor, \"n_mazes\"] = torch.sum(final_logits, dim=1) - answer_logits\n",
    "\n",
    "    print(f\"{answer_logits.shape = }\")\n",
    "\n",
    "    print(f\"{other_logits.shape = }\")\n",
    "    print(f\"{other_logits_sum.shape = }\")\n",
    "    print(f\"{torch.allclose(other_logits, other_logits_sum) = }\")\n",
    "\n",
    "    print(f\"{other_logits_sum.shape = }\")\n",
    "    # print(f\"{answer_logits = }\")\n",
    "    # answer_logit_diff = answer_logits[:, 0] - answer_logits[:, 1]\n",
    "    return answer_logits - other_logits_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_logits.shape = torch.Size([100, 75])\n",
      "answer_tokens.shape = torch.Size([100])\n",
      "answer_logits.shape = torch.Size([100])\n",
      "other_logits.shape = torch.Size([100])\n",
      "other_logits_sum.shape = torch.Size([100])\n",
      "torch.allclose(other_logits, other_logits_sum) = True\n",
      "other_logits_sum.shape = torch.Size([100])\n",
      "avg_logit_diff.shape = torch.Size([100])\n",
      "avg_logit_diff.mean().item() = 31.570226669311523\n"
     ]
    }
   ],
   "source": [
    "avg_logit_diff: Float[torch.Tensor, \"n_mazes\"] = logits_to_ave_logit_diff(final_logits=LAST_TOK_LOGITS, answer_tokens=DATASET_TARGET_IDS)\n",
    "\n",
    "print(f\"{avg_logit_diff.shape = }\")\n",
    "print(f\"{avg_logit_diff.mean().item() = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mazes_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mpad(tensor, padding, value\u001b[39m=\u001b[39mpad_value)\n\u001b[0;32m     28\u001b[0m \u001b[39m# Get a list of the <PATH_START> index for each maze example\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m path_start \u001b[39m=\u001b[39m token_id_find(mazes_tokens\u001b[39m=\u001b[39mmazes_tokens, specific_tok\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[39m# Using the <PATH_START> index, strip the tokenized maze of everything after <PATH_START> (so this is the last token)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m maze_only_tokens \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mazes_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "def token_id_find(mazes_tokens, specific_tok):\n",
    "    '''\n",
    "    Returns a list of token indexes for a specific token over a batch of mazes.\n",
    "    '''\n",
    "\n",
    "    path_start = []\n",
    "    for maze in mazes_tokens:\n",
    "        for idx, tok in enumerate(maze):\n",
    "            if tok == specific_tok:\n",
    "                path_start.append(idx)\n",
    "    \n",
    "    if len(path_start) > mazes_tokens.shape[0]:\n",
    "        return f'More than one intance of token in one or more mazes'\n",
    "    else:\n",
    "        return path_start\n",
    "\n",
    "def pad_tensor_to_shape(tensor, target_shape, pad_value=10):\n",
    "    '''\n",
    "    Used to pad the front of a tokenized maze so examples are all the same length\n",
    "    '''\n",
    "    padding = []\n",
    "    for i in range(len(tensor.shape)-1, -1, -1):\n",
    "        total_padding = target_shape[i] - tensor.shape[i]\n",
    "        padding.extend([total_padding, 0])\n",
    "\n",
    "    return F.pad(tensor, padding, value=pad_value)\n",
    "\n",
    "# Get a list of the <PATH_START> index for each maze example\n",
    "path_start = token_id_find(mazes_tokens=mazes_tokens, specific_tok=6)\n",
    "\n",
    "# Using the <PATH_START> index, strip the tokenized maze of everything after <PATH_START> (so this is the last token)\n",
    "maze_only_tokens = []\n",
    "for idx, tok in enumerate(path_start):\n",
    "    maze_only_tokens.append(mazes_tokens[idx][:tok+1])\n",
    "\n",
    "# Pad the front of this stripped tokenized maze so that they are all the same length\n",
    "padded_tokens = []\n",
    "for maze in maze_only_tokens:\n",
    "    padded_tokens.append(pad_tensor_to_shape(maze, (166,), 10))\n",
    "\n",
    "# Make this list of examples into a stacked tensor on the correct device and appropriate dtype\n",
    "padded_tokens = torch.stack(padded_tokens).long().to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Predictions\n",
    "Using the model to make predictions and caching associated activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Have the model predict on the maze examples, storing logits and activations in cache\n",
    "with torch.no_grad():\n",
    "\tlogits, cache = MODEL.run_with_cache(padded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# For this architecture, this should of length 208\n",
    "len(cache.cache_dict.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Logits are in shape [100, 166, 47] corresponding to batch = 100 (100 maze examples), sequence_length = 166 and vocab size = 47.\n",
    "\n",
    "We want to predict on the next token (the first path coordinate), thus logits associated with final token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the last token prediction from the model\n",
    "last_token_logits = logits[:, -1, :]\n",
    "predictions = []\n",
    "for sample in last_token_logits:\n",
    "    last_token_pred = torch.argmax(sample).item()\n",
    "    predictions.append(last_token_pred)\n",
    "predictions = torch.tensor(predictions)\n",
    "print(f'Prediction from first maze: {predictions[0]}, shape of predictions: {predictions.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Are the maze tokens the same?\n",
    "print(f'Are all mazes the same in the maze tokens dataset? {padded_tokens.all()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Measuring model performance\n",
    "\n",
    "**The rest of this notebook is mainly from https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/main/Exploratory_Analysis_Demo.ipynb just applying it to our mazes**\n",
    "\n",
    "`answer_tokens` is just a list of [correct, incorrect] tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Lets create an list of for these predictions in the [correct, incorrect] format.\n",
    "# The correct token is taken from the maze definition as first token after <PATH_START>, the incorrect token is set to the <TARGET>.\n",
    "\n",
    "answer_tokens = []\n",
    "for maze in mazes_tokens:\n",
    "    for idx, tok in enumerate(maze):\n",
    "        if tok == 6:\n",
    "            answer_tokens.append([maze[idx+1], maze[len(maze)-2]])\n",
    "    \n",
    "answer_tokens = torch.tensor(answer_tokens).to(device=device)\n",
    "answer_tokens.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It shows us the models performance by comparing the logits associated with a correct response minus those with an incorrect response in this case (1, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m         \u001b[39mreturn\u001b[39;00m answer_logit_diff\u001b[39m.\u001b[39mmean()\n\u001b[1;32m---> 12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPer prompt logit difference:\u001b[39m\u001b[39m\"\u001b[39m, logits_to_ave_logit_diff(logits, answer_tokens, per_prompt\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[0;32m     13\u001b[0m original_average_logit_diff \u001b[39m=\u001b[39m logits_to_ave_logit_diff(logits, answer_tokens)\n\u001b[0;32m     14\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAverage logit difference:\u001b[39m\u001b[39m\"\u001b[39m, logits_to_ave_logit_diff(logits, answer_tokens)\u001b[39m.\u001b[39mitem())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logits' is not defined"
     ]
    }
   ],
   "source": [
    "# From Neels explanatory notebook: https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/main/Exploratory_Analysis_Demo.ipynb\n",
    "def logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False):\n",
    "    # Only the final logits are relevant for the answer\n",
    "    final_logits = logits[:, -1, :]\n",
    "    answer_logits = final_logits.gather(dim=-1, index=answer_tokens)\n",
    "    answer_logit_diff = answer_logits[:, 0] - answer_logits[:, 1]\n",
    "    if per_prompt:\n",
    "        return answer_logit_diff\n",
    "    else:\n",
    "        return answer_logit_diff.mean()\n",
    "\n",
    "print(\"Per prompt logit difference:\", logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=True))\n",
    "original_average_logit_diff = logits_to_ave_logit_diff(logits, answer_tokens)\n",
    "print(\"Average logit difference:\", logits_to_ave_logit_diff(logits, answer_tokens).item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Mapping tokens into the model's residual stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)\n",
    "print(f\"Answer residual directions shape: {answer_residual_directions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logit_diff_directions = answer_residual_directions[:, 0] - answer_residual_directions[:, 1]\n",
    "print(f\"Logit diff directions shape: {logit_diff_directions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# cache the values at the end of the residual stream\n",
    "final_residual_stream = cache[\"resid_post\", -1]\n",
    "print(f\"Final reisudal stream shape: {final_residual_stream.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the final token resid stream values (like we did above with last_pred_token)\n",
    "final_token_residual_stream = final_residual_stream[:, -1, :]\n",
    "print(f'Final token residual stream value shape: {final_token_residual_stream.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Scaling the values in residual stream with layer norm\n",
    "scaled_final_token_residual_stream = cache.apply_ln_to_stack(final_token_residual_stream, layer = -1, pos_slice=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Average logit diff from residual stream method\n",
    "average_logit_diff = einsum(\"batch d_model, batch d_model -> \", scaled_final_token_residual_stream, logit_diff_directions)/len(answer_tokens)\n",
    "print(\"Calculated average logit diff:\", average_logit_diff.item())\n",
    "print(\"Original logit difference:\",original_average_logit_diff.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "These match quite closely meaning that the residual stream has been correctly scaled."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Logit Lens\n",
    "\n",
    "This implementation is directly from Neel's Exploratory Analysis notebook, found here:\n",
    "https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/main/Exploratory_Analysis_Demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def residual_stack_to_logit_diff(residual_stack: Float[torch.Tensor, \"components batch d_model\"], cache: ActivationCache) -> float:\n",
    "    scaled_residual_stack = cache.apply_ln_to_stack(residual_stack, layer = -1, pos_slice=-1)\n",
    "    return einsum(\"... batch d_model, batch d_model -> ...\", scaled_residual_stack, logit_diff_directions)/len(answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accumulated_residual, labels = cache.accumulated_resid(layer=-1, incl_mid=True, pos_slice=-1, return_labels=True)\n",
    "logit_lens_logit_diffs = residual_stack_to_logit_diff(accumulated_residual, cache)\n",
    "logit_lens_cpu = logit_lens_logit_diffs.to(\"cpu\")\n",
    "y = logit_lens_cpu.numpy()\n",
    "x = np.arange(GPT_CONFIG.n_layers*2+1)/2\n",
    "print(type(x), type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Logit Difference from Accumulated Residual Stream\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Layer Attribution\n",
    "per_layer_residual, labels = cache.decompose_resid(layer=-1, pos_slice=-1, return_labels=True)\n",
    "per_layer_logit_diffs = residual_stack_to_logit_diff(per_layer_residual, cache)\n",
    "y = per_layer_logit_diffs.to(\"cpu\").numpy()\n",
    "x = np.arange(len(y))\n",
    "print(type(x), type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Logit Difference for each layer\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Direct Logit Attribution\n",
    "Again from Neel's exploratory analysis notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "per_head_residual, labels = cache.stack_head_results(layer=-1, pos_slice=-1, return_labels=True)\n",
    "per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, cache)\n",
    "per_head_logit_diffs = einops.rearrange(per_head_logit_diffs, \"(layer head_index) -> layer head_index\", layer=GPT_CONFIG.n_layers, head_index=GPT_CONFIG.n_heads)\n",
    "data = per_head_logit_diffs.to(\"cpu\").numpy()\n",
    "plt.imshow(data, cmap = \"RdBu\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Logit Difference from each head\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maze-transformer",
   "language": "python",
   "name": "maze-transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
