{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic\n",
    "import typing\n",
    "import os\n",
    "from pathlib import Path\n",
    "import typing\n",
    "import json\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# muutils\n",
    "from muutils.nbutils.configure_notebook import configure_notebook\n",
    "from zanj.zanj import ZANJ, ZANJ_GLOBAL_DEFAULTS\n",
    "\n",
    "# maze-dataset\n",
    "from maze_dataset.generation import LatticeMazeGenerators\n",
    "from maze_dataset import MazeDataset, MazeDatasetConfig\n",
    "from maze_dataset.dataset.configs import MAZE_DATASET_CONFIGS\n",
    "\n",
    "# maze-transformer\n",
    "from maze_transformer.training.config import ConfigHolder, ZanjHookedTransformer, BaseGPTConfig, TrainConfig\n",
    "from maze_transformer.training.train_model import TrainingResult, train_model\n",
    "from maze_transformer.training.wandb_logger import WandbProject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set up plots with PLOT_MODE = 'inline', FIG_OUTPUT_FMT = None, FIG_BASEPATH = None\n",
      "DEVICE = device(type='cpu')\n"
     ]
    }
   ],
   "source": [
    "# set global defaults for ZANJ\n",
    "ZANJ_GLOBAL_DEFAULTS.external_array_threshold = 1024\n",
    "ZANJ_GLOBAL_DEFAULTS.external_list_threshold = 1024\n",
    "\n",
    "# paths\n",
    "PATH_EXAMPLES: Path = Path(\"../examples/\")\n",
    "PATH_DATA: Path = Path(\"../data/\")\n",
    "\n",
    "# reproducibility and device\n",
    "DEVICE = configure_notebook(seed=42, dark_mode=True)\n",
    "print(f\"{DEVICE = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list(MAZE_DATASET_CONFIGS.keys()) = ['test-g3-n5-a_dfs-h73257', 'demo_small-g3-n100-a_dfs-h44636', 'demo-g6-n10K-a_dfs-h50618']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{list(MAZE_DATASET_CONFIGS.keys()) = }\")\n",
    "\n",
    "# if you want to specify a custom config, you can do so here\n",
    "CFG_CUSTOM: ConfigHolder = ConfigHolder(\n",
    "    name = \"custom\",\n",
    "    dataset_cfg = MazeDatasetConfig(\n",
    "\t\tname=\"custom-dataset\",\n",
    "\t\tgrid_n=6,\n",
    "\t\tn_mazes=10000,\n",
    "\t\tmaze_ctor=LatticeMazeGenerators.gen_dfs,\n",
    "\t),\n",
    "    model_cfg = BaseGPTConfig(\n",
    "        name=\"custom-model\",\n",
    "        act_fn=\"gelu\",\n",
    "        d_model=8,\n",
    "        d_head=4,\n",
    "        n_layers=2,\n",
    "    ),\n",
    "    train_cfg = TrainConfig(\n",
    "        name=\"custom-train\",\n",
    "        optimizer=torch.optim.RMSprop,\n",
    "        optimizer_kwargs=dict(lr=0.0001),\n",
    "        batch_size=16,\n",
    "        dataloader_cfg=dict(\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "        ),\n",
    "        intervals_count=dict(\n",
    "            print_loss=100,\n",
    "            checkpoint=5,\n",
    "            eval_fast=10,\n",
    "            eval_slow=5,\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "CFG_HALLWAY: ConfigHolder = ConfigHolder(\n",
    "    name = \"hallway_v3\",\n",
    "    dataset_cfg = MazeDatasetConfig(\n",
    "\t\tname=\"hallway\",\n",
    "\t\tgrid_n=7,\n",
    "\t\tn_mazes=3_000_000,\n",
    "\t\tmaze_ctor=LatticeMazeGenerators.gen_dfs,\n",
    "        maze_ctor_kwargs=dict(\n",
    "            do_forks=False,\n",
    "        ),\n",
    "        applied_filters=[{'name': 'collect_generation_meta', 'args': (), 'kwargs': {}}],\n",
    "        seq_len_max=256,\n",
    "\t),\n",
    "    model_cfg = BaseGPTConfig(\n",
    "        name=\"custom-model\",\n",
    "        act_fn=\"gelu\",\n",
    "        d_model=128,\n",
    "        d_head=32,\n",
    "        n_layers=6,\n",
    "    ),\n",
    "    train_cfg = TrainConfig(\n",
    "        name=\"custom-train\",\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer_kwargs=dict(lr=0.0001),\n",
    "        batch_size=32,\n",
    "        dataloader_cfg=dict(\n",
    "            shuffle=False,\n",
    "            num_workers=8,\n",
    "            drop_last=False,\n",
    "        ),\n",
    "        intervals_count=dict(\n",
    "            print_loss=100,\n",
    "            checkpoint=20,\n",
    "            eval_fast=100,\n",
    "            eval_slow=50,\n",
    "        ),\n",
    "        validation_dataset_cfg=100,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for training a \"real\" demo model\n",
    "CFG_DEMO: ConfigHolder = ConfigHolder.get_config_multisource(\n",
    "    cfg_names=(\"demo-g6-n10K-a_dfs-h50618\", \"tiny-v1\", \"sweep-v1\"),\n",
    ")\n",
    "\n",
    "# this is smaller, for testing\n",
    "CFG_TEST: ConfigHolder = ConfigHolder.get_config_multisource(\n",
    "    cfg_names=(\"demo_small-g3-n100-a_dfs-h44636\", \"nano-v1\", \"test-v1\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is where to specify which config to actually use\n",
    "CFG: ConfigHolder = CFG_DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"multsrc_demo-g6-n10K-a_dfs-h50618_tiny-v1_sweep-v1\",\n",
      "  \"dataset_cfg\": {\n",
      "    \"name\": \"demo\",\n",
      "    \"fname\": \"demo-g6-n10K-a_dfs-h50618\",\n",
      "    \"sdc_hash\": 109846810483272090382151445746698648148887661707374334566910004327535649450618,\n",
      "    \"seed\": 42,\n",
      "    \"seq_len_min\": 1,\n",
      "    \"seq_len_max\": 512,\n",
      "    \"applied_filters\": [],\n",
      "    \"grid_n\": 6,\n",
      "    \"n_mazes\": 10000,\n",
      "    \"maze_ctor_name\": \"gen_dfs\",\n",
      "    \"maze_ctor_kwargs\": {},\n",
      "    \"endpoint_kwargs\": {}\n",
      "  },\n",
      "  \"model_cfg\": {\n",
      "    \"name\": \"tiny-v1\",\n",
      "    \"act_fn\": \"gelu\",\n",
      "    \"d_model\": 32,\n",
      "    \"d_head\": 16,\n",
      "    \"n_layers\": 4,\n",
      "    \"positional_embedding_type\": \"standard\",\n",
      "    \"weight_processing\": {\n",
      "      \"are_layernorms_folded\": false,\n",
      "      \"are_weights_processed\": false\n",
      "    },\n",
      "    \"n_heads\": 2\n",
      "  },\n",
      "  \"train_cfg\": {\n",
      "    \"name\": \"sweep-v1\",\n",
      "    \"optimizer\": \"AdamW\",\n",
      "    \"optimizer_kwargs\": {\n",
      "      \"lr\": 0.0001\n",
      "    },\n",
      "    \"batch_size\": 64,\n",
      "    \"dataloader_cfg\": {\n",
      "      \"shuffle\": true,\n",
      "      \"num_workers\": 8,\n",
      "      \"persistent_workers\": true,\n",
      "      \"drop_last\": true\n",
      "    },\n",
      "    \"intervals\": null,\n",
      "    \"intervals_count\": null,\n",
      "    \"evals_max_new_tokens\": 8,\n",
      "    \"validation_dataset_cfg\": 50\n",
      "  },\n",
      "  \"pretrainedtokenizer_kwargs\": null,\n",
      "  \"maze_tokenizer\": {\n",
      "    \"prompt_sequencer\": \"AOTP(UT(), AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=1), ConnectionEdges(walls=F), RandomCoords()), Unlabeled(post=F), StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=F, intra=F, post=F))\",\n",
      "    \"coord_tokenizer\": \"UT()\",\n",
      "    \"adj_list_tokenizer\": \"AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=1), ConnectionEdges(walls=F), RandomCoords())\",\n",
      "    \"edge_grouping\": \"Ungrouped(connection_token_ordinal=1)\",\n",
      "    \"edge_subset\": \"ConnectionEdges(walls=F)\",\n",
      "    \"edge_permuter\": \"RandomCoords()\",\n",
      "    \"target_tokenizer\": \"Unlabeled(post=F)\",\n",
      "    \"path_tokenizer\": \"StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=F, intra=F, post=F)\",\n",
      "    \"step_size\": \"Singles()\",\n",
      "    \"step_tokenizers\": \"Coord()\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(CFG.summary(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to get the dataset 'demo-g6-n10K-a_dfs-h50618'\n",
      "loading dataset from ../data/demo-g6-n10K-a_dfs-h50618.zanj\n",
      "load successful!\n",
      "Got dataset demo with 10000 items. output.cfg.to_fname() = 'demo-g6-n10K-a_dfs-h53138'\n"
     ]
    }
   ],
   "source": [
    "# get just the dataset, generating it if needed. \n",
    "# This step can be skipped if you set `do_generate_dataset=True` when calling `train_model`\n",
    "# or if the dataset in question already exists\n",
    "\n",
    "# load the dataset\n",
    "DATASET: MazeDataset = MazeDataset.from_config(CFG.dataset_cfg, verbose=True, local_base_path=PATH_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-21 12:21:40 ERROR Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: miv. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\KNC\\maze-transformer\\notebooks\\wandb\\run-20240821_122142-01x8r45d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/miv/understanding-search/runs/01x8r45d' target=\"_blank\">hardy-serenity-26</a></strong> to <a href='https://wandb.ai/miv/understanding-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/miv/understanding-search' target=\"_blank\">https://wandb.ai/miv/understanding-search</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/miv/understanding-search/runs/01x8r45d' target=\"_blank\">https://wandb.ai/miv/understanding-search/runs/01x8r45d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-21 12:21:46 INFO config ={'__format__': 'ConfigHolder(SerializableDataclass)', 'dataset_cfg': {'__format__': 'MazeDatasetConfig(SerializableDataclass)', 'name': 'demo', 'seq_len_min': 1, 'seq_len_max': 512, 'seed': 42, 'applied_filters': [], 'grid_n': 6, 'n_mazes': 10000, 'maze_ctor': {'__name__': 'gen_dfs', '__module__': 'maze_dataset.generation.generators', '__doc__': ['generate a lattice maze using depth first search, iterative', '', '        # Arguments', '        - `grid_shape: Coord`: the shape of the grid', '        - `lattice_dim: int`: the dimension of the lattice', '          (default: `2`)', '        - `accessible_cells: int | float |None`: the number of accessible cells in the maze. If `None`, defaults to the total number of cells in the grid. if a float, asserts it is <= 1 and treats it as a proportion of **total cells**', '            (default: `None`)', '        - `max_tree_depth: int | float | None`: the maximum depth of the tree. If `None`, defaults to `2 * accessible_cells`. if a float, asserts it is <= 1 and treats it as a proportion of the **sum of the grid shape**', '            (default: `None`)', '        - `do_forks: bool`: whether to allow forks in the maze. If `False`, the maze will be have no forks and will be a simple hallway.', '        - `start_coord: Coord | None`: the starting coordinate of the generation algorithm. If `None`, defaults to a random coordinate.', '', '        # algorithm', '        1. Choose the initial cell, mark it as visited and push it to the stack', '        2. While the stack is not empty', '                1. Pop a cell from the stack and make it a current cell', '                2. If the current cell has any neighbours which have not been visited', '                        1. Push the current cell to the stack', '                        2. Choose one of the unvisited neighbours', '                        3. Remove the wall between the current cell and the chosen cell', '                        4. Mark the chosen cell as visited and push it to the stack', '        '], 'source_code': ['    @staticmethod', '    def gen_dfs(', '        grid_shape: Coord,', '        lattice_dim: int = 2,', '        accessible_cells: int | float | None = None,', '        max_tree_depth: int | float | None = None,', '        do_forks: bool = True,', '        randomized_stack: bool = False,', '        start_coord: Coord | None = None,', '    ) -> LatticeMaze:', '        \"\"\"generate a lattice maze using depth first search, iterative', '', '        # Arguments', '        - `grid_shape: Coord`: the shape of the grid', '        - `lattice_dim: int`: the dimension of the lattice', '          (default: `2`)', '        - `accessible_cells: int | float |None`: the number of accessible cells in the maze. If `None`, defaults to the total number of cells in the grid. if a float, asserts it is <= 1 and treats it as a proportion of **total cells**', '            (default: `None`)', '        - `max_tree_depth: int | float | None`: the maximum depth of the tree. If `None`, defaults to `2 * accessible_cells`. if a float, asserts it is <= 1 and treats it as a proportion of the **sum of the grid shape**', '            (default: `None`)', '        - `do_forks: bool`: whether to allow forks in the maze. If `False`, the maze will be have no forks and will be a simple hallway.', '        - `start_coord: Coord | None`: the starting coordinate of the generation algorithm. If `None`, defaults to a random coordinate.', '', '        # algorithm', '        1. Choose the initial cell, mark it as visited and push it to the stack', '        2. While the stack is not empty', '                1. Pop a cell from the stack and make it a current cell', '                2. If the current cell has any neighbours which have not been visited', '                        1. Push the current cell to the stack', '                        2. Choose one of the unvisited neighbours', '                        3. Remove the wall between the current cell and the chosen cell', '                        4. Mark the chosen cell as visited and push it to the stack', '        \"\"\"', '', '        # Default values if no constraints have been passed', '        grid_shape: Coord = np.array(grid_shape)', '        n_total_cells: int = int(np.prod(grid_shape))', '', '        n_accessible_cells: int', '        if accessible_cells is None:', '            n_accessible_cells = n_total_cells', '        elif isinstance(accessible_cells, float):', '            assert (', '                accessible_cells <= 1', '            ), f\"accessible_cells must be an int (count) or a float in the range [0, 1] (proportion), got {accessible_cells}\"', '', '            n_accessible_cells = int(accessible_cells * n_total_cells)', '        else:', '            assert isinstance(accessible_cells, int)', '            n_accessible_cells = accessible_cells', '', '        if max_tree_depth is None:', '            max_tree_depth = (', '                2 * n_total_cells', '            )  # We define max tree depth counting from the start coord in two directions. Therefore we divide by two in the if clause for neighboring sites later and multiply by two here.', '        elif isinstance(max_tree_depth, float):', '            assert (', '                max_tree_depth <= 1', '            ), f\"max_tree_depth must be an int (count) or a float in the range [0, 1] (proportion), got {max_tree_depth}\"', '', '            max_tree_depth = int(max_tree_depth * np.sum(grid_shape))', '', '        # choose a random start coord', '        start_coord = _random_start_coord(grid_shape, start_coord)', '', '        # initialize the maze with no connections', '        connection_list: ConnectionList = np.zeros(', '            (lattice_dim, grid_shape[0], grid_shape[1]), dtype=np.bool_', '        )', '', '        # initialize the stack with the target coord', '        visited_cells: set[tuple[int, int]] = set()', '        visited_cells.add(tuple(start_coord))  # this wasnt a bug after all lol', '        stack: list[Coord] = [start_coord]', '', '        # initialize tree_depth_counter', '        current_tree_depth: int = 1', '', '        # loop until the stack is empty or n_connected_cells is reached', '        while stack and (len(visited_cells) < n_accessible_cells):', '            # get the current coord from the stack', '            current_coord: Coord', '            if randomized_stack:', '                current_coord = stack.pop(random.randint(0, len(stack) - 1))', '            else:', '                current_coord = stack.pop()', '', '            # filter neighbors by being within grid bounds and being unvisited', '            unvisited_neighbors_deltas: list[tuple[Coord, Coord]] = [', '                (neighbor, delta)', '                for neighbor, delta in zip(', '                    current_coord + NEIGHBORS_MASK, NEIGHBORS_MASK', '                )', '                if (', '                    (tuple(neighbor) not in visited_cells)', '                    and (0 <= neighbor[0] < grid_shape[0])', '                    and (0 <= neighbor[1] < grid_shape[1])', '                )', '            ]', '', \"            # don't continue if max_tree_depth/2 is already reached (divide by 2 because we can branch to multiple directions)\", '            if unvisited_neighbors_deltas and (', '                current_tree_depth <= max_tree_depth / 2', '            ):', \"                # if we want a maze without forks, simply don't add the current coord back to the stack\", '                if do_forks and (len(unvisited_neighbors_deltas) > 1):', '                    stack.append(current_coord)', '', '                # choose one of the unvisited neighbors', '                chosen_neighbor, delta = random.choice(unvisited_neighbors_deltas)', '', '                # add connection', '                dim: int = np.argmax(np.abs(delta))', '                # if positive, down/right from current coord', '                # if negative, up/left from current coord (down/right from neighbor)', '                clist_node: Coord = (', '                    current_coord if (delta.sum() > 0) else chosen_neighbor', '                )', '                connection_list[dim, clist_node[0], clist_node[1]] = True', '', '                # add to visited cells and stack', '                visited_cells.add(tuple(chosen_neighbor))', '                stack.append(chosen_neighbor)', '', '                # Update current tree depth', '                current_tree_depth += 1', '            else:', '                current_tree_depth -= 1', '', '        output = LatticeMaze(', '            connection_list=connection_list,', '            generation_meta=dict(', '                func_name=\"gen_dfs\",', '                grid_shape=grid_shape,', '                start_coord=start_coord,', '                n_accessible_cells=int(n_accessible_cells),', '                max_tree_depth=int(max_tree_depth),', \"                # oh my god this took so long to track down. its almost 5am and I've spent like 2 hours on this bug\", '                # it was checking that len(visited_cells) == n_accessible_cells, but this means that the maze is', '                # treated as fully connected even when it is most certainly not, causing solving the maze to break', '                fully_connected=bool(len(visited_cells) == n_total_cells),', '                visited_cells={tuple(int(x) for x in coord) for coord in visited_cells},', '            ),', '        )', '', '        return output']}, 'maze_ctor_kwargs': {}, 'endpoint_kwargs': {}, 'grid_shape': (6, 6)}, 'model_cfg': {'__format__': 'BaseGPTConfig(SerializableDataclass)', 'name': 'tiny-v1', 'act_fn': 'gelu', 'd_model': 32, 'd_head': 16, 'n_layers': 4, 'positional_embedding_type': 'standard', 'weight_processing': {'are_layernorms_folded': False, 'are_weights_processed': False}, 'n_heads': 2}, 'train_cfg': {'__format__': 'TrainConfig(SerializableDataclass)', 'name': 'sweep-v1', 'evals_max_new_tokens': 8, 'validation_dataset_cfg': 50, 'optimizer': 'AdamW', 'optimizer_kwargs': {'lr': 0.0001}, 'batch_size': 64, 'dataloader_cfg': {'shuffle': True, 'num_workers': 8, 'persistent_workers': True, 'drop_last': True}, 'intervals': None, 'intervals_count': None}, 'name': 'multsrc_demo-g6-n10K-a_dfs-h50618_tiny-v1_sweep-v1', 'pretrainedtokenizer_kwargs': None, 'maze_tokenizer': {'__format__': 'MazeTokenizerModular(SerializableDataclass)', 'prompt_sequencer': {'__format__': 'AOTP(SerializableDataclass)', 'coord_tokenizer': {'__format__': 'UT(SerializableDataclass)', '_type_': \"<class 'maze_dataset.tokenization.maze_tokenizer.CoordTokenizers.UT'>\"}, 'adj_list_tokenizer': {'__format__': 'AdjListCoord(SerializableDataclass)', 'pre': False, 'post': True, 'shuffle_d0': True, 'edge_grouping': {'__format__': 'Ungrouped(SerializableDataclass)', '_type_': \"<class 'maze_dataset.tokenization.maze_tokenizer.EdgeGroupings.Ungrouped'>\", 'connection_token_ordinal': 1}, 'edge_subset': {'__format__': 'ConnectionEdges(SerializableDataclass)', '_type_': \"<class 'maze_dataset.tokenization.maze_tokenizer.EdgeSubsets.ConnectionEdges'>\", 'walls': False}, 'edge_permuter': {'__format__': 'RandomCoords(SerializableDataclass)', '_type_': \"<class 'maze_dataset.tokenization.maze_tokenizer.EdgePermuters.RandomCoords'>\"}, '_type_': \"<class 'maze_dataset.tokenization.maze_tokenizer.AdjListTokenizers.AdjListCoord'>\"}, '_type_': \"<class 'maze_dataset.tokenization.maze_tokenizer.PromptSequencers.AOTP'>\", 'target_tokenizer': {'__format__': 'Unlabeled(SerializableDataclass)', '_type_': \"<class 'maze_dataset.tokenization.maze_tokenizer.TargetTokenizers.Unlabeled'>\", 'post': False}, 'path_tokenizer': {'__format__': 'StepSequence(SerializableDataclass)', '_type_': \"<class 'maze_dataset.tokenization.maze_tokenizer.PathTokenizers.StepSequence'>\", 'step_size': {'__format__': 'Singles(SerializableDataclass)', '_type_': \"<class 'maze_dataset.tokenization.maze_tokenizer.StepSizes.Singles'>\"}, 'step_tokenizers': [{'__format__': 'Coord(SerializableDataclass)', '_type_': \"<class 'maze_dataset.tokenization.maze_tokenizer.StepTokenizers.Coord'>\"}], 'pre': False, 'intra': False, 'post': False}}, 'tokenizer_element_tree_concrete': 'MazeTokenizerModular\\n\\tAOTP\\n\\t\\tUT\\n\\t\\tAdjListCoord\\n\\t\\t\\tUngrouped\\n\\t\\t\\tConnectionEdges\\n\\t\\t\\tRandomCoords\\n\\t\\tUnlabeled\\n\\t\\tStepSequence\\n\\t\\t\\tSingles\\n\\t\\t\\tCoord\\n', 'name': 'MazeTokenizerModular-AOTP(UT(), AdjListCoord(pre=F, post=T, shuffle_d0=T, Ungrouped(connection_token_ordinal=1), ConnectionEdges(walls=F), RandomCoords()), Unlabeled(post=F), StepSequence(Singles(), step_tokenizers=(Coord(), ), pre=F, intra=F, post=F))'}, '_tokenizer': 'None'}\n",
      "2024-08-21 12:21:46 INFO Initialized logger\n",
      "2024-08-21 12:21:46 INFO Summary logged, getting dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\KNC\\maze-transformer\\maze_transformer\\training\\train_model.py:140: UserWarning:\n",
      "\n",
      "dataset has different config than cfg.dataset_cfg, but the only difference is in applied_filters, so using passed dataset. This is due to fast dataset loading collecting generation metadata for performance reasons\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-21 12:21:46 INFO finished getting training dataset with 10000 samples\n",
      "2024-08-21 12:21:46 INFO got validation dataset by splitting training dataset into 9950 train and 50 validation samples\n",
      "2024-08-21 12:21:46 INFO Loaded 9950 sequences\n",
      "2024-08-21 12:21:46 INFO Creating dataloader\n",
      "2024-08-21 12:21:46 INFO finished dataloader, passing to train()\n",
      "2024-08-21 12:21:46 INFO Initializing model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mivan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-transformer-2cGx2R0F-py3.12\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning:\n",
      "\n",
      "`clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n",
      "2024-08-21 12:21:47 INFO Initializing optimizer\n",
      "2024-08-21 12:21:50 INFO will train for 155 batches, evals_enabled=True, with intervals: {'print_loss': 1, 'checkpoint': 15, 'eval_fast': 7, 'eval_slow': 15}\n",
      "2024-08-21 12:21:50 INFO Starting training\n",
      "2024-08-21 12:22:36 INFO Running evals: eval_fast\n",
      "2024-08-21 12:22:40 INFO Running evals: eval_slow\n",
      "2024-08-21 12:22:43 INFO iteration 0/155: loss=8.466\n",
      "2024-08-21 12:22:43 INFO Saving model checkpoint to ../data/multsrc_demo-g6-n10K-a_dfs-h50618_tiny-v1_sweep-v1_2024-08-21-12-21-39/checkpoints/model.iter_0.zanj\n",
      "2024-08-21 12:22:46 INFO iteration 1/155: loss=8.426\n",
      "2024-08-21 12:22:47 INFO iteration 2/155: loss=8.392\n",
      "2024-08-21 12:22:49 INFO iteration 3/155: loss=8.333\n",
      "2024-08-21 12:22:51 INFO iteration 4/155: loss=8.290\n",
      "2024-08-21 12:22:53 INFO iteration 5/155: loss=8.255\n",
      "2024-08-21 12:22:55 INFO iteration 6/155: loss=8.237\n",
      "2024-08-21 12:22:57 INFO Running evals: eval_fast\n",
      "2024-08-21 12:23:00 INFO iteration 7/155: loss=8.195\n",
      "2024-08-21 12:23:02 INFO iteration 8/155: loss=8.167\n",
      "2024-08-21 12:23:03 INFO iteration 9/155: loss=8.132\n",
      "2024-08-21 12:23:05 INFO iteration 10/155: loss=8.093\n",
      "2024-08-21 12:23:06 INFO iteration 11/155: loss=8.065\n",
      "2024-08-21 12:23:08 INFO iteration 12/155: loss=8.036\n",
      "2024-08-21 12:23:09 INFO iteration 13/155: loss=8.002\n",
      "2024-08-21 12:23:11 INFO Running evals: eval_fast\n",
      "2024-08-21 12:23:14 INFO iteration 14/155: loss=7.987\n",
      "2024-08-21 12:23:16 INFO Running evals: eval_slow\n",
      "2024-08-21 12:23:19 INFO iteration 15/155: loss=7.942\n",
      "2024-08-21 12:23:19 INFO Saving model checkpoint to ../data/multsrc_demo-g6-n10K-a_dfs-h50618_tiny-v1_sweep-v1_2024-08-21-12-21-39/checkpoints/model.iter_15.zanj\n",
      "2024-08-21 12:23:21 INFO iteration 16/155: loss=7.920\n",
      "2024-08-21 12:23:22 INFO iteration 17/155: loss=7.905\n",
      "2024-08-21 12:23:24 INFO iteration 18/155: loss=7.886\n",
      "2024-08-21 12:23:25 INFO iteration 19/155: loss=7.834\n",
      "2024-08-21 12:23:27 INFO iteration 20/155: loss=7.817\n",
      "2024-08-21 12:23:28 INFO Running evals: eval_fast\n",
      "2024-08-21 12:23:31 INFO iteration 21/155: loss=7.796\n",
      "2024-08-21 12:23:32 INFO iteration 22/155: loss=7.761\n",
      "2024-08-21 12:23:34 INFO iteration 23/155: loss=7.731\n",
      "2024-08-21 12:23:35 INFO iteration 24/155: loss=7.702\n",
      "2024-08-21 12:23:37 INFO iteration 25/155: loss=7.694\n",
      "2024-08-21 12:23:38 INFO iteration 26/155: loss=7.659\n",
      "2024-08-21 12:23:40 INFO iteration 27/155: loss=7.639\n",
      "2024-08-21 12:23:41 INFO Running evals: eval_fast\n",
      "2024-08-21 12:23:44 INFO iteration 28/155: loss=7.627\n",
      "2024-08-21 12:23:46 INFO iteration 29/155: loss=7.598\n",
      "2024-08-21 12:23:47 INFO Running evals: eval_slow\n",
      "2024-08-21 12:23:50 INFO iteration 30/155: loss=7.571\n",
      "2024-08-21 12:23:50 INFO Saving model checkpoint to ../data/multsrc_demo-g6-n10K-a_dfs-h50618_tiny-v1_sweep-v1_2024-08-21-12-21-39/checkpoints/model.iter_30.zanj\n",
      "2024-08-21 12:23:52 INFO iteration 31/155: loss=7.549\n",
      "2024-08-21 12:23:54 INFO iteration 32/155: loss=7.523\n",
      "2024-08-21 12:23:55 INFO iteration 33/155: loss=7.489\n",
      "2024-08-21 12:23:57 INFO iteration 34/155: loss=7.488\n",
      "2024-08-21 12:23:58 INFO Running evals: eval_fast\n",
      "2024-08-21 12:24:02 INFO iteration 35/155: loss=7.435\n",
      "2024-08-21 12:24:04 INFO iteration 36/155: loss=7.424\n",
      "2024-08-21 12:24:06 INFO iteration 37/155: loss=7.417\n",
      "2024-08-21 12:24:07 INFO iteration 38/155: loss=7.400\n",
      "2024-08-21 12:24:09 INFO iteration 39/155: loss=7.356\n",
      "2024-08-21 12:24:11 INFO iteration 40/155: loss=7.336\n",
      "2024-08-21 12:24:12 INFO iteration 41/155: loss=7.331\n",
      "2024-08-21 12:24:14 INFO Running evals: eval_fast\n",
      "2024-08-21 12:24:16 INFO iteration 42/155: loss=7.336\n",
      "2024-08-21 12:24:18 INFO iteration 43/155: loss=7.296\n",
      "2024-08-21 12:24:20 INFO iteration 44/155: loss=7.266\n",
      "2024-08-21 12:24:21 INFO Running evals: eval_slow\n",
      "2024-08-21 12:24:24 INFO iteration 45/155: loss=7.239\n",
      "2024-08-21 12:24:24 INFO Saving model checkpoint to ../data/multsrc_demo-g6-n10K-a_dfs-h50618_tiny-v1_sweep-v1_2024-08-21-12-21-39/checkpoints/model.iter_45.zanj\n",
      "2024-08-21 12:24:26 INFO iteration 46/155: loss=7.243\n",
      "2024-08-21 12:24:27 INFO iteration 47/155: loss=7.228\n",
      "2024-08-21 12:24:29 INFO iteration 48/155: loss=7.176\n",
      "2024-08-21 12:24:31 INFO Running evals: eval_fast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\KNC\\maze-transformer\\maze_transformer\\evaluation\\path_evals.py:98: RuntimeWarning:\n",
      "\n",
      "fraction_connections_adjacent_lattice called on path of length less than 2, retuning NaN\n",
      "prediction = array([[24, 25]])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-21 12:24:34 INFO iteration 49/155: loss=7.220\n",
      "2024-08-21 12:24:36 INFO iteration 50/155: loss=7.215\n",
      "2024-08-21 12:24:37 INFO iteration 51/155: loss=7.158\n",
      "2024-08-21 12:24:39 INFO iteration 52/155: loss=7.115\n",
      "2024-08-21 12:24:42 INFO iteration 53/155: loss=7.113\n",
      "2024-08-21 12:24:44 INFO iteration 54/155: loss=7.084\n",
      "2024-08-21 12:24:46 INFO iteration 55/155: loss=7.102\n",
      "2024-08-21 12:24:49 INFO Running evals: eval_fast\n",
      "2024-08-21 12:24:54 INFO iteration 56/155: loss=7.065\n",
      "2024-08-21 12:24:56 INFO iteration 57/155: loss=7.040\n",
      "2024-08-21 12:24:59 INFO iteration 58/155: loss=7.064\n",
      "2024-08-21 12:25:01 INFO iteration 59/155: loss=7.026\n",
      "2024-08-21 12:25:03 INFO Running evals: eval_slow\n",
      "2024-08-21 12:25:07 INFO iteration 60/155: loss=7.031\n",
      "2024-08-21 12:25:07 INFO Saving model checkpoint to ../data/multsrc_demo-g6-n10K-a_dfs-h50618_tiny-v1_sweep-v1_2024-08-21-12-21-39/checkpoints/model.iter_60.zanj\n",
      "2024-08-21 12:25:12 INFO iteration 61/155: loss=6.986\n",
      "2024-08-21 12:25:16 INFO iteration 62/155: loss=7.000\n",
      "2024-08-21 12:25:18 INFO Running evals: eval_fast\n",
      "2024-08-21 12:25:21 INFO iteration 63/155: loss=6.926\n",
      "2024-08-21 12:25:23 INFO iteration 64/155: loss=6.970\n",
      "2024-08-21 12:25:25 INFO iteration 65/155: loss=6.973\n",
      "2024-08-21 12:25:26 INFO iteration 66/155: loss=6.941\n",
      "2024-08-21 12:25:28 INFO iteration 67/155: loss=6.894\n",
      "2024-08-21 12:25:30 INFO iteration 68/155: loss=6.883\n",
      "2024-08-21 12:25:31 INFO iteration 69/155: loss=6.882\n",
      "2024-08-21 12:25:33 INFO Running evals: eval_fast\n",
      "2024-08-21 12:25:36 INFO iteration 70/155: loss=6.847\n",
      "2024-08-21 12:25:37 INFO iteration 71/155: loss=6.878\n",
      "2024-08-21 12:25:39 INFO iteration 72/155: loss=6.900\n",
      "2024-08-21 12:25:40 INFO iteration 73/155: loss=6.856\n",
      "2024-08-21 12:25:41 INFO iteration 74/155: loss=6.877\n",
      "2024-08-21 12:25:43 INFO Running evals: eval_slow\n",
      "2024-08-21 12:25:47 INFO iteration 75/155: loss=6.821\n",
      "2024-08-21 12:25:47 INFO Saving model checkpoint to ../data/multsrc_demo-g6-n10K-a_dfs-h50618_tiny-v1_sweep-v1_2024-08-21-12-21-39/checkpoints/model.iter_75.zanj\n",
      "2024-08-21 12:25:49 INFO iteration 76/155: loss=6.805\n",
      "2024-08-21 12:25:51 INFO Running evals: eval_fast\n",
      "2024-08-21 12:25:53 INFO iteration 77/155: loss=6.799\n",
      "2024-08-21 12:25:55 INFO iteration 78/155: loss=6.830\n",
      "2024-08-21 12:25:56 INFO iteration 79/155: loss=6.760\n",
      "2024-08-21 12:25:58 INFO iteration 80/155: loss=6.783\n",
      "2024-08-21 12:25:59 INFO iteration 81/155: loss=6.757\n",
      "2024-08-21 12:26:01 INFO iteration 82/155: loss=6.756\n",
      "2024-08-21 12:26:02 INFO iteration 83/155: loss=6.674\n",
      "2024-08-21 12:26:04 INFO Running evals: eval_fast\n",
      "2024-08-21 12:26:07 INFO iteration 84/155: loss=6.688\n",
      "2024-08-21 12:26:09 INFO iteration 85/155: loss=6.687\n",
      "2024-08-21 12:26:10 INFO iteration 86/155: loss=6.689\n",
      "2024-08-21 12:26:12 INFO iteration 87/155: loss=6.652\n",
      "2024-08-21 12:26:13 INFO iteration 88/155: loss=6.713\n",
      "2024-08-21 12:26:15 INFO iteration 89/155: loss=6.620\n",
      "2024-08-21 12:26:16 INFO Running evals: eval_slow\n",
      "2024-08-21 12:26:19 INFO iteration 90/155: loss=6.631\n",
      "2024-08-21 12:26:19 INFO Saving model checkpoint to ../data/multsrc_demo-g6-n10K-a_dfs-h50618_tiny-v1_sweep-v1_2024-08-21-12-21-39/checkpoints/model.iter_90.zanj\n",
      "2024-08-21 12:26:21 INFO Running evals: eval_fast\n",
      "2024-08-21 12:26:24 INFO iteration 91/155: loss=6.671\n",
      "2024-08-21 12:26:26 INFO iteration 92/155: loss=6.598\n",
      "2024-08-21 12:26:28 INFO iteration 93/155: loss=6.618\n",
      "2024-08-21 12:26:30 INFO iteration 94/155: loss=6.563\n",
      "2024-08-21 12:26:32 INFO iteration 95/155: loss=6.594\n",
      "2024-08-21 12:26:34 INFO iteration 96/155: loss=6.551\n",
      "2024-08-21 12:26:38 INFO iteration 97/155: loss=6.511\n",
      "2024-08-21 12:26:43 INFO Running evals: eval_fast\n",
      "2024-08-21 12:26:51 INFO iteration 98/155: loss=6.531\n",
      "2024-08-21 12:26:53 INFO iteration 99/155: loss=6.485\n",
      "2024-08-21 12:26:55 INFO iteration 100/155: loss=6.560\n",
      "2024-08-21 12:26:57 INFO iteration 101/155: loss=6.546\n",
      "2024-08-21 12:26:59 INFO iteration 102/155: loss=6.527\n",
      "2024-08-21 12:27:00 INFO iteration 103/155: loss=6.511\n",
      "2024-08-21 12:27:03 INFO iteration 104/155: loss=6.505\n",
      "2024-08-21 12:27:04 INFO Running evals: eval_fast\n",
      "2024-08-21 12:27:07 INFO Running evals: eval_slow\n",
      "2024-08-21 12:27:10 INFO iteration 105/155: loss=6.484\n",
      "2024-08-21 12:27:10 INFO Saving model checkpoint to ../data/multsrc_demo-g6-n10K-a_dfs-h50618_tiny-v1_sweep-v1_2024-08-21-12-21-39/checkpoints/model.iter_105.zanj\n",
      "2024-08-21 12:27:12 INFO iteration 106/155: loss=6.482\n",
      "2024-08-21 12:27:13 INFO iteration 107/155: loss=6.459\n",
      "2024-08-21 12:27:14 INFO iteration 108/155: loss=6.480\n",
      "2024-08-21 12:27:16 INFO iteration 109/155: loss=6.464\n",
      "2024-08-21 12:27:17 INFO iteration 110/155: loss=6.468\n",
      "2024-08-21 12:27:19 INFO iteration 111/155: loss=6.324\n",
      "2024-08-21 12:27:20 INFO Running evals: eval_fast\n",
      "2024-08-21 12:27:23 INFO iteration 112/155: loss=6.339\n",
      "2024-08-21 12:27:24 INFO iteration 113/155: loss=6.378\n",
      "2024-08-21 12:27:26 INFO iteration 114/155: loss=6.426\n",
      "2024-08-21 12:27:27 INFO iteration 115/155: loss=6.343\n",
      "2024-08-21 12:27:29 INFO iteration 116/155: loss=6.375\n",
      "2024-08-21 12:27:30 INFO iteration 117/155: loss=6.387\n",
      "2024-08-21 12:27:31 INFO iteration 118/155: loss=6.413\n",
      "2024-08-21 12:27:33 INFO Running evals: eval_fast\n",
      "2024-08-21 12:27:35 INFO iteration 119/155: loss=6.317\n",
      "2024-08-21 12:27:37 INFO Running evals: eval_slow\n",
      "2024-08-21 12:27:40 INFO iteration 120/155: loss=6.317\n",
      "2024-08-21 12:27:40 INFO Saving model checkpoint to ../data/multsrc_demo-g6-n10K-a_dfs-h50618_tiny-v1_sweep-v1_2024-08-21-12-21-39/checkpoints/model.iter_120.zanj\n",
      "2024-08-21 12:27:42 INFO iteration 121/155: loss=6.237\n",
      "2024-08-21 12:27:45 INFO iteration 122/155: loss=6.345\n",
      "2024-08-21 12:27:47 INFO iteration 123/155: loss=6.282\n",
      "2024-08-21 12:27:49 INFO iteration 124/155: loss=6.311\n",
      "2024-08-21 12:27:50 INFO iteration 125/155: loss=6.274\n",
      "2024-08-21 12:27:52 INFO Running evals: eval_fast\n",
      "2024-08-21 12:27:56 INFO iteration 126/155: loss=6.327\n",
      "2024-08-21 12:27:58 INFO iteration 127/155: loss=6.220\n",
      "2024-08-21 12:28:00 INFO iteration 128/155: loss=6.253\n",
      "2024-08-21 12:28:02 INFO iteration 129/155: loss=6.252\n",
      "2024-08-21 12:28:04 INFO iteration 130/155: loss=6.280\n",
      "2024-08-21 12:28:06 INFO iteration 131/155: loss=6.200\n",
      "2024-08-21 12:28:07 INFO iteration 132/155: loss=6.245\n",
      "2024-08-21 12:28:09 INFO Running evals: eval_fast\n",
      "2024-08-21 12:28:11 INFO iteration 133/155: loss=6.280\n",
      "2024-08-21 12:28:13 INFO iteration 134/155: loss=6.197\n",
      "2024-08-21 12:28:14 INFO Running evals: eval_slow\n",
      "2024-08-21 12:28:17 INFO iteration 135/155: loss=6.196\n",
      "2024-08-21 12:28:17 INFO Saving model checkpoint to ../data/multsrc_demo-g6-n10K-a_dfs-h50618_tiny-v1_sweep-v1_2024-08-21-12-21-39/checkpoints/model.iter_135.zanj\n",
      "2024-08-21 12:28:19 INFO iteration 136/155: loss=6.091\n",
      "2024-08-21 12:28:20 INFO iteration 137/155: loss=6.234\n",
      "2024-08-21 12:28:22 INFO iteration 138/155: loss=6.146\n",
      "2024-08-21 12:28:23 INFO iteration 139/155: loss=6.217\n",
      "2024-08-21 12:28:25 INFO Running evals: eval_fast\n",
      "2024-08-21 12:28:29 INFO iteration 140/155: loss=6.154\n",
      "2024-08-21 12:28:31 INFO iteration 141/155: loss=6.204\n",
      "2024-08-21 12:28:32 INFO iteration 142/155: loss=6.198\n",
      "2024-08-21 12:28:34 INFO iteration 143/155: loss=6.201\n",
      "2024-08-21 12:28:35 INFO iteration 144/155: loss=6.145\n",
      "2024-08-21 12:28:37 INFO iteration 145/155: loss=6.144\n",
      "2024-08-21 12:28:39 INFO iteration 146/155: loss=6.203\n",
      "2024-08-21 12:28:40 INFO Running evals: eval_fast\n",
      "2024-08-21 12:28:43 INFO iteration 147/155: loss=6.085\n",
      "2024-08-21 12:28:44 INFO iteration 148/155: loss=6.081\n",
      "2024-08-21 12:28:46 INFO iteration 149/155: loss=6.086\n",
      "2024-08-21 12:28:48 INFO Running evals: eval_slow\n",
      "2024-08-21 12:28:51 INFO iteration 150/155: loss=6.045\n",
      "2024-08-21 12:28:51 INFO Saving model checkpoint to ../data/multsrc_demo-g6-n10K-a_dfs-h50618_tiny-v1_sweep-v1_2024-08-21-12-21-39/checkpoints/model.iter_150.zanj\n",
      "2024-08-21 12:28:53 INFO iteration 151/155: loss=6.129\n",
      "2024-08-21 12:28:55 INFO iteration 152/155: loss=6.097\n",
      "2024-08-21 12:28:57 INFO iteration 153/155: loss=6.058\n",
      "2024-08-21 12:28:58 INFO Running evals: eval_fast\n",
      "2024-08-21 12:29:02 INFO iteration 154/155: loss=6.003\n",
      "2024-08-21 12:29:02 INFO Saving final model to ../data/multsrc_demo-g6-n10K-a_dfs-h50618_tiny-v1_sweep-v1_2024-08-21-12-21-39/model.final.zanj\n",
      "2024-08-21 12:29:03 INFO Done training!\n"
     ]
    }
   ],
   "source": [
    "result: TrainingResult = train_model(\n",
    "\tbase_path=PATH_DATA,\n",
    "    cfg=CFG,\n",
    "\twandb_project=WandbProject.UNDERSTANDING_SEARCH, # change this to WandbProject.DEMO_NOTEBOOKS!\n",
    "\tdo_generate_dataset=False,\n",
    "\tdataset_verbose=True,\n",
    "    dataset=DATASET,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maze-transformer-2cGx2R0F-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
