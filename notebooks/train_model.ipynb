{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic\n",
    "import typing\n",
    "import os\n",
    "from pathlib import Path\n",
    "import typing\n",
    "import json\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# muutils\n",
    "from zanj.zanj import ZANJ, ZANJ_GLOBAL_DEFAULTS\n",
    "\n",
    "# Our Code\n",
    "from muutils.nbutils.configure_notebook import configure_notebook\n",
    "from maze_transformer.training.config import ConfigHolder, ZanjHookedTransformer, BaseGPTConfig, TrainConfig\n",
    "from maze_dataset import MazeDataset, MazeDatasetConfig\n",
    "from maze_dataset.dataset.configs import MAZE_DATASET_CONFIGS\n",
    "from maze_dataset.generation import LatticeMazeGenerators\n",
    "from maze_transformer.training.train_model import TrainingResult, train_model\n",
    "from maze_transformer.training.wandb_logger import WandbProject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "# set global defaults for ZANJ\n",
    "ZANJ_GLOBAL_DEFAULTS.external_array_threshold = 1024\n",
    "ZANJ_GLOBAL_DEFAULTS.external_list_threshold = 1024\n",
    "\n",
    "# paths\n",
    "PATH_EXAMPLES: Path = Path(\"../examples/\")\n",
    "PATH_DATA: Path = Path(\"../data/\")\n",
    "\n",
    "# reproducibility and device\n",
    "DEVICE = configure_notebook(seed=42, dark_mode=True)\n",
    "print(f\"{DEVICE = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list(MAZE_DATASET_CONFIGS.keys()) = ['test-g3-n5-a_dfs-h75556', 'demo_small-g3-n100-a_dfs-h88371', 'demo-g6-n10K-a_dfs-h30615']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{list(MAZE_DATASET_CONFIGS.keys()) = }\")\n",
    "\n",
    "# if you want to specify a custom config, you can do so here\n",
    "# CFG_CUSTOM: ConfigHolder = ConfigHolder(\n",
    "#     name = \"custom\",\n",
    "#     dataset_cfg = MazeDatasetConfig(\n",
    "# \t\tname=\"custom-dataset\",\n",
    "# \t\tgrid_n=6,\n",
    "# \t\tn_mazes=10000,\n",
    "# \t\tmaze_ctor=LatticeMazeGenerators.gen_dfs,\n",
    "# \t),\n",
    "#     model_cfg = BaseGPTConfig(\n",
    "#         name=\"custom-model\",\n",
    "#         act_fn=\"gelu\",\n",
    "#         d_model=8,\n",
    "#         d_head=4,\n",
    "#         n_layers=2,\n",
    "#     ),\n",
    "#     train_cfg = TrainConfig(\n",
    "#         name=\"custom-train\",\n",
    "#         optimizer=torch.optim.RMSprop,\n",
    "#         optimizer_kwargs=dict(lr=0.0001),\n",
    "#         batch_size=16,\n",
    "#         dataloader_cfg=dict(\n",
    "#             shuffle=True,\n",
    "#             num_workers=0,\n",
    "#             drop_last=False,\n",
    "#         ),\n",
    "#         intervals_count=dict(\n",
    "#             print_loss=100,\n",
    "#             checkpoint=5,\n",
    "#             eval_fast=10,\n",
    "#             eval_slow=5,\n",
    "#         )\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "CFG_HALLWAY: ConfigHolder = ConfigHolder(\n",
    "    name = \"hallway-medium\",\n",
    "    dataset_cfg = MazeDatasetConfig(\n",
    "\t\tname=\"custom-hallway\",\n",
    "\t\tgrid_n=8,\n",
    "\t\tn_mazes=3_000_000,\n",
    "\t\tmaze_ctor=LatticeMazeGenerators.gen_dfs,\n",
    "        maze_ctor_kwargs=dict(\n",
    "            do_forks=False,\n",
    "        ),\n",
    "        applied_filters=[{'name': 'collect_generation_meta', 'args': (), 'kwargs': {}}],\n",
    "        seq_len_max=256,\n",
    "\t),\n",
    "    model_cfg = BaseGPTConfig(\n",
    "        name=\"custom-model\",\n",
    "        act_fn=\"gelu\",\n",
    "        d_model=128,\n",
    "        d_head=32,\n",
    "        n_layers=6,\n",
    "    ),\n",
    "    train_cfg = TrainConfig(\n",
    "        name=\"custom-train\",\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer_kwargs=dict(lr=0.001),\n",
    "        batch_size=32,\n",
    "        dataloader_cfg=dict(\n",
    "            shuffle=False,\n",
    "            num_workers=8,\n",
    "            drop_last=False,\n",
    "        ),\n",
    "        intervals_count=dict(\n",
    "            print_loss=100,\n",
    "            checkpoint=20,\n",
    "            eval_fast=100,\n",
    "            eval_slow=50,\n",
    "        ),\n",
    "        validation_dataset_cfg=100,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is for training a \"real\" demo model\n",
    "# CFG_DEMO: ConfigHolder = ConfigHolder.get_config_multisource(\n",
    "#     cfg_names=(\"test-g3-n5-a_dfs-h75556\", \"tiny-v1\", \"sweep-v1\"),\n",
    "# )\n",
    "\n",
    "# # this is smaller, for testing\n",
    "# CFG_TEST: ConfigHolder = ConfigHolder.get_config_multisource(\n",
    "#     cfg_names=(\"demo_small-g3-n100-a_dfs-h88371\", \"nano-v1\", \"test-v1\"),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is where to specify which config to actually use\n",
    "CFG: ConfigHolder = CFG_HALLWAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"hallway-medium\",\n",
      "  \"dataset_cfg\": {\n",
      "    \"name\": \"custom-hallway\",\n",
      "    \"fname\": \"custom-hallway-g8-n3.0M-a_dfs-h66295\",\n",
      "    \"sdc_hash\": 38558245839505946605550258446345100654876335964881215358923857434720332766295,\n",
      "    \"seed\": 42,\n",
      "    \"seq_len_min\": 1,\n",
      "    \"seq_len_max\": 256,\n",
      "    \"applied_filters\": [\n",
      "      {\n",
      "        \"name\": \"collect_generation_meta\",\n",
      "        \"args\": [],\n",
      "        \"kwargs\": {}\n",
      "      }\n",
      "    ],\n",
      "    \"grid_n\": 8,\n",
      "    \"grid_shape\": [\n",
      "      8,\n",
      "      8\n",
      "    ],\n",
      "    \"n_mazes\": 3000000,\n",
      "    \"maze_ctor_name\": \"gen_dfs\",\n",
      "    \"maze_ctor_kwargs\": {\n",
      "      \"do_forks\": false\n",
      "    }\n",
      "  },\n",
      "  \"model_cfg\": {\n",
      "    \"name\": \"custom-model\",\n",
      "    \"act_fn\": \"gelu\",\n",
      "    \"d_model\": 128,\n",
      "    \"d_head\": 32,\n",
      "    \"n_layers\": 6,\n",
      "    \"weight_processing\": {\n",
      "      \"are_layernorms_folded\": false,\n",
      "      \"are_weights_processed\": false\n",
      "    },\n",
      "    \"n_heads\": 4\n",
      "  },\n",
      "  \"train_cfg\": {\n",
      "    \"name\": \"custom-train\",\n",
      "    \"optimizer\": \"AdamW\",\n",
      "    \"optimizer_kwargs\": {\n",
      "      \"lr\": 0.001\n",
      "    },\n",
      "    \"batch_size\": 32,\n",
      "    \"dataloader_cfg\": {\n",
      "      \"shuffle\": false,\n",
      "      \"num_workers\": 8,\n",
      "      \"drop_last\": false\n",
      "    },\n",
      "    \"intervals\": null,\n",
      "    \"intervals_count\": {\n",
      "      \"print_loss\": 100,\n",
      "      \"checkpoint\": 20,\n",
      "      \"eval_fast\": 100,\n",
      "      \"eval_slow\": 50\n",
      "    },\n",
      "    \"evals_max_new_tokens\": 8,\n",
      "    \"validation_dataset_cfg\": 100\n",
      "  },\n",
      "  \"pretrainedtokenizer_kwargs\": null,\n",
      "  \"maze_tokenizer\": {\n",
      "    \"tokenization_mode\": \"AOTP_UT_uniform\",\n",
      "    \"max_grid_size\": 8,\n",
      "    \"vocab_size\": 75\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(CFG.summary(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to get the dataset 'custom-hallway-g8-n3.0M-a_dfs-h66295'\n",
      "seeing if we can download the dataset...\n",
      "no download found, or download failed\n",
      "generating dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating & solving mazes: 100%|██████████| 3000000/3000000 [1:00:22<00:00, 828.27maze/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving dataset to ..\\data\\custom-hallway-g8-n3.0M-a_dfs-h66295.zanj\n",
      "Got dataset custom-hallway with 3000000 items. output.cfg.to_fname() = 'custom-hallway-g8-n3.0M-a_dfs-h66295'\n"
     ]
    }
   ],
   "source": [
    "# get just the dataset, generating it if needed. \n",
    "# This step can be skipped if you set `do_generate_dataset=True` when calling `train_model`\n",
    "# or if the dataset in question already exists\n",
    "\n",
    "# load the dataset\n",
    "DATASET: MazeDataset = MazeDataset.from_config(CFG.dataset_cfg, verbose=True, local_base_path=PATH_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-02 04:50:44 ERROR Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmiv\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\KNC\\maze-transformer\\notebooks\\wandb\\run-20231002_045046-sn2icf41</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/miv/understanding-search/runs/sn2icf41' target=\"_blank\">peach-dream-10</a></strong> to <a href='https://wandb.ai/miv/understanding-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/miv/understanding-search' target=\"_blank\">https://wandb.ai/miv/understanding-search</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/miv/understanding-search/runs/sn2icf41' target=\"_blank\">https://wandb.ai/miv/understanding-search/runs/sn2icf41</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-02 04:50:47 INFO config ={'__format__': 'ConfigHolder(SerializableDataclass)', 'dataset_cfg': {'__format__': 'MazeDatasetConfig(SerializableDataclass)', 'name': 'custom-hallway', 'seq_len_min': 1, 'seq_len_max': 256, 'seed': 42, 'applied_filters': [{'name': 'collect_generation_meta', 'args': (), 'kwargs': {}}], 'grid_n': 8, 'n_mazes': 3000000, 'maze_ctor': {'__name__': 'gen_dfs', '__module__': 'maze_dataset.generation.generators', '__doc__': ['generate a lattice maze using depth first search, iterative', '', '        # Arguments', '        - `grid_shape: Coord`: the shape of the grid', '        - `lattice_dim: int`: the dimension of the lattice', '          (default: `2`)', '        - `accessible_cells: int | float |None`: the number of accessible cells in the maze. If `None`, defaults to the total number of cells in the grid. if a float, asserts it is <= 1 and treats it as a proportion of **total cells**', '            (default: `None`)', '        - `max_tree_depth: int | float | None`: the maximum depth of the tree. If `None`, defaults to `2 * accessible_cells`. if a float, asserts it is <= 1 and treats it as a proportion of the **sum of the grid shape**', '            (default: `None`)', '        - `do_forks: bool`: whether to allow forks in the maze. If `False`, the maze will be have no forks and will be a simple hallway.', '        - `start_coord: Coord | None`: the starting coordinate of the generation algorithm. If `None`, defaults to a random coordinate.', '', '        # algorithm', '        1. Choose the initial cell, mark it as visited and push it to the stack', '        2. While the stack is not empty', '                1. Pop a cell from the stack and make it a current cell', '                2. If the current cell has any neighbours which have not been visited', '                        1. Push the current cell to the stack', '                        2. Choose one of the unvisited neighbours', '                        3. Remove the wall between the current cell and the chosen cell', '                        4. Mark the chosen cell as visited and push it to the stack', '        '], 'source_code': ['    @staticmethod', '    def gen_dfs(', '        grid_shape: Coord,', '        lattice_dim: int = 2,', '        accessible_cells: int | float | None = None,', '        max_tree_depth: int | float | None = None,', '        do_forks: bool = True,', '        randomized_stack: bool = False,', '        start_coord: Coord | None = None,', '    ) -> LatticeMaze:', '        \"\"\"generate a lattice maze using depth first search, iterative', '', '        # Arguments', '        - `grid_shape: Coord`: the shape of the grid', '        - `lattice_dim: int`: the dimension of the lattice', '          (default: `2`)', '        - `accessible_cells: int | float |None`: the number of accessible cells in the maze. If `None`, defaults to the total number of cells in the grid. if a float, asserts it is <= 1 and treats it as a proportion of **total cells**', '            (default: `None`)', '        - `max_tree_depth: int | float | None`: the maximum depth of the tree. If `None`, defaults to `2 * accessible_cells`. if a float, asserts it is <= 1 and treats it as a proportion of the **sum of the grid shape**', '            (default: `None`)', '        - `do_forks: bool`: whether to allow forks in the maze. If `False`, the maze will be have no forks and will be a simple hallway.', '        - `start_coord: Coord | None`: the starting coordinate of the generation algorithm. If `None`, defaults to a random coordinate.', '', '        # algorithm', '        1. Choose the initial cell, mark it as visited and push it to the stack', '        2. While the stack is not empty', '                1. Pop a cell from the stack and make it a current cell', '                2. If the current cell has any neighbours which have not been visited', '                        1. Push the current cell to the stack', '                        2. Choose one of the unvisited neighbours', '                        3. Remove the wall between the current cell and the chosen cell', '                        4. Mark the chosen cell as visited and push it to the stack', '        \"\"\"', '', '        # Default values if no constraints have been passed', '        grid_shape: Coord = np.array(grid_shape)', '        n_total_cells: int = int(np.prod(grid_shape))', '', '        n_accessible_cells: int', '        if accessible_cells is None:', '            n_accessible_cells = n_total_cells', '        elif isinstance(accessible_cells, float):', '            assert (', '                accessible_cells <= 1', '            ), f\"accessible_cells must be an int (count) or a float in the range [0, 1] (proportion), got {accessible_cells}\"', '', '            n_accessible_cells = int(accessible_cells * n_total_cells)', '        else:', '            assert isinstance(accessible_cells, int)', '            n_accessible_cells = accessible_cells', '', '        if max_tree_depth is None:', '            max_tree_depth = (', '                2 * n_total_cells', '            )  # We define max tree depth counting from the start coord in two directions. Therefore we divide by two in the if clause for neighboring sites later and multiply by two here.', '        elif isinstance(max_tree_depth, float):', '            assert (', '                max_tree_depth <= 1', '            ), f\"max_tree_depth must be an int (count) or a float in the range [0, 1] (proportion), got {max_tree_depth}\"', '', '            max_tree_depth = int(max_tree_depth * np.sum(grid_shape))', '', '        # choose a random start coord', '        start_coord = _random_start_coord(grid_shape, start_coord)', '', '        # initialize the maze with no connections', '        connection_list: ConnectionList = np.zeros(', '            (lattice_dim, grid_shape[0], grid_shape[1]), dtype=np.bool_', '        )', '', '        # initialize the stack with the target coord', '        visited_cells: set[tuple[int, int]] = set()', '        visited_cells.add(tuple(start_coord))  # this wasnt a bug after all lol', '        stack: list[Coord] = [start_coord]', '', '        # initialize tree_depth_counter', '        current_tree_depth: int = 1', '', '        # loop until the stack is empty or n_connected_cells is reached', '        while stack and (len(visited_cells) < n_accessible_cells):', '            # get the current coord from the stack', '            current_coord: Coord', '            if randomized_stack:', '                current_coord = stack.pop(random.randint(0, len(stack) - 1))', '            else:', '                current_coord = stack.pop()', '', '            # filter neighbors by being within grid bounds and being unvisited', '            unvisited_neighbors_deltas: list[tuple[Coord, Coord]] = [', '                (neighbor, delta)', '                for neighbor, delta in zip(', '                    current_coord + NEIGHBORS_MASK, NEIGHBORS_MASK', '                )', '                if (', '                    (tuple(neighbor) not in visited_cells)', '                    and (0 <= neighbor[0] < grid_shape[0])', '                    and (0 <= neighbor[1] < grid_shape[1])', '                )', '            ]', '', \"            # don't continue if max_tree_depth/2 is already reached (divide by 2 because we can branch to multiple directions)\", '            if unvisited_neighbors_deltas and (', '                current_tree_depth <= max_tree_depth / 2', '            ):', \"                # if we want a maze without forks, simply don't add the current coord back to the stack\", '                if do_forks and (len(unvisited_neighbors_deltas) > 1):', '                    stack.append(current_coord)', '', '                # choose one of the unvisited neighbors', '                chosen_neighbor, delta = random.choice(unvisited_neighbors_deltas)', '', '                # add connection', '                dim: int = np.argmax(np.abs(delta))', '                # if positive, down/right from current coord', '                # if negative, up/left from current coord (down/right from neighbor)', '                clist_node: Coord = (', '                    current_coord if (delta.sum() > 0) else chosen_neighbor', '                )', '                connection_list[dim, clist_node[0], clist_node[1]] = True', '', '                # add to visited cells and stack', '                visited_cells.add(tuple(chosen_neighbor))', '                stack.append(chosen_neighbor)', '', '                # Update current tree depth', '                current_tree_depth += 1', '            else:', '                current_tree_depth -= 1', '', '        output = LatticeMaze(', '            connection_list=connection_list,', '            generation_meta=dict(', '                func_name=\"gen_dfs\",', '                grid_shape=grid_shape,', '                start_coord=start_coord,', '                n_accessible_cells=int(n_accessible_cells),', '                max_tree_depth=int(max_tree_depth),', \"                # oh my god this took so long to track down. its almost 5am and I've spent like 2 hours on this bug\", '                # it was checking that len(visited_cells) == n_accessible_cells, but this means that the maze is', '                # treated as fully connected even when it is most certainly not, causing solving the maze to break', '                fully_connected=bool(len(visited_cells) == n_total_cells),', '                visited_cells={tuple(int(x) for x in coord) for coord in visited_cells},', '            ),', '        )', '', '        return output']}, 'maze_ctor_kwargs': {'do_forks': False}, 'grid_shape': (8, 8)}, 'model_cfg': {'__format__': 'BaseGPTConfig(SerializableDataclass)', 'name': 'custom-model', 'act_fn': 'gelu', 'd_model': 128, 'd_head': 32, 'n_layers': 6, 'weight_processing': {'are_layernorms_folded': False, 'are_weights_processed': False}, 'n_heads': 4}, 'train_cfg': {'__format__': 'TrainConfig(SerializableDataclass)', 'name': 'custom-train', 'evals_max_new_tokens': 8, 'validation_dataset_cfg': 100, 'optimizer': 'AdamW', 'optimizer_kwargs': {'lr': 0.001}, 'batch_size': 32, 'dataloader_cfg': {'shuffle': False, 'num_workers': 8, 'drop_last': False}, 'intervals': None, 'intervals_count': {'print_loss': 100, 'checkpoint': 20, 'eval_fast': 100, 'eval_slow': 50}}, 'name': 'hallway-medium', 'pretrainedtokenizer_kwargs': None, 'maze_tokenizer': {'__format__': 'MazeTokenizer(SerializableDataclass)', 'tokenization_mode': 'AOTP_UT_uniform', 'max_grid_size': 8, 'name': 'maze_tokenizer-AOTP_UT_uniform-g8', 'token_arr': ['<ADJLIST_START>', '<ADJLIST_END>', '<TARGET_START>', '<TARGET_END>', '<ORIGIN_START>', '<ORIGIN_END>', '<PATH_START>', '<PATH_END>', '<-->', ';', '<PADDING>', '(0,0)', '(0,1)', '(1,0)', '(1,1)', '(0,2)', '(2,0)', '(1,2)', '(2,1)', '(2,2)', '(0,3)', '(3,0)', '(3,1)', '(2,3)', '(3,2)', '(1,3)', '(3,3)', '(0,4)', '(2,4)', '(4,0)', '(1,4)', '(4,1)', '(4,2)', '(3,4)', '(4,3)', '(4,4)', '(0,5)', '(5,0)', '(5,1)', '(2,5)', '(5,2)', '(5,3)', '(4,5)', '(5,4)', '(1,5)', '(3,5)', '(5,5)', '(0,6)', '(2,6)', '(4,6)', '(6,0)', '(1,6)', '(6,1)', '(6,2)', '(3,6)', '(6,3)', '(6,4)', '(5,6)', '(6,5)', '(6,6)', '(0,7)', '(7,0)', '(7,1)', '(2,7)', '(7,2)', '(7,3)', '(4,7)', '(7,4)', '(7,5)', '(6,7)', '(7,6)', '(1,7)', '(3,7)', '(5,7)', '(7,7)'], 'tokenizer_map': {'<ADJLIST_START>': 0, '<ADJLIST_END>': 1, '<TARGET_START>': 2, '<TARGET_END>': 3, '<ORIGIN_START>': 4, '<ORIGIN_END>': 5, '<PATH_START>': 6, '<PATH_END>': 7, '<-->': 8, ';': 9, '<PADDING>': 10, '(0,0)': 11, '(0,1)': 12, '(1,0)': 13, '(1,1)': 14, '(0,2)': 15, '(2,0)': 16, '(1,2)': 17, '(2,1)': 18, '(2,2)': 19, '(0,3)': 20, '(3,0)': 21, '(3,1)': 22, '(2,3)': 23, '(3,2)': 24, '(1,3)': 25, '(3,3)': 26, '(0,4)': 27, '(2,4)': 28, '(4,0)': 29, '(1,4)': 30, '(4,1)': 31, '(4,2)': 32, '(3,4)': 33, '(4,3)': 34, '(4,4)': 35, '(0,5)': 36, '(5,0)': 37, '(5,1)': 38, '(2,5)': 39, '(5,2)': 40, '(5,3)': 41, '(4,5)': 42, '(5,4)': 43, '(1,5)': 44, '(3,5)': 45, '(5,5)': 46, '(0,6)': 47, '(2,6)': 48, '(4,6)': 49, '(6,0)': 50, '(1,6)': 51, '(6,1)': 52, '(6,2)': 53, '(3,6)': 54, '(6,3)': 55, '(6,4)': 56, '(5,6)': 57, '(6,5)': 58, '(6,6)': 59, '(0,7)': 60, '(7,0)': 61, '(7,1)': 62, '(2,7)': 63, '(7,2)': 64, '(7,3)': 65, '(4,7)': 66, '(7,4)': 67, '(7,5)': 68, '(6,7)': 69, '(7,6)': 70, '(1,7)': 71, '(3,7)': 72, '(5,7)': 73, '(7,7)': 74}, 'vocab_size': 75, 'padding_token_index': 10}}\n",
      "2023-10-02 04:50:47 INFO Initialized logger\n",
      "2023-10-02 04:50:47 INFO Summary logged, getting dataset\n",
      "2023-10-02 04:50:47 INFO passed dataset has matching config, using that\n",
      "2023-10-02 04:50:47 INFO finished getting training dataset with 3000000 samples\n",
      "2023-10-02 04:50:47 INFO got validation dataset by splitting training dataset into 2999900 train and 100 validation samples\n",
      "2023-10-02 04:50:47 INFO Loaded 2999900 sequences\n",
      "2023-10-02 04:50:47 INFO Creating dataloader\n",
      "2023-10-02 04:50:47 INFO finished dataloader, passing to train()\n",
      "2023-10-02 04:50:47 INFO Initializing model\n",
      "Moving model to device:  cuda\n",
      "2023-10-02 04:50:47 INFO Initializing optimizer\n",
      "2023-10-02 04:50:47 INFO will train for 93747 batches, evals_enabled=True, with intervals: {'print_loss': 937, 'checkpoint': 4687, 'eval_fast': 937, 'eval_slow': 1874}\n",
      "2023-10-02 04:50:47 INFO Starting training\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result: TrainingResult \u001b[39m=\u001b[39m train_model(\n\u001b[0;32m      2\u001b[0m \tbase_path\u001b[39m=\u001b[39;49mPATH_DATA,\n\u001b[0;32m      3\u001b[0m     cfg\u001b[39m=\u001b[39;49mCFG,\n\u001b[0;32m      4\u001b[0m \twandb_project\u001b[39m=\u001b[39;49mWandbProject\u001b[39m.\u001b[39;49mUNDERSTANDING_SEARCH, \u001b[39m# change this to WandbProject.DEMO_NOTEBOOKS!\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \tdo_generate_dataset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m      6\u001b[0m \tdataset_verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      7\u001b[0m     dataset\u001b[39m=\u001b[39;49mDATASET,\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32mF:\\KNC\\maze-transformer\\maze_transformer\\training\\train_model.py:169\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(base_path, wandb_project, cfg, cfg_file, cfg_names, do_generate_dataset, dataset_verbose, dataset, allow_dataset_override, device, help, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m dataloader: DataLoader \u001b[39m=\u001b[39m get_dataloader(dataset, cfg, logger)\n\u001b[0;32m    168\u001b[0m logger\u001b[39m.\u001b[39mprogress(\u001b[39m\"\u001b[39m\u001b[39mfinished dataloader, passing to train()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 169\u001b[0m trained_model: ZanjHookedTransformer \u001b[39m=\u001b[39m train(\n\u001b[0;32m    170\u001b[0m     cfg\u001b[39m=\u001b[39;49mcfg,\n\u001b[0;32m    171\u001b[0m     dataloader\u001b[39m=\u001b[39;49mdataloader,\n\u001b[0;32m    172\u001b[0m     logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[0;32m    173\u001b[0m     output_dir\u001b[39m=\u001b[39;49moutput_path,\n\u001b[0;32m    174\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    175\u001b[0m     val_dataset\u001b[39m=\u001b[39;49mval_dataset,\n\u001b[0;32m    176\u001b[0m )\n\u001b[0;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m TrainingResult(\n\u001b[0;32m    179\u001b[0m     output_path\u001b[39m=\u001b[39moutput_path,\n\u001b[0;32m    180\u001b[0m     model\u001b[39m=\u001b[39mtrained_model,\n\u001b[0;32m    181\u001b[0m )\n",
      "File \u001b[1;32mF:\\KNC\\maze-transformer\\maze_transformer\\training\\training.py:133\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(cfg, dataloader, logger, output_dir, device, val_dataset, zanj, model)\u001b[0m\n\u001b[0;32m    130\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m    131\u001b[0m logger\u001b[39m.\u001b[39mprogress(\u001b[39m\"\u001b[39m\u001b[39mStarting training\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 133\u001b[0m \u001b[39mfor\u001b[39;00m iteration, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39;49m(dataloader):\n\u001b[0;32m    134\u001b[0m     \u001b[39m# forward pass\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     loss: SingleLoss\n\u001b[0;32m    137\u001b[0m     logits: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_vocab\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\mivan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-transformer-2cGx2R0F-py3.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:441\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    440\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32mc:\\Users\\mivan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-transformer-2cGx2R0F-py3.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\mivan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-transformer-2cGx2R0F-py3.10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1042\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1035\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m   1043\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1044\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mC:\\Python\\Python3_10\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python\\Python3_10\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32mC:\\Python\\Python3_10\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32mC:\\Python\\Python3_10\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Python\\Python3_10\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result: TrainingResult = train_model(\n",
    "\tbase_path=PATH_DATA,\n",
    "    cfg=CFG,\n",
    "\twandb_project=WandbProject.UNDERSTANDING_SEARCH, # change this to WandbProject.DEMO_NOTEBOOKS!\n",
    "\tdo_generate_dataset=False,\n",
    "\tdataset_verbose=True,\n",
    "    dataset=DATASET,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maze-transformer",
   "language": "python",
   "name": "maze-transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
