{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "# Generic\n",
                "import html\n",
                "import os\n",
                "from pathlib import Path\n",
                "import typing\n",
                "import json\n",
                "import os\n",
                "from pathlib import Path\n",
                "import typing\n",
                "import html\n",
                "import copy\n",
                "\n",
                "# Transformers\n",
                "import circuitsvis\n",
                "from circuitsvis.attention import attention_heads\n",
                "from circuitsvis.tokens import colored_tokens_multi\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Numerical Computing\n",
                "import numpy as np\n",
                "import torch\n",
                "import pandas as pd\n",
                "from jaxtyping import Float, Int\n",
                "\n",
                "# Utilities\n",
                "from muutils.statcounter import StatCounter\n",
                "from muutils.json_serialize import serializable_dataclass, SerializableDataclass, serializable_field\n",
                "\n",
                "# Our Code\n",
                "from maze_transformer.utils.notebook_utils import configure_notebook\n",
                "from maze_transformer.generation.lattice_maze import LatticeMaze, SolvedMaze\n",
                "from maze_transformer.generation.generators import LatticeMazeGenerators\n",
                "from maze_transformer.dataset.tokenizer import SPECIAL_TOKENS, HuggingMazeTokenizer\n",
                "from maze_transformer.training.config import ConfigHolder, ZanjHookedTransformer\n",
                "from maze_transformer.evaluation.plot_maze import MazePlot, PathFormat\n",
                "from maze_transformer.evaluation.eval_model import load_model_with_configs\n",
                "from maze_transformer.utils.token_utils import tokens_to_coords\n",
                "from maze_transformer.dataset.maze_dataset import MazeDataset, MazeDatasetConfig\n",
                "from maze_transformer.dataset.maze_dataset_configs import MAZE_DATASET_CONFIGS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "will try to get model from ../examples/multsrc_demo-g6-n10K-a_dfs-h92077_tiny-v1_sweep-v1_2023-05-20-21-30-02/model.final.zanj\n",
                        "loaded model: multsrc_demo-g6-n10K-a_dfs-h92077_tiny-v1_sweep-v1 with 70319 parameters\n",
                        "loading dataset from ../data/demo-g6-n100-a_dfs-h88923.zanj\n",
                        "Got dataset demo with 100 items. output.cfg.to_fname() = 'demo-g6-n100-a_dfs-h88923'\n",
                        "got test dataset: demo with 100 mazes\n"
                    ]
                }
            ],
            "source": [
                "# Setup\n",
                "DEVICE = configure_notebook(seed=42, dark_mode=True)\n",
                "PATH_EXAMPLES: Path = Path(\"../examples/\")\n",
                "PATH_DATA: Path = Path(\"../data/\")\n",
                "\n",
                "# We won't be training any models\n",
                "torch.set_grad_enabled(False)\n",
                "\n",
                "MODEL_PATH: Path = PATH_EXAMPLES / \"multsrc_demo-g6-n10K-a_dfs-h92077_tiny-v1_sweep-v1_2023-05-20-21-30-02/model.final.zanj\"\n",
                "print(f\"will try to get model from {MODEL_PATH.as_posix()}\")\n",
                "\n",
                "# get the default model from examples\n",
                "MODEL: ZanjHookedTransformer = ZanjHookedTransformer.read(MODEL_PATH)\n",
                "print(f\"loaded model: {MODEL.zanj_model_config.name} with {MODEL.num_params()} parameters\")\n",
                "\n",
                "# generate a smaller test dataset\n",
                "DATASET_TEST_CFG: MazeDatasetConfig = copy.deepcopy(MODEL.zanj_model_config.dataset_cfg)\n",
                "DATASET_TEST_CFG.n_mazes = 100\n",
                "DATASET_TEST: MazeDataset = MazeDataset.from_config(\n",
                "    DATASET_TEST_CFG,\n",
                "    local_base_path=PATH_DATA,\n",
                "    verbose=True,\n",
                ")\n",
                "print(f\"got test dataset: {DATASET_TEST.cfg.name} with {len(DATASET_TEST)} mazes\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "@serializable_dataclass\n",
                "class ProcessedMazeAttention(SerializableDataclass):\n",
                "    input_maze: SolvedMaze\n",
                "    tokens: list[str]\n",
                "    maze_only_tokens: list[str]\n",
                "    logits: Float[torch.Tensor, \"n_vocab\"]\n",
                "    attention: dict[str, Float[torch.Tensor, \"1 n_heads n_positions n_positions\"]]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "def get_attention(\n",
                "\tmodel: ZanjHookedTransformer,\n",
                "\tdataset: MazeDataset,\n",
                "\tn_mazes: int = 1,\n",
                ") -> list[ProcessedMazeAttention]:\n",
                "\n",
                "\toutputs: list[ProcessedMazeAttention] = list()\n",
                "\n",
                "\n",
                "\tfor i in range(n_mazes):\n",
                "\t\t\n",
                "\t\t# get the maze from the dataset and process into tokens\n",
                "\t\tsolved_maze: SolvedMaze = dataset[i]\n",
                "\t\ttokens: list[str] = solved_maze.as_tokens(dataset.cfg.node_token_map)\n",
                "\t\tpath_start_index: int = tokens.index(SPECIAL_TOKENS[\"path_start\"])\n",
                "\t\tmaze_only_tokens: list[str] = tokens[:path_start_index + 1]\n",
                "\n",
                "\t\t# get the model's prediction and attention data\n",
                "\t\twith torch.no_grad():\n",
                "\t\t\tlogits, cache = model.run_with_cache(maze_only_tokens)\n",
                "\n",
                "\t\t# filter and append to outputs\n",
                "\t\tattention_data: dict[str, torch.Tensor] = {\n",
                "\t\t\tk:w \n",
                "\t\t\tfor k, w in cache.items() \n",
                "\t\t\t# if 'hook_pattern' in k\n",
                "\t\t}\n",
                "\t\t\n",
                "\t\toutputs.append(ProcessedMazeAttention(\n",
                "\t\t\tinput_maze=solved_maze,\n",
                "\t\t\ttokens=tokens,\n",
                "\t\t\tmaze_only_tokens=maze_only_tokens,\n",
                "\t\t\tlogits=logits,\n",
                "\t\t\tattention=attention_data,\n",
                "\t\t))\n",
                "\n",
                "\treturn outputs\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "def plot_attention_data(data: ProcessedMazeAttention):\n",
                "\tn_layers: int = len(data.attention)\n",
                "\texample_attention_data: Float[torch.Tensor, \"1 n_heads n_positions n_positions\"] = data.attention[list(data.attention.keys())[0]]\n",
                "\tn_heads: int = example_attention_data.shape[1]\n",
                "\tn_tokens: int = example_attention_data.shape[2]\n",
                "\n",
                "\tattention_to_plot: Float[torch.Tensor, \"n_layers n_heads n_tokens n_tokens\"] = torch.concatenate(\n",
                "\t\t[w for k,w in data.attention.items()],\n",
                "\t\tdim=0,\n",
                "\t).reshape(-1, n_tokens, n_tokens)\n",
                "\n",
                "\tattention_head_names: list[str] = [\n",
                "\t\tf\"Layer {i} Head {j}\" \n",
                "\t\tfor i in range(n_layers) \n",
                "\t\tfor j in range(n_heads)\n",
                "\t]\n",
                "\n",
                "\treturn attention_heads(\n",
                "\t\tattention_to_plot, \n",
                "\t\tdata.maze_only_tokens, \n",
                "\t\tattention_head_names,\n",
                "\t)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{\n",
                        "  \"name\": \"tiny-v1\",\n",
                        "  \"act_fn\": \"gelu\",\n",
                        "  \"d_model\": 32,\n",
                        "  \"d_head\": 16,\n",
                        "  \"n_layers\": 4,\n",
                        "  \"weight_processing\": {\n",
                        "    \"are_layernorms_folded\": false,\n",
                        "    \"are_weights_processed\": true\n",
                        "  }\n",
                        "}\n",
                        "ATTENTION_DATA.tokens = ['<ADJLIST_START>', '(5,5)', '<-->', '(5,4)', ';', '(2,5)', '<-->', '(1,5)', ';', '(2,0)', '<-->', '(3,0)', ';', '(3,2)', '<-->', '(3,3)', ';', '(2,5)', '<-->', '(2,4)', ';', '(4,4)', '<-->', '(4,3)', ';', '(1,0)', '<-->', '(1,1)', ';', '(4,0)', '<-->', '(5,0)', ';', '(1,4)', '<-->', '(0,4)', ';', '(4,3)', '<-->', '(3,3)', ';', '(5,3)', '<-->', '(5,4)', ';', '(5,1)', '<-->', '(5,2)', ';', '(0,2)', '<-->', '(1,2)', ';', '(4,0)', '<-->', '(4,1)', ';', '(0,3)', '<-->', '(0,4)', ';', '(1,2)', '<-->', '(2,2)', ';', '(2,1)', '<-->', '(3,1)', ';', '(3,4)', '<-->', '(3,5)', ';', '(1,0)', '<-->', '(0,0)', ';', '(0,5)', '<-->', '(1,5)', ';', '(1,3)', '<-->', '(2,3)', ';', '(2,2)', '<-->', '(2,1)', ';', '(3,1)', '<-->', '(3,2)', ';', '(0,2)', '<-->', '(0,1)', ';', '(4,5)', '<-->', '(3,5)', ';', '(5,1)', '<-->', '(5,0)', ';', '(1,4)', '<-->', '(1,5)', ';', '(5,3)', '<-->', '(5,2)', ';', '(0,2)', '<-->', '(0,3)', ';', '(0,0)', '<-->', '(0,1)', ';', '(2,0)', '<-->', '(1,0)', ';', '(4,2)', '<-->', '(4,1)', ';', '(4,5)', '<-->', '(5,5)', ';', '(4,3)', '<-->', '(4,2)', ';', '(2,3)', '<-->', '(2,4)', ';', '<ADJLIST_END>', '<ORIGIN_START>', '(2,5)', '<ORIGIN_END>', '<TARGET_START>', '(2,1)', '<TARGET_END>', '<PATH_START>', '(2,5)', '(1,5)', '(1,4)', '(0,4)', '(0,3)', '(0,2)', '(1,2)', '(2,2)', '(2,1)', '<PATH_END>']\n",
                        "ATTENTION_DATA.maze_only_tokens = ['<ADJLIST_START>', '(5,5)', '<-->', '(5,4)', ';', '(2,5)', '<-->', '(1,5)', ';', '(2,0)', '<-->', '(3,0)', ';', '(3,2)', '<-->', '(3,3)', ';', '(2,5)', '<-->', '(2,4)', ';', '(4,4)', '<-->', '(4,3)', ';', '(1,0)', '<-->', '(1,1)', ';', '(4,0)', '<-->', '(5,0)', ';', '(1,4)', '<-->', '(0,4)', ';', '(4,3)', '<-->', '(3,3)', ';', '(5,3)', '<-->', '(5,4)', ';', '(5,1)', '<-->', '(5,2)', ';', '(0,2)', '<-->', '(1,2)', ';', '(4,0)', '<-->', '(4,1)', ';', '(0,3)', '<-->', '(0,4)', ';', '(1,2)', '<-->', '(2,2)', ';', '(2,1)', '<-->', '(3,1)', ';', '(3,4)', '<-->', '(3,5)', ';', '(1,0)', '<-->', '(0,0)', ';', '(0,5)', '<-->', '(1,5)', ';', '(1,3)', '<-->', '(2,3)', ';', '(2,2)', '<-->', '(2,1)', ';', '(3,1)', '<-->', '(3,2)', ';', '(0,2)', '<-->', '(0,1)', ';', '(4,5)', '<-->', '(3,5)', ';', '(5,1)', '<-->', '(5,0)', ';', '(1,4)', '<-->', '(1,5)', ';', '(5,3)', '<-->', '(5,2)', ';', '(0,2)', '<-->', '(0,3)', ';', '(0,0)', '<-->', '(0,1)', ';', '(2,0)', '<-->', '(1,0)', ';', '(4,2)', '<-->', '(4,1)', ';', '(4,5)', '<-->', '(5,5)', ';', '(4,3)', '<-->', '(4,2)', ';', '(2,3)', '<-->', '(2,4)', ';', '<ADJLIST_END>', '<ORIGIN_START>', '(2,5)', '<ORIGIN_END>', '<TARGET_START>', '(2,1)', '<TARGET_END>', '<PATH_START>']\n",
                        "ATTENTION_DATA.logits.shape = torch.Size([149, 2, 47])\n",
                        "list(ATTENTION_DATA.attention.keys()) = ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_mid', 'blocks.0.ln2.hook_scale', 'blocks.0.ln2.hook_normalized', 'blocks.0.mlp.hook_pre', 'blocks.0.mlp.hook_post', 'blocks.0.hook_mlp_out', 'blocks.0.hook_resid_post', 'blocks.1.hook_resid_pre', 'blocks.1.ln1.hook_scale', 'blocks.1.ln1.hook_normalized', 'blocks.1.attn.hook_q', 'blocks.1.attn.hook_k', 'blocks.1.attn.hook_v', 'blocks.1.attn.hook_attn_scores', 'blocks.1.attn.hook_pattern', 'blocks.1.attn.hook_z', 'blocks.1.hook_attn_out', 'blocks.1.hook_resid_mid', 'blocks.1.ln2.hook_scale', 'blocks.1.ln2.hook_normalized', 'blocks.1.mlp.hook_pre', 'blocks.1.mlp.hook_post', 'blocks.1.hook_mlp_out', 'blocks.1.hook_resid_post', 'blocks.2.hook_resid_pre', 'blocks.2.ln1.hook_scale', 'blocks.2.ln1.hook_normalized', 'blocks.2.attn.hook_q', 'blocks.2.attn.hook_k', 'blocks.2.attn.hook_v', 'blocks.2.attn.hook_attn_scores', 'blocks.2.attn.hook_pattern', 'blocks.2.attn.hook_z', 'blocks.2.hook_attn_out', 'blocks.2.hook_resid_mid', 'blocks.2.ln2.hook_scale', 'blocks.2.ln2.hook_normalized', 'blocks.2.mlp.hook_pre', 'blocks.2.mlp.hook_post', 'blocks.2.hook_mlp_out', 'blocks.2.hook_resid_post', 'blocks.3.hook_resid_pre', 'blocks.3.ln1.hook_scale', 'blocks.3.ln1.hook_normalized', 'blocks.3.attn.hook_q', 'blocks.3.attn.hook_k', 'blocks.3.attn.hook_v', 'blocks.3.attn.hook_attn_scores', 'blocks.3.attn.hook_pattern', 'blocks.3.attn.hook_z', 'blocks.3.hook_attn_out', 'blocks.3.hook_resid_mid', 'blocks.3.ln2.hook_scale', 'blocks.3.ln2.hook_normalized', 'blocks.3.mlp.hook_pre', 'blocks.3.mlp.hook_post', 'blocks.3.hook_mlp_out', 'blocks.3.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']\n",
                        "hook_embed torch.Size([149, 2, 32])\n",
                        "hook_pos_embed torch.Size([149, 2, 32])\n",
                        "blocks.0.hook_resid_pre torch.Size([149, 2, 32])\n",
                        "blocks.0.ln1.hook_scale torch.Size([149, 2, 1])\n",
                        "blocks.0.ln1.hook_normalized torch.Size([149, 2, 32])\n",
                        "blocks.0.attn.hook_q torch.Size([149, 2, 2, 16])\n",
                        "blocks.0.attn.hook_k torch.Size([149, 2, 2, 16])\n",
                        "blocks.0.attn.hook_v torch.Size([149, 2, 2, 16])\n",
                        "blocks.0.attn.hook_attn_scores torch.Size([149, 2, 2, 2])\n",
                        "blocks.0.attn.hook_pattern torch.Size([149, 2, 2, 2])\n",
                        "blocks.0.attn.hook_z torch.Size([149, 2, 2, 16])\n",
                        "blocks.0.hook_attn_out torch.Size([149, 2, 32])\n",
                        "blocks.0.hook_resid_mid torch.Size([149, 2, 32])\n",
                        "blocks.0.ln2.hook_scale torch.Size([149, 2, 1])\n",
                        "blocks.0.ln2.hook_normalized torch.Size([149, 2, 32])\n",
                        "blocks.0.mlp.hook_pre torch.Size([149, 2, 128])\n",
                        "blocks.0.mlp.hook_post torch.Size([149, 2, 128])\n",
                        "blocks.0.hook_mlp_out torch.Size([149, 2, 32])\n",
                        "blocks.0.hook_resid_post torch.Size([149, 2, 32])\n",
                        "blocks.1.hook_resid_pre torch.Size([149, 2, 32])\n",
                        "blocks.1.ln1.hook_scale torch.Size([149, 2, 1])\n",
                        "blocks.1.ln1.hook_normalized torch.Size([149, 2, 32])\n",
                        "blocks.1.attn.hook_q torch.Size([149, 2, 2, 16])\n",
                        "blocks.1.attn.hook_k torch.Size([149, 2, 2, 16])\n",
                        "blocks.1.attn.hook_v torch.Size([149, 2, 2, 16])\n",
                        "blocks.1.attn.hook_attn_scores torch.Size([149, 2, 2, 2])\n",
                        "blocks.1.attn.hook_pattern torch.Size([149, 2, 2, 2])\n",
                        "blocks.1.attn.hook_z torch.Size([149, 2, 2, 16])\n",
                        "blocks.1.hook_attn_out torch.Size([149, 2, 32])\n",
                        "blocks.1.hook_resid_mid torch.Size([149, 2, 32])\n",
                        "blocks.1.ln2.hook_scale torch.Size([149, 2, 1])\n",
                        "blocks.1.ln2.hook_normalized torch.Size([149, 2, 32])\n",
                        "blocks.1.mlp.hook_pre torch.Size([149, 2, 128])\n",
                        "blocks.1.mlp.hook_post torch.Size([149, 2, 128])\n",
                        "blocks.1.hook_mlp_out torch.Size([149, 2, 32])\n",
                        "blocks.1.hook_resid_post torch.Size([149, 2, 32])\n",
                        "blocks.2.hook_resid_pre torch.Size([149, 2, 32])\n",
                        "blocks.2.ln1.hook_scale torch.Size([149, 2, 1])\n",
                        "blocks.2.ln1.hook_normalized torch.Size([149, 2, 32])\n",
                        "blocks.2.attn.hook_q torch.Size([149, 2, 2, 16])\n",
                        "blocks.2.attn.hook_k torch.Size([149, 2, 2, 16])\n",
                        "blocks.2.attn.hook_v torch.Size([149, 2, 2, 16])\n",
                        "blocks.2.attn.hook_attn_scores torch.Size([149, 2, 2, 2])\n",
                        "blocks.2.attn.hook_pattern torch.Size([149, 2, 2, 2])\n",
                        "blocks.2.attn.hook_z torch.Size([149, 2, 2, 16])\n",
                        "blocks.2.hook_attn_out torch.Size([149, 2, 32])\n",
                        "blocks.2.hook_resid_mid torch.Size([149, 2, 32])\n",
                        "blocks.2.ln2.hook_scale torch.Size([149, 2, 1])\n",
                        "blocks.2.ln2.hook_normalized torch.Size([149, 2, 32])\n",
                        "blocks.2.mlp.hook_pre torch.Size([149, 2, 128])\n",
                        "blocks.2.mlp.hook_post torch.Size([149, 2, 128])\n",
                        "blocks.2.hook_mlp_out torch.Size([149, 2, 32])\n",
                        "blocks.2.hook_resid_post torch.Size([149, 2, 32])\n",
                        "blocks.3.hook_resid_pre torch.Size([149, 2, 32])\n",
                        "blocks.3.ln1.hook_scale torch.Size([149, 2, 1])\n",
                        "blocks.3.ln1.hook_normalized torch.Size([149, 2, 32])\n",
                        "blocks.3.attn.hook_q torch.Size([149, 2, 2, 16])\n",
                        "blocks.3.attn.hook_k torch.Size([149, 2, 2, 16])\n",
                        "blocks.3.attn.hook_v torch.Size([149, 2, 2, 16])\n",
                        "blocks.3.attn.hook_attn_scores torch.Size([149, 2, 2, 2])\n",
                        "blocks.3.attn.hook_pattern torch.Size([149, 2, 2, 2])\n",
                        "blocks.3.attn.hook_z torch.Size([149, 2, 2, 16])\n",
                        "blocks.3.hook_attn_out torch.Size([149, 2, 32])\n",
                        "blocks.3.hook_resid_mid torch.Size([149, 2, 32])\n",
                        "blocks.3.ln2.hook_scale torch.Size([149, 2, 1])\n",
                        "blocks.3.ln2.hook_normalized torch.Size([149, 2, 32])\n",
                        "blocks.3.mlp.hook_pre torch.Size([149, 2, 128])\n",
                        "blocks.3.mlp.hook_post torch.Size([149, 2, 128])\n",
                        "blocks.3.hook_mlp_out torch.Size([149, 2, 32])\n",
                        "blocks.3.hook_resid_post torch.Size([149, 2, 32])\n",
                        "ln_final.hook_scale torch.Size([149, 2, 1])\n",
                        "ln_final.hook_normalized torch.Size([149, 2, 32])\n"
                    ]
                },
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "ATTENTION_DATA: ProcessedMazeAttention = get_attention(MODEL, DATASET_TEST, n_mazes=1)[0]\n",
                "\n",
                "print(json.dumps(MODEL.zanj_model_config.model_cfg.summary(), indent=2))\n",
                "\n",
                "plt.imshow(ATTENTION_DATA.input_maze.as_pixels())\n",
                "\n",
                "print(f\"{ATTENTION_DATA.tokens = }\")\n",
                "print(f\"{ATTENTION_DATA.maze_only_tokens = }\")\n",
                "print(f\"{ATTENTION_DATA.logits.shape = }\")\n",
                "print(f\"{list(ATTENTION_DATA.attention.keys()) = }\")\n",
                "for k, v in ATTENTION_DATA.attention.items():\n",
                "\tprint(k, v.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_attention_data(ATTENTION_DATA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "#! ALEX note - there used to be a np.power(head_np, 1/4) here, not sure what that's about?\n",
                "FROM_TOKEN = -1 # Look at attention from this token position to the rest of the sequence\n",
                "attentions_from_token = torch.concatenate([w[0, :, FROM_TOKEN, :] for w in attentions], dim=0)\n",
                "colored_tokens_multi(context_str, attentions_from_token.T, labels=attention_head_names)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "def prediction_contained_a_coordinate_token(tokens: list[str], tokenizer: HuggingMazeTokenizer) -> bool:\n",
                "\t\"\"\"Check if the prediction contains a coordinate token\"\"\"\n",
                "\tfor t in tokens:\n",
                "\t\tif t not in list(tokenizer.special_tokens_map.values()) + tokenizer.additional_special_tokens:\n",
                "\t\t\treturn True\n",
                "\tprint(\"FAIL: Sampled a path - No coordinate token found before EOS\")\n",
                "\treturn False\n",
                "\n",
                "predicted_tokens = []\n",
                "while not prediction_contained_a_coordinate_token(predicted_tokens, model.tokenizer):\n",
                "\tpredictions = model.generate(array_tensor, max_new_tokens=50, stop_at_eos=True, verbose=False)\n",
                "\tpredicted_tokens = model.to_str_tokens(predictions)[len(maze_only_tokens):]\n",
                "print(\"SUCCESS: Model predicted the path:\")\n",
                "print(predicted_tokens)\n",
                "\n",
                "path_predicted: list[tuple[int,int]] = tokens_to_coords(\n",
                "\tpredicted_tokens,\n",
                "\tmaze_data_cfg = cfg.dataset_cfg, \n",
                "\twhen_noncoord = \"skip\",\n",
                ")\n",
                "\n",
                "# plot the maze and both solutions\n",
                "# for label, fmt, color, path in paths\n",
                "MazePlot(maze).add_true_path(path_true).add_predicted_path(path_predicted).plot()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "maze-transformer",
            "language": "python",
            "name": "maze-transformer"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.1"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "170637793197da0d440deb6cb249c898d613b24c548839ecbbac11596710dfc2"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
