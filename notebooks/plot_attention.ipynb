{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "# Generic\n",
                "import html\n",
                "import os\n",
                "from pathlib import Path\n",
                "import typing\n",
                "import json\n",
                "import os\n",
                "from pathlib import Path\n",
                "import typing\n",
                "import html\n",
                "import copy\n",
                "\n",
                "# Transformers\n",
                "import circuitsvis\n",
                "from circuitsvis.attention import attention_heads\n",
                "from circuitsvis.tokens import colored_tokens_multi\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Numerical Computing\n",
                "import numpy as np\n",
                "import torch\n",
                "import pandas as pd\n",
                "from jaxtyping import Float, Int\n",
                "\n",
                "# Utilities\n",
                "from muutils.statcounter import StatCounter\n",
                "from muutils.json_serialize import serializable_dataclass, SerializableDataclass, serializable_field\n",
                "\n",
                "# Our Code\n",
                "from maze_transformer.utils.notebook_utils import configure_notebook\n",
                "from maze_transformer.generation.lattice_maze import LatticeMaze, SolvedMaze\n",
                "from maze_transformer.generation.generators import LatticeMazeGenerators\n",
                "from maze_transformer.dataset.tokenizer import SPECIAL_TOKENS, HuggingMazeTokenizer\n",
                "from maze_transformer.training.config import ConfigHolder, ZanjHookedTransformer\n",
                "from maze_transformer.evaluation.plot_maze import MazePlot, PathFormat\n",
                "from maze_transformer.evaluation.eval_model import load_model_with_configs\n",
                "from maze_transformer.utils.token_utils import tokens_to_coords\n",
                "from maze_transformer.dataset.maze_dataset import MazeDataset, MazeDatasetConfig\n",
                "from maze_transformer.dataset.maze_dataset_configs import MAZE_DATASET_CONFIGS\n",
                "\n",
                "\n",
                "def pprint_summary(summary: dict):\n",
                "\tprint(json.dumps(summary, indent=2))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "will try to get model from ../examples/multsrc_demo-g6-n10K-a_dfs-h92077_tiny-v1_sweep-v1_2023-05-20-21-30-02/model.final.zanj\n",
                        "loaded model: multsrc_demo-g6-n10K-a_dfs-h92077_tiny-v1_sweep-v1 with 70319 parameters\n",
                        "loading dataset from ../data/demo-g6-n100-a_dfs-h88923.zanj\n",
                        "Got dataset demo with 100 items. output.cfg.to_fname() = 'demo-g6-n100-a_dfs-h88923'\n",
                        "got test dataset: demo with 100 mazes\n"
                    ]
                }
            ],
            "source": [
                "# Setup\n",
                "DEVICE = configure_notebook(seed=42, dark_mode=True)\n",
                "PATH_EXAMPLES: Path = Path(\"../examples/\")\n",
                "PATH_DATA: Path = Path(\"../data/\")\n",
                "\n",
                "# We won't be training any models\n",
                "torch.set_grad_enabled(False)\n",
                "\n",
                "MODEL_PATH: Path = PATH_EXAMPLES / \"multsrc_demo-g6-n10K-a_dfs-h92077_tiny-v1_sweep-v1_2023-05-20-21-30-02/model.final.zanj\"\n",
                "print(f\"will try to get model from {MODEL_PATH.as_posix()}\")\n",
                "\n",
                "# get the default model from examples\n",
                "MODEL: ZanjHookedTransformer = ZanjHookedTransformer.read(MODEL_PATH)\n",
                "print(f\"loaded model: {MODEL.zanj_model_config.name} with {MODEL.num_params()} parameters\")\n",
                "\n",
                "# generate a smaller test dataset\n",
                "DATASET_TEST_CFG: MazeDatasetConfig = copy.deepcopy(MODEL.zanj_model_config.dataset_cfg)\n",
                "DATASET_TEST_CFG.n_mazes = 100\n",
                "DATASET_TEST: MazeDataset = MazeDataset.from_config(\n",
                "    DATASET_TEST_CFG,\n",
                "    local_base_path=PATH_DATA,\n",
                "    verbose=True,\n",
                ")\n",
                "print(f\"got test dataset: {DATASET_TEST.cfg.name} with {len(DATASET_TEST)} mazes\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [],
            "source": [
                "@serializable_dataclass\n",
                "class ProcessedMazeAttention(SerializableDataclass):\n",
                "    input_maze: SolvedMaze\n",
                "    tokens: list[str]\n",
                "    maze_only_tokens: list[str]\n",
                "    logits: Float[torch.Tensor, \"n_vocab\"]\n",
                "    n_layers: int\n",
                "    n_heads: int\n",
                "    attention_dict: dict[str, Float[torch.Tensor, \"1 n_heads n_positions n_positions\"]]\n",
                "    attention_tensored: Float[torch.Tensor, \"n_layers n_heads n_tokens n_tokens\"]\n",
                "    attention_names: list[str] # names for attention_tensored\n",
                "\n",
                "    def plot_attentions(self):\n",
                "        return attention_heads(\n",
                "            self.attention_tensored, \n",
                "            self.maze_only_tokens, \n",
                "            self.attention_names,\n",
                "        )\n",
                "    \n",
                "    def summary(self) -> dict:\n",
                "        return {\n",
                "            \"tokens\": \" \".join(self.tokens),\n",
                "            \"maze_only_tokens\": \" \".join(self.maze_only_tokens),\n",
                "            \"n_layers\": self.n_layers,\n",
                "            \"n_heads\": self.n_heads,\n",
                "            \"logits.shape\": self.logits.shape,\n",
                "            \"attention_tensored.shape\": self.attention_tensored.shape,\n",
                "            \"attention_names\": \" \".join(self.attention_names),\n",
                "            \"attention_dict.keys()\": \" \".join(list(self.attention_dict.keys())),\n",
                "        }\n",
                "    \n",
                "    @classmethod\n",
                "    def from_model_and_dataset(\n",
                "        cls,\n",
                "        model: ZanjHookedTransformer,\n",
                "        dataset: MazeDataset,\n",
                "        n_mazes: int = 1,\n",
                "    ) -> list[\"ProcessedMazeAttention\"]:\n",
                "\n",
                "        outputs: list[ProcessedMazeAttention] = list()\n",
                "\n",
                "\n",
                "        for i in range(n_mazes):\n",
                "            \n",
                "            # get the maze from the dataset and process into tokens\n",
                "            solved_maze: SolvedMaze = dataset[i]\n",
                "            tokens: list[str] = solved_maze.as_tokens(dataset.cfg.node_token_map)\n",
                "            path_start_index: int = tokens.index(SPECIAL_TOKENS[\"path_start\"])\n",
                "            maze_only_tokens: list[str] = tokens[:path_start_index + 1]\n",
                "\n",
                "            # get the model's prediction and attention data\n",
                "            with torch.no_grad():\n",
                "                # we have to join here, since otherwise run_with_cache assumes each token is a separate batch\n",
                "                logits, cache = model.run_with_cache(' '.join(maze_only_tokens))\n",
                "\n",
                "            # filter and append to outputs\n",
                "            attention_data: dict[str, torch.Tensor] = {\n",
                "                k:w \n",
                "                for k, w in cache.items() \n",
                "                if 'hook_pattern' in k\n",
                "            }\n",
                "\n",
                "            assert model.zanj_model_config.model_cfg.n_layers == len(attention_data)\n",
                "            example_attention_data: Float[torch.Tensor, \"1 n_heads n_positions n_positions\"] = attention_data[list(attention_data.keys())[0]]\n",
                "            assert model.zanj_model_config.model_cfg.n_heads == example_attention_data.shape[1]\n",
                "            n_tokens: int = example_attention_data.shape[2]\n",
                "\n",
                "            attention_tensored: Float[torch.Tensor, \"n_layers_heads n_tokens n_tokens\"] = torch.concatenate(\n",
                "                [w for k,w in attention_data.items()],\n",
                "                dim=0,\n",
                "            ).reshape(-1, n_tokens, n_tokens)\n",
                "            \n",
                "            outputs.append(ProcessedMazeAttention(\n",
                "                input_maze=solved_maze,\n",
                "                tokens=tokens,\n",
                "                maze_only_tokens=maze_only_tokens,\n",
                "                logits=logits,\n",
                "                n_layers=model.zanj_model_config.model_cfg.n_layers,\n",
                "                n_heads=model.zanj_model_config.model_cfg.n_heads,\n",
                "                attention_dict=attention_data,\n",
                "                attention_tensored=attention_tensored,\n",
                "                attention_names=[\n",
                "                    f\"Layer {i} Head {j}\" \n",
                "                    for i in range(model.zanj_model_config.model_cfg.n_layers) \n",
                "                    for j in range(model.zanj_model_config.model_cfg.n_heads)\n",
                "                ],\n",
                "            ))\n",
                "\n",
                "        return outputs\n",
                "    \n",
                "    def plot_colored_tokens_multi(self, from_token: int = 0):\n",
                "        attentions_from_token: Float[torch.Tensor, \"n_tokens-from_token n_layers_heads\"] = torch.sum(\n",
                "            self.attention_tensored[:, from_token+1:, from_token+1:],\n",
                "            dim=2,\n",
                "        ).T\n",
                "        print(f\"{attentions_from_token.shape = }\")\n",
                "        return colored_tokens_multi(self.maze_only_tokens[from_token:], attentions_from_token, self.attention_names)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{\n",
                        "  \"name\": \"tiny-v1\",\n",
                        "  \"act_fn\": \"gelu\",\n",
                        "  \"d_model\": 32,\n",
                        "  \"d_head\": 16,\n",
                        "  \"n_layers\": 4,\n",
                        "  \"weight_processing\": {\n",
                        "    \"are_layernorms_folded\": false,\n",
                        "    \"are_weights_processed\": true\n",
                        "  },\n",
                        "  \"n_heads\": 2\n",
                        "}\n",
                        "{\n",
                        "  \"tokens\": \"<ADJLIST_START> (3,5) <--> (4,5) ; (4,2) <--> (4,1) ; (4,5) <--> (5,5) ; (4,1) <--> (4,0) ; (1,0) <--> (2,0) ; (1,5) <--> (0,5) ; (1,4) <--> (1,5) ; (0,2) <--> (0,1) ; (0,3) <--> (0,4) ; (5,0) <--> (5,1) ; (4,0) <--> (5,0) ; (0,3) <--> (0,2) ; (3,2) <--> (3,1) ; (2,3) <--> (1,3) ; (0,2) <--> (1,2) ; (2,2) <--> (1,2) ; (4,3) <--> (3,3) ; (0,4) <--> (1,4) ; (4,3) <--> (4,4) ; (3,1) <--> (2,1) ; (4,2) <--> (4,3) ; (5,1) <--> (5,2) ; (3,0) <--> (2,0) ; (5,4) <--> (5,3) ; (5,4) <--> (5,5) ; (0,0) <--> (0,1) ; (2,5) <--> (1,5) ; (1,0) <--> (1,1) ; (5,3) <--> (5,2) ; (2,4) <--> (2,3) ; (1,0) <--> (0,0) ; (3,3) <--> (3,2) ; (2,4) <--> (2,5) ; (3,4) <--> (3,5) ; (2,2) <--> (2,1) ; <ADJLIST_END> <ORIGIN_START> (2,5) <ORIGIN_END> <TARGET_START> (2,1) <TARGET_END> <PATH_START> (2,5) (1,5) (1,4) (0,4) (0,3) (0,2) (1,2) (2,2) (2,1) <PATH_END>\",\n",
                        "  \"maze_only_tokens\": \"<ADJLIST_START> (3,5) <--> (4,5) ; (4,2) <--> (4,1) ; (4,5) <--> (5,5) ; (4,1) <--> (4,0) ; (1,0) <--> (2,0) ; (1,5) <--> (0,5) ; (1,4) <--> (1,5) ; (0,2) <--> (0,1) ; (0,3) <--> (0,4) ; (5,0) <--> (5,1) ; (4,0) <--> (5,0) ; (0,3) <--> (0,2) ; (3,2) <--> (3,1) ; (2,3) <--> (1,3) ; (0,2) <--> (1,2) ; (2,2) <--> (1,2) ; (4,3) <--> (3,3) ; (0,4) <--> (1,4) ; (4,3) <--> (4,4) ; (3,1) <--> (2,1) ; (4,2) <--> (4,3) ; (5,1) <--> (5,2) ; (3,0) <--> (2,0) ; (5,4) <--> (5,3) ; (5,4) <--> (5,5) ; (0,0) <--> (0,1) ; (2,5) <--> (1,5) ; (1,0) <--> (1,1) ; (5,3) <--> (5,2) ; (2,4) <--> (2,3) ; (1,0) <--> (0,0) ; (3,3) <--> (3,2) ; (2,4) <--> (2,5) ; (3,4) <--> (3,5) ; (2,2) <--> (2,1) ; <ADJLIST_END> <ORIGIN_START> (2,5) <ORIGIN_END> <TARGET_START> (2,1) <TARGET_END> <PATH_START>\",\n",
                        "  \"n_layers\": 4,\n",
                        "  \"n_heads\": 2,\n",
                        "  \"logits.shape\": [\n",
                        "    1,\n",
                        "    150,\n",
                        "    47\n",
                        "  ],\n",
                        "  \"attention_tensored.shape\": [\n",
                        "    8,\n",
                        "    150,\n",
                        "    150\n",
                        "  ],\n",
                        "  \"attention_names\": \"Layer 0 Head 0 Layer 0 Head 1 Layer 1 Head 0 Layer 1 Head 1 Layer 2 Head 0 Layer 2 Head 1 Layer 3 Head 0 Layer 3 Head 1\",\n",
                        "  \"attention_dict.keys()\": \"blocks.0.attn.hook_pattern blocks.1.attn.hook_pattern blocks.2.attn.hook_pattern blocks.3.attn.hook_pattern\"\n",
                        "}\n",
                        "attentions_from_token.shape = torch.Size([149, 8])\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div id=\"circuits-vis-26e3bbd5-3ef7\" style=\"margin: 15px 0;\"/>\n",
                            "    <script crossorigin type=\"module\">\n",
                            "    import { render, ColoredTokensMulti } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
                            "    render(\n",
                            "      \"circuits-vis-26e3bbd5-3ef7\",\n",
                            "      ColoredTokensMulti,\n",
                            "      {\"tokens\": [\"<ADJLIST_START>\", \"(3,5)\", \"<-->\", \"(4,5)\", \";\", \"(4,2)\", \"<-->\", \"(4,1)\", \";\", \"(4,5)\", \"<-->\", \"(5,5)\", \";\", \"(4,1)\", \"<-->\", \"(4,0)\", \";\", \"(1,0)\", \"<-->\", \"(2,0)\", \";\", \"(1,5)\", \"<-->\", \"(0,5)\", \";\", \"(1,4)\", \"<-->\", \"(1,5)\", \";\", \"(0,2)\", \"<-->\", \"(0,1)\", \";\", \"(0,3)\", \"<-->\", \"(0,4)\", \";\", \"(5,0)\", \"<-->\", \"(5,1)\", \";\", \"(4,0)\", \"<-->\", \"(5,0)\", \";\", \"(0,3)\", \"<-->\", \"(0,2)\", \";\", \"(3,2)\", \"<-->\", \"(3,1)\", \";\", \"(2,3)\", \"<-->\", \"(1,3)\", \";\", \"(0,2)\", \"<-->\", \"(1,2)\", \";\", \"(2,2)\", \"<-->\", \"(1,2)\", \";\", \"(4,3)\", \"<-->\", \"(3,3)\", \";\", \"(0,4)\", \"<-->\", \"(1,4)\", \";\", \"(4,3)\", \"<-->\", \"(4,4)\", \";\", \"(3,1)\", \"<-->\", \"(2,1)\", \";\", \"(4,2)\", \"<-->\", \"(4,3)\", \";\", \"(5,1)\", \"<-->\", \"(5,2)\", \";\", \"(3,0)\", \"<-->\", \"(2,0)\", \";\", \"(5,4)\", \"<-->\", \"(5,3)\", \";\", \"(5,4)\", \"<-->\", \"(5,5)\", \";\", \"(0,0)\", \"<-->\", \"(0,1)\", \";\", \"(2,5)\", \"<-->\", \"(1,5)\", \";\", \"(1,0)\", \"<-->\", \"(1,1)\", \";\", \"(5,3)\", \"<-->\", \"(5,2)\", \";\", \"(2,4)\", \"<-->\", \"(2,3)\", \";\", \"(1,0)\", \"<-->\", \"(0,0)\", \";\", \"(3,3)\", \"<-->\", \"(3,2)\", \";\", \"(2,4)\", \"<-->\", \"(2,5)\", \";\", \"(3,4)\", \"<-->\", \"(3,5)\", \";\", \"(2,2)\", \"<-->\", \"(2,1)\", \";\", \"<ADJLIST_END>\", \"<ORIGIN_START>\", \"(2,5)\", \"<ORIGIN_END>\", \"<TARGET_START>\", \"(2,1)\", \"<TARGET_END>\", \"<PATH_START>\"], \"values\": [[0.2664419710636139, 0.6601418256759644, 0.35105016827583313, 0.4654652774333954, 0.2718855142593384, 0.42835524678230286, 0.51409512758255, 0.4496367275714874], [0.48914021253585815, 0.4778677225112915, 0.7245819568634033, 0.5881588459014893, 0.5859537124633789, 0.7799190282821655, 0.6274685859680176, 0.6688582897186279], [0.9354619979858398, 0.8539270162582397, 0.7997384667396545, 0.7049201726913452, 0.5414506196975708, 0.7085213661193848, 0.686504602432251, 0.6788873672485352], [0.8891892433166504, 0.7725356221199036, 0.8682423830032349, 0.7426905035972595, 0.6010853052139282, 0.7571929693222046, 0.783152163028717, 0.6612164378166199], [0.8435062170028687, 0.700192928314209, 0.8266695141792297, 0.7468677759170532, 0.7252665162086487, 0.9049749970436096, 0.8578289151191711, 0.8752743005752563], [0.814733624458313, 0.9166727066040039, 0.9154214859008789, 0.8177432417869568, 0.8391900658607483, 0.8824352622032166, 0.8238683938980103, 0.7089328169822693], [0.8934230208396912, 0.9329145550727844, 0.9427270293235779, 0.7799926400184631, 0.7663639783859253, 0.8430415391921997, 0.8373420238494873, 0.6469041705131531], [0.9271904826164246, 0.9219649434089661, 0.9520559310913086, 0.8077860474586487, 0.9484574794769287, 0.9121349453926086, 0.9356337189674377, 0.8538737893104553], [0.81516432762146, 0.8893427848815918, 0.9054336547851562, 0.7837725877761841, 0.9079259037971497, 0.9864370822906494, 0.953151285648346, 0.8963643908500671], [0.9759190082550049, 0.8352667689323425, 0.9720379710197449, 0.7272958755493164, 0.9182356595993042, 0.9636892080307007, 0.9513055682182312, 0.8713924884796143], [0.9200330376625061, 0.9394555687904358, 0.9016525745391846, 0.8593361973762512, 0.923873245716095, 0.9509550333023071, 0.8731355667114258, 0.9089988470077515], [0.9537935853004456, 0.9444754719734192, 0.9330312013626099, 0.9065104722976685, 0.9636690020561218, 0.9468121528625488, 0.9480876922607422, 0.8154289126396179], [0.8908065557479858, 0.8869905471801758, 0.9091362953186035, 0.8904870748519897, 0.9458783864974976, 0.9908403754234314, 0.9397181272506714, 0.9469889402389526], [0.9083039164543152, 0.9497547745704651, 0.9585112929344177, 0.8985257744789124, 0.9454374313354492, 0.9484367370605469, 0.9342053532600403, 0.9354502558708191], [0.9721002578735352, 0.9309762120246887, 0.9568096399307251, 0.9084466695785522, 0.9422464966773987, 0.9586457014083862, 0.8722401857376099, 0.8871331810951233], [0.9480167627334595, 0.9247953295707703, 0.9585388898849487, 0.9610464572906494, 0.9836479425430298, 0.9765791893005371, 0.8558672666549683, 0.9131333827972412], [0.9560892581939697, 0.8969300389289856, 0.9583245515823364, 0.8779032826423645, 0.9705135226249695, 0.9805281758308411, 0.9585112929344177, 0.9184556007385254], [0.9825870990753174, 0.9365414381027222, 0.974283754825592, 0.9337027072906494, 0.9815618991851807, 0.974724292755127, 0.8692867159843445, 0.9118818044662476], [0.9637296199798584, 0.9487543702125549, 0.965402364730835, 0.9718111157417297, 0.9723402261734009, 0.9621564149856567, 0.865439236164093, 0.8588209748268127], [0.9111126065254211, 0.9772119522094727, 0.9154092669487, 0.9461828470230103, 0.9906398057937622, 0.9869484901428223, 0.9594223499298096, 0.9223151206970215], [0.8856266140937805, 0.9231434464454651, 0.9671117067337036, 0.9725037813186646, 0.960972785949707, 0.9892426133155823, 0.9318090081214905, 0.8901901245117188], [0.944769024848938, 0.9531606435775757, 0.9680330753326416, 0.9156330823898315, 0.9776692986488342, 0.9811559915542603, 0.9161914587020874, 0.9729433059692383], [0.9789012670516968, 0.9301698207855225, 0.9569771885871887, 0.9807148575782776, 0.9755780100822449, 0.9747222661972046, 0.8902204632759094, 0.8726617693901062], [0.977616548538208, 0.9713518619537354, 0.9743524789810181, 0.9803850054740906, 0.9864237904548645, 0.9666606187820435, 0.9244934916496277, 0.7513583302497864], [0.9628651738166809, 0.9521966576576233, 0.9487133622169495, 0.9490189552307129, 0.9762910008430481, 0.9949944019317627, 0.9613955020904541, 0.9586193561553955], [0.9543760418891907, 0.9599622488021851, 0.967467725276947, 0.9908176064491272, 0.9755896329879761, 0.978461742401123, 0.9020555019378662, 0.870505690574646], [0.9750913381576538, 0.9758995771408081, 0.9652494192123413, 0.9803562164306641, 0.9824636578559875, 0.9862952828407288, 0.9215224981307983, 0.9240421056747437], [0.8846529722213745, 0.9791008234024048, 0.9586617946624756, 0.9814224243164062, 0.973526120185852, 0.9831944108009338, 0.8853798508644104, 0.9406505823135376], [0.9661269783973694, 0.9262886643409729, 0.9597298502922058, 0.9570267200469971, 0.9824512004852295, 0.9947257041931152, 0.9633514881134033, 0.93839031457901], [0.9777184724807739, 0.9708098769187927, 0.9499304890632629, 0.9924229383468628, 0.978395938873291, 0.9888331294059753, 0.9289482831954956, 0.8780490159988403], [0.9815205931663513, 0.9810027480125427, 0.9708174467086792, 0.9918211102485657, 0.9806538820266724, 0.9872320890426636, 0.8894108533859253, 0.9230868816375732], [0.9820749759674072, 0.9720403552055359, 0.9663850665092468, 0.9864451289176941, 0.9919184446334839, 0.9873409271240234, 0.9175981879234314, 0.9278745055198669], [0.9669229984283447, 0.9412003755569458, 0.9517448544502258, 0.9579663872718811, 0.9793244004249573, 0.9934104681015015, 0.9773644804954529, 0.9736008048057556], [0.9877699017524719, 0.9176673889160156, 0.9808856248855591, 0.9196118116378784, 0.9879587888717651, 0.9931082725524902, 0.9544721841812134, 0.8933621644973755], [0.9558296203613281, 0.9808284640312195, 0.9666164517402649, 0.991386353969574, 0.9892235398292542, 0.9827572107315063, 0.9059560298919678, 0.9540524482727051], [0.974650502204895, 0.9882597923278809, 0.9682060480117798, 0.9909573793411255, 0.9885174036026001, 0.9832373261451721, 0.9402393102645874, 0.8808701634407043], [0.9244154095649719, 0.9590011835098267, 0.9714853763580322, 0.9790499210357666, 0.9820241928100586, 0.9963526725769043, 0.9644114971160889, 0.9609583020210266], [0.9779613614082336, 0.9214743375778198, 0.9846502542495728, 0.9801788330078125, 0.9936777353286743, 0.9951291084289551, 0.9513135552406311, 0.9495792984962463], [0.9883241653442383, 0.9853160977363586, 0.973267138004303, 0.995663046836853, 0.9893608093261719, 0.9865025281906128, 0.8727805614471436, 0.9452118277549744], [0.9903553128242493, 0.9857320785522461, 0.9665684103965759, 0.980262279510498, 0.9904038906097412, 0.9937436580657959, 0.9479796290397644, 0.9303674101829529], [0.9665944576263428, 0.9393703937530518, 0.956823468208313, 0.9570521712303162, 0.9778058528900146, 0.9970129132270813, 0.9817288517951965, 0.9672972559928894], [0.9715420007705688, 0.9762630462646484, 0.9489467144012451, 0.9834676384925842, 0.9911929368972778, 0.9918159246444702, 0.9556339383125305, 0.9726073145866394], [0.9789875745773315, 0.9798443913459778, 0.9735332727432251, 0.9877604842185974, 0.9873075485229492, 0.9916695356369019, 0.9228879809379578, 0.9438239932060242], [0.9838385581970215, 0.9585996270179749, 0.9795387983322144, 0.986939549446106, 0.9877922534942627, 0.9941184520721436, 0.9244346618652344, 0.9543126225471497], [0.9728955626487732, 0.9569748044013977, 0.9609987139701843, 0.9767270088195801, 0.9829692840576172, 0.9965435266494751, 0.9686728715896606, 0.952427864074707], [0.9884657859802246, 0.9666565656661987, 0.9755823612213135, 0.9825459718704224, 0.9918949604034424, 0.9902973175048828, 0.9558559060096741, 0.8865689039230347], [0.9782307147979736, 0.9893327951431274, 0.9716010689735413, 0.9913387298583984, 0.9884881377220154, 0.9864326119422913, 0.913507342338562, 0.9537410736083984], [0.9932502508163452, 0.9835879802703857, 0.9846567511558533, 0.9651739597320557, 0.9879581928253174, 0.9953566789627075, 0.9373669028282166, 0.9502596259117126], [0.971228301525116, 0.9770048260688782, 0.9597021341323853, 0.9736459851264954, 0.9882737398147583, 0.9967846274375916, 0.9704190492630005, 0.9671973586082458], [0.9873822331428528, 0.9775876998901367, 0.9697861671447754, 0.9807475209236145, 0.9941996335983276, 0.9937989711761475, 0.9601538777351379, 0.9600246548652649], [0.9842727184295654, 0.9821600914001465, 0.9739421010017395, 0.9847060441970825, 0.9935032725334167, 0.9874057769775391, 0.9404932260513306, 0.9644930958747864], [0.997681736946106, 0.9745948910713196, 0.9878334403038025, 0.9865571856498718, 0.9891338348388672, 0.9950779676437378, 0.9469253420829773, 0.9606071710586548], [0.9634940028190613, 0.9825443625450134, 0.9697867631912231, 0.9796480536460876, 0.9886376261711121, 0.9973827004432678, 0.9555909633636475, 0.9697248339653015], [0.9949371814727783, 0.9654221534729004, 0.9802631139755249, 0.986247718334198, 0.9905280470848083, 0.990943193435669, 0.9602529406547546, 0.929246723651886], [0.9846588373184204, 0.9751678705215454, 0.9685478210449219, 0.983685314655304, 0.9893149137496948, 0.9920667409896851, 0.9525612592697144, 0.973532497882843], [0.9908773899078369, 0.9844375252723694, 0.9635783433914185, 0.9912214875221252, 0.990269660949707, 0.9928578734397888, 0.9664373993873596, 0.9309413433074951], [0.9819128513336182, 0.978547990322113, 0.9698728322982788, 0.985424816608429, 0.9939180016517639, 0.9968116283416748, 0.9715380072593689, 0.9753439426422119], [0.9952553510665894, 0.9856372475624084, 0.9714886546134949, 0.9961698055267334, 0.9912432432174683, 0.9955360889434814, 0.9200032949447632, 0.9747442007064819], [0.990851104259491, 0.9837080240249634, 0.9735874533653259, 0.9840269684791565, 0.9903369545936584, 0.993384599685669, 0.9552584886550903, 0.9754259586334229], [0.9422361254692078, 0.9838306307792664, 0.9813895225524902, 0.992598831653595, 0.9935510754585266, 0.9866641759872437, 0.9308678507804871, 0.942139744758606], [0.977307915687561, 0.9688906669616699, 0.9724292159080505, 0.9772553443908691, 0.9881867170333862, 0.9987164735794067, 0.974484384059906, 0.9785041809082031], [0.9541049003601074, 0.9879811406135559, 0.9809613227844238, 0.983515739440918, 0.9954934120178223, 0.9922059178352356, 0.9603338837623596, 0.9820043444633484], [0.99489426612854, 0.9913643002510071, 0.9838326573371887, 0.9893306493759155, 0.9900598526000977, 0.9916398525238037, 0.9417742490768433, 0.9581701159477234], [0.9870803952217102, 0.9885018467903137, 0.98871248960495, 0.9953681230545044, 0.9936738014221191, 0.989313006401062, 0.9118646383285522, 0.8874390125274658], [0.9824334383010864, 0.9664233922958374, 0.9661723375320435, 0.9836557507514954, 0.9832485318183899, 0.997625470161438, 0.979598879814148, 0.9784848093986511], [0.9748697876930237, 0.9801086783409119, 0.9738593101501465, 0.9902303814888, 0.9937934279441833, 0.9960501194000244, 0.9308934211730957, 0.9613710045814514], [0.9854221940040588, 0.9909427165985107, 0.9805914163589478, 0.9950792193412781, 0.9921186566352844, 0.9848796725273132, 0.9225334525108337, 0.9532226920127869], [0.9887608289718628, 0.9829420447349548, 0.984713613986969, 0.9903455972671509, 0.9949289560317993, 0.9971276521682739, 0.9549161791801453, 0.9501200914382935], [0.9773433208465576, 0.9729050397872925, 0.9846157431602478, 0.9774435758590698, 0.9904508590698242, 0.9989961981773376, 0.9822055697441101, 0.9599308967590332], [0.9920356869697571, 0.9876793622970581, 0.9856747388839722, 0.9945504665374756, 0.9924903512001038, 0.9949330687522888, 0.9533573389053345, 0.9320052266120911], [0.9890914559364319, 0.984612762928009, 0.9831002950668335, 0.9922433495521545, 0.9927474856376648, 0.9912827014923096, 0.9215121269226074, 0.9897803068161011], [0.982734739780426, 0.9917444586753845, 0.9733433127403259, 0.9965026378631592, 0.9913763999938965, 0.9951961040496826, 0.9543382525444031, 0.9415359497070312], [0.9929705262184143, 0.989072322845459, 0.9693605899810791, 0.9853255748748779, 0.9929928183555603, 0.9981334805488586, 0.978754460811615, 0.9874016046524048], [0.9795805811882019, 0.9912886619567871, 0.9895342588424683, 0.9927568435668945, 0.9891679286956787, 0.9895844459533691, 0.9092033505439758, 0.9297244548797607], [0.9910994172096252, 0.9913548827171326, 0.9849386215209961, 0.9935546517372131, 0.9919425249099731, 0.989506185054779, 0.9415276646614075, 0.9575671553611755], [0.9976706504821777, 0.9825555682182312, 0.9848301410675049, 0.9957789182662964, 0.9897544384002686, 0.9909754991531372, 0.9225648641586304, 0.9591826796531677], [0.9902967810630798, 0.9707702398300171, 0.9732717871665955, 0.9851953387260437, 0.987085223197937, 0.9986051321029663, 0.9821382761001587, 0.9833592772483826], [0.9904754161834717, 0.9931219816207886, 0.9924311637878418, 0.9945902824401855, 0.99351966381073, 0.9952993392944336, 0.9432710409164429, 0.9539378881454468], [0.9734695553779602, 0.98929762840271, 0.9846086502075195, 0.9941590428352356, 0.9928532242774963, 0.9851759672164917, 0.9368893504142761, 0.9765821695327759], [0.9774901866912842, 0.9933106899261475, 0.9823154211044312, 0.9938151240348816, 0.9940241575241089, 0.9944833517074585, 0.9531959295272827, 0.9571921229362488], [0.9748196601867676, 0.9854245185852051, 0.9825399518013, 0.9915287494659424, 0.9907264113426208, 0.9980995655059814, 0.9701493978500366, 0.9579035043716431], [0.9816223978996277, 0.9918097257614136, 0.9792877435684204, 0.9949039220809937, 0.9935606718063354, 0.9956201314926147, 0.9551881551742554, 0.9624985456466675], [0.9860323071479797, 0.9935039281845093, 0.982875645160675, 0.9961982369422913, 0.9940879940986633, 0.9937706589698792, 0.9206297397613525, 0.9777896404266357], [0.9919100403785706, 0.9870035648345947, 0.9881976842880249, 0.9943504929542542, 0.9938601851463318, 0.9959229826927185, 0.9372234344482422, 0.9441146850585938], [0.9873983263969421, 0.9798905849456787, 0.9745206832885742, 0.9903010725975037, 0.990106999874115, 0.9988137483596802, 0.9829404354095459, 0.9845021367073059], [0.9949311017990112, 0.9940625429153442, 0.9790180921554565, 0.9930015802383423, 0.9945456981658936, 0.9966942071914673, 0.9485741853713989, 0.9688042402267456], [0.9917088150978088, 0.9949929714202881, 0.9820274114608765, 0.9928972721099854, 0.9954805970191956, 0.9944442510604858, 0.9555953145027161, 0.973401665687561], [0.9905032515525818, 0.9868223071098328, 0.9904206991195679, 0.9914005994796753, 0.9969207048416138, 0.9971723556518555, 0.9478665590286255, 0.9557017087936401], [0.9845632314682007, 0.9829323291778564, 0.9766058921813965, 0.9869159460067749, 0.9892221093177795, 0.9981614947319031, 0.9871718287467957, 0.9896084070205688], [0.9922568798065186, 0.9912152290344238, 0.9806101322174072, 0.9970962405204773, 0.9976388216018677, 0.9946903586387634, 0.9298036098480225, 0.9931755661964417], [0.9844588041305542, 0.9926737546920776, 0.9874099493026733, 0.9939402341842651, 0.9942790269851685, 0.9936474561691284, 0.9442000389099121, 0.9757483005523682], [0.9868784546852112, 0.9928510785102844, 0.9725461602210999, 0.9953764081001282, 0.9950800538063049, 0.9963067770004272, 0.9635208249092102, 0.9736671447753906], [0.9899348020553589, 0.9868281483650208, 0.9735245704650879, 0.9868994951248169, 0.9884066581726074, 0.9987951517105103, 0.9896097183227539, 0.9928957223892212], [0.9866346120834351, 0.9920995235443115, 0.9832077026367188, 0.9968099594116211, 0.9907702803611755, 0.9979169368743896, 0.9669884443283081, 0.9807843565940857], [0.9944964647293091, 0.9925989508628845, 0.9887162446975708, 0.9963645339012146, 0.993013858795166, 0.9931161403656006, 0.9262542724609375, 0.9728761911392212], [0.9913152456283569, 0.9935870170593262, 0.9805774092674255, 0.9956886768341064, 0.9934393763542175, 0.9972763061523438, 0.95638507604599, 0.9765164852142334], [0.9789284467697144, 0.9800370931625366, 0.9779818058013916, 0.9865926504135132, 0.99164879322052, 0.9991905689239502, 0.9806767702102661, 0.9852811098098755], [0.9879552125930786, 0.9957413673400879, 0.9834681153297424, 0.9982066750526428, 0.9920557141304016, 0.9962952733039856, 0.9410566091537476, 0.9866970777511597], [0.9934324026107788, 0.9950006008148193, 0.9878728985786438, 0.9942238330841064, 0.9924293756484985, 0.9951550960540771, 0.9543585181236267, 0.9746145009994507], [0.9923743605613708, 0.9813737273216248, 0.9894078969955444, 0.99530029296875, 0.9966768026351929, 0.9978039860725403, 0.9549912810325623, 0.9668460488319397], [0.9945920705795288, 0.9846395254135132, 0.9800623059272766, 0.992396891117096, 0.9904309511184692, 0.998192310333252, 0.9833027124404907, 0.9866517782211304], [0.981320858001709, 0.9903551340103149, 0.9841853976249695, 0.9862690567970276, 0.9960784316062927, 0.9980992674827576, 0.9726025462150574, 0.9715386629104614], [0.997416079044342, 0.9926217198371887, 0.9897732138633728, 0.9951374530792236, 0.9944272041320801, 0.9944398403167725, 0.9467440247535706, 0.9633532166481018], [0.9974683523178101, 0.9927642941474915, 0.984166145324707, 0.9947147965431213, 0.9954996109008789, 0.9981515407562256, 0.9561206102371216, 0.9697180986404419], [0.9885615110397339, 0.9804837703704834, 0.9861115217208862, 0.9952359795570374, 0.9957606792449951, 0.9984780550003052, 0.9800428152084351, 0.9860278367996216], [0.9928524494171143, 0.9928775429725647, 0.9830543994903564, 0.9925647974014282, 0.9962666034698486, 0.9981011152267456, 0.945833146572113, 0.9646729826927185], [0.9932585954666138, 0.9939519166946411, 0.980716347694397, 0.9951951503753662, 0.995464563369751, 0.9939612746238708, 0.9524152874946594, 0.9881784915924072], [0.962978184223175, 0.988561749458313, 0.9844727516174316, 0.994075357913971, 0.9959827065467834, 0.9961295127868652, 0.9646967053413391, 0.9799531102180481], [0.9854446649551392, 0.9817535877227783, 0.9826409220695496, 0.9852484464645386, 0.9936615228652954, 0.9990991950035095, 0.9856702089309692, 0.9936419129371643], [0.9978424906730652, 0.9885789155960083, 0.9861894845962524, 0.9977782368659973, 0.9940294623374939, 0.9934966564178467, 0.9449713230133057, 0.9861620664596558], [0.9867711067199707, 0.9899287223815918, 0.9854625463485718, 0.995366632938385, 0.9944745898246765, 0.9938784241676331, 0.9490572214126587, 0.9811471104621887], [0.9934128522872925, 0.9779818058013916, 0.9862318634986877, 0.9932483434677124, 0.9960086941719055, 0.9976493716239929, 0.9666186571121216, 0.974862813949585], [0.9911248683929443, 0.994035005569458, 0.9859790802001953, 0.9921593070030212, 0.9955190420150757, 0.9992032051086426, 0.9776008129119873, 0.984403133392334], [0.9915934801101685, 0.986141562461853, 0.9891345500946045, 0.9944977164268494, 0.9964240193367004, 0.9977144002914429, 0.9660806655883789, 0.9607581496238708], [0.9892587661743164, 0.9954628348350525, 0.9899688959121704, 0.9958540797233582, 0.9947675466537476, 0.9957883954048157, 0.9449478983879089, 0.9817211627960205], [0.9931427240371704, 0.9963041543960571, 0.9869464635848999, 0.9961568713188171, 0.9955742359161377, 0.99510657787323, 0.9468759298324585, 0.9587498307228088], [0.9725685715675354, 0.9843589663505554, 0.9847511053085327, 0.9928832650184631, 0.993703305721283, 0.9983274936676025, 0.9795152544975281, 0.9672554731369019], [0.9909576177597046, 0.9975468516349792, 0.9886340498924255, 0.9980455636978149, 0.9946472644805908, 0.9964728355407715, 0.9593590497970581, 0.9658690094947815], [0.9926620721817017, 0.9951372742652893, 0.9876493215560913, 0.9946571588516235, 0.9948890805244446, 0.9964594841003418, 0.9592533111572266, 0.9907201528549194], [0.9985416531562805, 0.9919352531433105, 0.9916059374809265, 0.9927347302436829, 0.9943568706512451, 0.9976301193237305, 0.9678629636764526, 0.9565460681915283], [0.9910136461257935, 0.9899433851242065, 0.9802720546722412, 0.9906550049781799, 0.9925037026405334, 0.9987841248512268, 0.9882615804672241, 0.9879588484764099], [0.9961634874343872, 0.9877692461013794, 0.9906811714172363, 0.9916960597038269, 0.9973745346069336, 0.9965550899505615, 0.9635138511657715, 0.9775009155273438], [0.9897485375404358, 0.9938640594482422, 0.9850856065750122, 0.9971703886985779, 0.9955834746360779, 0.9941225051879883, 0.9501249194145203, 0.986356258392334], [0.991823673248291, 0.9849913120269775, 0.9923598170280457, 0.973862886428833, 0.997508704662323, 0.9988124966621399, 0.9801178574562073, 0.9789653420448303], [0.9914572238922119, 0.9839307069778442, 0.9836779832839966, 0.980453372001648, 0.9932119250297546, 0.9992189407348633, 0.9856185913085938, 0.9850360751152039], [0.9963915944099426, 0.9914132356643677, 0.9919585585594177, 0.995042085647583, 0.9981069564819336, 0.9971279501914978, 0.9712998867034912, 0.9657109975814819], [0.9957103133201599, 0.9934387803077698, 0.9892563819885254, 0.9960392117500305, 0.9956086874008179, 0.9969596862792969, 0.964170515537262, 0.9855509400367737], [0.9905027151107788, 0.9958929419517517, 0.9918034076690674, 0.9964430332183838, 0.9963372945785522, 0.9965429902076721, 0.9536784291267395, 0.9645658731460571], [0.9954104423522949, 0.9788666367530823, 0.9871460199356079, 0.9946218729019165, 0.9938198924064636, 0.9992592334747314, 0.9826308488845825, 0.9909558296203613], [0.9877438545227051, 0.9910739064216614, 0.9887276887893677, 0.9973575472831726, 0.995034396648407, 0.9933803081512451, 0.967009961605072, 0.9672889709472656], [0.9956916570663452, 0.9963890314102173, 0.9901351928710938, 0.9969117045402527, 0.9949173331260681, 0.9931350350379944, 0.9577431082725525, 0.9844164252281189], [0.9952806830406189, 0.9959619045257568, 0.9844006299972534, 0.996162474155426, 0.9964030981063843, 0.9969795942306519, 0.9614949822425842, 0.9620755314826965], [0.9945572018623352, 0.9706738591194153, 0.9851037263870239, 0.9900081753730774, 0.9954399466514587, 0.9986289739608765, 0.9842989444732666, 0.990283727645874], [0.987662136554718, 0.9865973591804504, 0.9906172156333923, 0.9917044043540955, 0.996725857257843, 0.9989955425262451, 0.968177080154419, 0.9662432074546814], [0.9945536255836487, 0.9873430132865906, 0.9913330078125, 0.9965882301330566, 0.9946655631065369, 0.996783435344696, 0.9510519504547119, 0.9856200814247131], [0.991036593914032, 0.9868385791778564, 0.9933963418006897, 0.9863625168800354, 0.9971533417701721, 0.999251127243042, 0.9756668210029602, 0.9857887029647827], [0.9917478561401367, 0.9885572195053101, 0.983664333820343, 0.9937227368354797, 0.9939761161804199, 0.9991466403007507, 0.9850915670394897, 0.9896868467330933], [0.9954798817634583, 0.9927133917808533, 0.9903358221054077, 0.9961003065109253, 0.9969653487205505, 0.9959363341331482, 0.9646202325820923, 0.9833781719207764], [0.9929453730583191, 0.9944437742233276, 0.9917733073234558, 0.9974679350852966, 0.9962235689163208, 0.9967293739318848, 0.9497455358505249, 0.9833458065986633], [0.9895868897438049, 0.9939074516296387, 0.9848933219909668, 0.9928227663040161, 0.9967368841171265, 0.9965547323226929, 0.98208087682724, 0.9877572059631348], [0.9866837859153748, 0.9874176383018494, 0.9827427268028259, 0.987189769744873, 0.9945911765098572, 0.998961329460144, 0.9910292625427246, 0.991192102432251], [0.9913026690483093, 0.9952656030654907, 0.9927374124526978, 0.9985634088516235, 0.9977661967277527, 0.9959186315536499, 0.927504301071167, 0.9636555314064026], [0.9961438179016113, 0.9918440580368042, 0.9898240566253662, 0.9947887659072876, 0.9964914321899414, 0.9989677667617798, 0.9585561752319336, 0.9911603331565857], [0.9963808059692383, 0.9932927489280701, 0.9855844974517822, 0.9881213903427124, 0.9970372319221497, 0.9991693496704102, 0.9747098684310913, 0.9856244921684265], [0.9938395619392395, 0.9977840185165405, 0.9915612936019897, 0.9969046115875244, 0.9978201389312744, 0.9981062412261963, 0.9666352272033691, 0.9752119183540344], [0.9957440495491028, 0.9950574636459351, 0.9915705323219299, 0.9893664121627808, 0.9960066676139832, 0.9970809817314148, 0.9743664264678955, 0.9797755479812622], [0.9957966804504395, 0.9936142563819885, 0.9931020736694336, 0.9970901608467102, 0.9967691898345947, 0.9960368871688843, 0.9541678428649902, 0.9756135940551758], [0.9913948178291321, 0.9972337484359741, 0.9906119108200073, 0.9969373941421509, 0.9972251653671265, 0.9973196387290955, 0.9612681269645691, 0.9749252796173096], [0.9973206520080566, 0.9943820834159851, 0.9905717372894287, 0.9954921007156372, 0.9944030046463013, 0.9989954233169556, 0.9744328856468201, 0.986289381980896]], \"labels\": [\"Layer 0 Head 0\", \"Layer 0 Head 1\", \"Layer 1 Head 0\", \"Layer 1 Head 1\", \"Layer 2 Head 0\", \"Layer 2 Head 1\", \"Layer 3 Head 0\", \"Layer 3 Head 1\"]}\n",
                            "    )\n",
                            "    </script>"
                        ],
                        "text/plain": [
                            "<circuitsvis.utils.render.RenderedHTML at 0x1d6be3ab6d0>"
                        ]
                    },
                    "execution_count": 48,
                    "metadata": {},
                    "output_type": "execute_result"
                },
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "ATTENTION_DATA: ProcessedMazeAttention = ProcessedMazeAttention.from_model_and_dataset(\n",
                "\tmodel=MODEL, dataset=DATASET_TEST, n_mazes=1,\n",
                ")[0]\n",
                "\n",
                "pprint_summary(MODEL.zanj_model_config.model_cfg.summary())\n",
                "pprint_summary(ATTENTION_DATA.summary())\n",
                "\n",
                "plt.imshow(ATTENTION_DATA.input_maze.as_pixels())\n",
                "\n",
                "# ATTENTION_DATA.plot_attentions()\n",
                "\n",
                "ATTENTION_DATA.plot_colored_tokens_multi()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "def prediction_contained_a_coordinate_token(tokens: list[str], tokenizer: HuggingMazeTokenizer) -> bool:\n",
                "\t\"\"\"Check if the prediction contains a coordinate token\"\"\"\n",
                "\tfor t in tokens:\n",
                "\t\tif t not in list(tokenizer.special_tokens_map.values()) + tokenizer.additional_special_tokens:\n",
                "\t\t\treturn True\n",
                "\tprint(\"FAIL: Sampled a path - No coordinate token found before EOS\")\n",
                "\treturn False\n",
                "\n",
                "predicted_tokens = []\n",
                "while not prediction_contained_a_coordinate_token(predicted_tokens, model.tokenizer):\n",
                "\tpredictions = model.generate(array_tensor, max_new_tokens=50, stop_at_eos=True, verbose=False)\n",
                "\tpredicted_tokens = model.to_str_tokens(predictions)[len(maze_only_tokens):]\n",
                "print(\"SUCCESS: Model predicted the path:\")\n",
                "print(predicted_tokens)\n",
                "\n",
                "path_predicted: list[tuple[int,int]] = tokens_to_coords(\n",
                "\tpredicted_tokens,\n",
                "\tmaze_data_cfg = cfg.dataset_cfg, \n",
                "\twhen_noncoord = \"skip\",\n",
                ")\n",
                "\n",
                "# plot the maze and both solutions\n",
                "# for label, fmt, color, path in paths\n",
                "MazePlot(maze).add_true_path(path_true).add_predicted_path(path_predicted).plot()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "maze-transformer",
            "language": "python",
            "name": "maze-transformer"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.1"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "170637793197da0d440deb6cb249c898d613b24c548839ecbbac11596710dfc2"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
