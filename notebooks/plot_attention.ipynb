{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "# Generic\n",
                "import html\n",
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "# Transformers\n",
                "from circuitsvis.attention import attention_heads\n",
                "from circuitsvis.tokens import colored_tokens_multi\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Numerical Computing\n",
                "import numpy as np\n",
                "import torch\n",
                "\n",
                "# Our Code\n",
                "from maze_transformer.utils.notebook_utils import configure_notebook\n",
                "from maze_transformer.generation.lattice_maze import LatticeMaze, SolvedMaze\n",
                "from maze_transformer.generation.generators import LatticeMazeGenerators\n",
                "from maze_transformer.training.tokenizer import SPECIAL_TOKENS, HuggingMazeTokenizer\n",
                "from maze_transformer.evaluation.plot_maze import MazePlot, PathFormat\n",
                "from maze_transformer.evaluation.eval_model import load_model_with_configs\n",
                "from maze_transformer.utils.token_utils import tokens_to_coords\n",
                "from maze_transformer.generation.constants import SPECIAL_TOKENS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "# Setup\n",
                "device = configure_notebook(seed=42, dark_mode=True)\n",
                "# We won't be training any models\n",
                "torch.set_grad_enabled(False)\n",
                "\n",
                "\n",
                "# Get latest model\n",
                "# this should point towards a directory containing a run. \n",
                "# If you don't have any runs, you can create a dataset with `poetry run python scripts/create_dataset.py create ./data/maze 10 --grid_n=4`\n",
                "# Then train a model with poetry run python scripts/train_model.py ./data/maze/g4-n10`\n",
                "run_path = Path(\"../examples/maze/g4-n10\")\n",
                "assert run_path.exists(), f\"Run path {run_path.as_posix()} does not exist\"\n",
                "model_path = list(sorted(run_path.glob(\"**/model.final.pt\"), key=os.path.getmtime))[\n",
                "\t-1\n",
                "].resolve()\n",
                "model, cfg = load_model_with_configs(model_path)\n",
                "maze_path = run_path / \"maze_tokens.jsonl\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "# generate a maze\n",
                "grid_n: int = cfg.dataset_cfg.grid_n\n",
                "maze: LatticeMaze = LatticeMazeGenerators.gen_dfs((grid_n, grid_n))\n",
                "c_start = (0, 0)\n",
                "c_end = (grid_n - 1, grid_n - 1)\n",
                "\n",
                "# solve the maze explicitly\n",
                "path_true = np.array(maze.find_shortest_path(\n",
                "\tc_start = c_start,\n",
                "\tc_end = c_end,\n",
                "))\n",
                "\n",
                "solved_maze: SolvedMaze = SolvedMaze.from_lattice_maze(lattice_maze=maze, solution=path_true)\n",
                "\n",
                "# tokenize the maze\n",
                "tokens = solved_maze.to_tokens(cfg.dataset_cfg.node_token_map)\n",
                "path_start_index = tokens.index(SPECIAL_TOKENS[\"path_start\"])\n",
                "maze_only_tokens = tokens[:path_start_index + 1]\n",
                "\n",
                "print(\"maze tokens:\", maze_only_tokens)\n",
                "\n",
                "array = model.to_tokens(\" \".join(maze_only_tokens), prepend_bos=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "# have the model predict some tokens\n",
                "context_str: list[str] = maze_only_tokens\n",
                "\n",
                "# escape for html\n",
                "context_str = [ html.escape(t) for t in context_str ]\n",
                "\n",
                "array_tensor = torch.tensor(array).long().to(device)\n",
                "with torch.no_grad():\n",
                "\tlogits, cache = model.run_with_cache(array_tensor)\n",
                "\n",
                "attentions = [w for k, w in cache.items() if 'hook_pattern' in k]\n",
                "print(f\"{logits.shape = }\\n{len(attentions) = }\\n{[x.shape for x in attentions] = }\")\n",
                "\n",
                "# `output.attentions` is a tuple of tensors, where each element of the tuple corresponds to a layer. \n",
                "#  The tensor has dimensions (1, n_heads, n_positions, n_positions)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "n_layers: int = len(attentions)\n",
                "n_heads: int = attentions[0].shape[1]\n",
                "n_tokens: int = attentions[0].shape[2]\n",
                "attention_to_plot = torch.concatenate(attentions, dim=0).reshape(-1, n_tokens, n_tokens)\n",
                "attention_head_names = [f\"Layer {i} Head {j}\" for i in range(n_layers) for j in range(n_heads)]\n",
                "attention_heads(attention_to_plot,maze_only_tokens, attention_head_names)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "#! ALEX note - there used to be a np.power(head_np, 1/4) here, not sure what that's about?\n",
                "FROM_TOKEN = -1 # Look at attention from this token position to the rest of the sequence\n",
                "attentions_from_token = torch.concatenate([w[0, :, FROM_TOKEN, :] for w in attentions], dim=0)\n",
                "colored_tokens_multi(context_str, attentions_from_token.T, labels=attention_head_names)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "def prediction_contained_a_coordinate_token(tokens: list[str], tokenizer: HuggingMazeTokenizer) -> bool:\n",
                "\t\"\"\"Check if the prediction contains a coordinate token\"\"\"\n",
                "\tfor t in tokens:\n",
                "\t\tif t not in list(tokenizer.special_tokens_map.values()) + tokenizer.additional_special_tokens:\n",
                "\t\t\treturn True\n",
                "\tprint(\"FAIL: Sampled a path - No coordinate token found before EOS\")\n",
                "\treturn False\n",
                "\n",
                "predicted_tokens = []\n",
                "while not prediction_contained_a_coordinate_token(predicted_tokens, model.tokenizer):\n",
                "\tpredictions = model.generate(array_tensor, max_new_tokens=50, stop_at_eos=True, verbose=False)\n",
                "\tpredicted_tokens = model.to_str_tokens(predictions)[len(maze_only_tokens):]\n",
                "print(\"SUCCESS: Model predicted the path:\")\n",
                "print(predicted_tokens)\n",
                "\n",
                "path_predicted: list[tuple[int,int]] = tokens_to_coords(\n",
                "\tpredicted_tokens,\n",
                "\tmaze_data_cfg = cfg.dataset_cfg, \n",
                "\twhen_noncoord = \"skip\",\n",
                ")\n",
                "\n",
                "# plot the maze and both solutions\n",
                "# for label, fmt, color, path in paths\n",
                "MazePlot(maze).add_true_path(path_true).add_predicted_path(path_predicted).plot()\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "maze-transformer",
            "language": "python",
            "name": "maze-transformer"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "170637793197da0d440deb6cb249c898d613b24c548839ecbbac11596710dfc2"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
