{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic\n",
    "import typing\n",
    "import os\n",
    "from pathlib import Path\n",
    "import typing\n",
    "import json\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# muutils\n",
    "from zanj.zanj import ZANJ, ZANJ_GLOBAL_DEFAULTS\n",
    "\n",
    "# Our Code\n",
    "from muutils.nbutils.configure_notebook import configure_notebook\n",
    "from maze_transformer.training.config import ConfigHolder, ZanjHookedTransformer, BaseGPTConfig, TrainConfig\n",
    "from maze_dataset import MazeDataset, MazeDatasetConfig\n",
    "from maze_dataset.dataset.configs import MAZE_DATASET_CONFIGS\n",
    "from maze_dataset.generation import LatticeMazeGenerators\n",
    "from maze_transformer.training.train_model import TrainingResult, train_model\n",
    "from maze_transformer.training.wandb_logger import WandbProject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "# set global defaults for ZANJ\n",
    "ZANJ_GLOBAL_DEFAULTS.external_array_threshold = 1024\n",
    "ZANJ_GLOBAL_DEFAULTS.external_list_threshold = 1024\n",
    "\n",
    "# paths\n",
    "PATH_EXAMPLES: Path = Path(\"../examples/\")\n",
    "PATH_DATA: Path = Path(\"../data/\")\n",
    "\n",
    "# reproducibility and device\n",
    "DEVICE = configure_notebook(seed=42, dark_mode=True)\n",
    "print(f\"{DEVICE = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list(MAZE_DATASET_CONFIGS.keys()) = ['test-g3-n5-a_dfs-h89001', 'demo_small-g3-n100-a_dfs-h58410', 'demo-g6-n10K-a_dfs-h86254']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{list(MAZE_DATASET_CONFIGS.keys()) = }\")\n",
    "\n",
    "# if you want to specify a custom config, you can do so here\n",
    "CFG_CUSTOM: ConfigHolder = ConfigHolder(\n",
    "    name = \"hallway-medium\",\n",
    "    dataset_cfg = MazeDatasetConfig(\n",
    "\t\tname=\"custom-hallway\",\n",
    "\t\tgrid_n=8,\n",
    "\t\tn_mazes=100_000,\n",
    "\t\tmaze_ctor=LatticeMazeGenerators.gen_dfs,\n",
    "        maze_ctor_kwargs=dict(\n",
    "            do_forks=False,\n",
    "        ),\n",
    "\t),\n",
    "    model_cfg = BaseGPTConfig(\n",
    "        name=\"custom-model\",\n",
    "        act_fn=\"gelu\",\n",
    "        d_model=128,\n",
    "        d_head=32,\n",
    "        n_layers=6,\n",
    "    ),\n",
    "    train_cfg = TrainConfig(\n",
    "        name=\"custom-train\",\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer_kwargs=dict(lr=0.00001),\n",
    "        batch_size=32,\n",
    "        dataloader_cfg=dict(\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            drop_last=False,\n",
    "        ),\n",
    "        intervals_count=dict(\n",
    "            print_loss=1000,\n",
    "            checkpoint=20,\n",
    "            eval_fast=100,\n",
    "            eval_slow=50,\n",
    "        ),\n",
    "        validation_dataset_cfg=10,\n",
    "    ),\n",
    ")\n",
    "\n",
    "CFG_TEST: ConfigHolder = ConfigHolder(\n",
    "        name = \"hallway-nano\",\n",
    "    dataset_cfg = MazeDatasetConfig(\n",
    "\t\tname=\"custom-hallway\",\n",
    "\t\tgrid_n=3,\n",
    "\t\tn_mazes=50,\n",
    "\t\tmaze_ctor=LatticeMazeGenerators.gen_dfs,\n",
    "        maze_ctor_kwargs=dict(\n",
    "            do_forks=False,\n",
    "        ),\n",
    "\t),\n",
    "    model_cfg = BaseGPTConfig(\n",
    "        name=\"custom-model\",\n",
    "        act_fn=\"gelu\",\n",
    "        d_model=8,\n",
    "        d_head=2,\n",
    "        n_layers=2,\n",
    "    ),\n",
    "    train_cfg = TrainConfig(\n",
    "        name=\"custom-train\",\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        optimizer_kwargs=dict(lr=0.0001),\n",
    "        batch_size=4,\n",
    "        dataloader_cfg=dict(\n",
    "            shuffle=True,\n",
    "            drop_last=False,\n",
    "        ),\n",
    "       validation_dataset_cfg=5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is where to specify which config to actually use\n",
    "CFG: ConfigHolder = CFG_CUSTOM # change to CFG_CUSTOM to train a \"real\" model, the CFG_TEST is for CI testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"hallway-medium\",\n",
      "  \"dataset_cfg\": {\n",
      "    \"name\": \"custom-hallway\",\n",
      "    \"fname\": \"custom-hallway-g8-n100K-a_dfs-h31024\",\n",
      "    \"sdc_hash\": 43825844172657897425957296941560401936207458692340074698180715204853252031024,\n",
      "    \"seed\": 42,\n",
      "    \"seq_len_min\": 1,\n",
      "    \"seq_len_max\": 512,\n",
      "    \"padding_token_index\": 10,\n",
      "    \"token_arr_joined\": \"<ADJLIST_START> <ADJLIST_END> <TARGET_START> <TARGET_END> <ORIGIN_START> <ORIGIN_END> <PATH_START> <PATH_END> <--> ; <PADDING> (0,0) (0,1) (1,0) (1,1) (0,2) (2,0) (1,2) (2,1) (2,2) (0,3) (3,0) (3,1) (2,3) (3,2) (1,3) (3,3) (0,4) (2,4) (4,0) (1,4) (4,1) (4,2) (3,4) (4,3) (4,4) (0,5) (5,0) (5,1) (2,5) (5,2) (5,3) (4,5) (5,4) (1,5) (3,5) (5,5) (0,6) (2,6) (4,6) (6,0) (1,6) (6,1) (6,2) (3,6) (6,3) (6,4) (5,6) (6,5) (6,6) (0,7) (7,0) (7,1) (2,7) (7,2) (7,3) (4,7) (7,4) (7,5) (6,7) (7,6) (1,7) (3,7) (5,7) (7,7)\",\n",
      "    \"applied_filters\": [],\n",
      "    \"grid_n\": 8,\n",
      "    \"grid_shape\": [\n",
      "      8,\n",
      "      8\n",
      "    ],\n",
      "    \"n_mazes\": 100000,\n",
      "    \"maze_ctor_name\": \"gen_dfs\",\n",
      "    \"maze_ctor_kwargs\": {\n",
      "      \"do_forks\": false\n",
      "    }\n",
      "  },\n",
      "  \"model_cfg\": {\n",
      "    \"name\": \"custom-model\",\n",
      "    \"act_fn\": \"gelu\",\n",
      "    \"d_model\": 128,\n",
      "    \"d_head\": 32,\n",
      "    \"n_layers\": 6,\n",
      "    \"weight_processing\": {\n",
      "      \"are_layernorms_folded\": false,\n",
      "      \"are_weights_processed\": false\n",
      "    },\n",
      "    \"n_heads\": 4\n",
      "  },\n",
      "  \"train_cfg\": {\n",
      "    \"name\": \"custom-train\",\n",
      "    \"optimizer\": \"AdamW\",\n",
      "    \"optimizer_kwargs\": {\n",
      "      \"lr\": 1e-05\n",
      "    },\n",
      "    \"batch_size\": 32,\n",
      "    \"dataloader_cfg\": {\n",
      "      \"shuffle\": true,\n",
      "      \"num_workers\": 4,\n",
      "      \"drop_last\": false\n",
      "    },\n",
      "    \"intervals\": null,\n",
      "    \"intervals_count\": {\n",
      "      \"print_loss\": 1000,\n",
      "      \"checkpoint\": 20,\n",
      "      \"eval_fast\": 100,\n",
      "      \"eval_slow\": 50\n",
      "    },\n",
      "    \"evals_max_new_tokens\": 8,\n",
      "    \"validation_dataset_cfg\": 10\n",
      "  },\n",
      "  \"pretrainedtokenizer_kwargs\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(CFG.summary(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeing if we can download the dataset...\n",
      "no download found, or download failed\n",
      "generating dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generating & solving mazes: 100%|██████████| 100000/100000 [02:51<00:00, 581.55maze/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving dataset to ..\\data\\custom-hallway-g8-n100K-a_dfs-h31024.zanj\n",
      "Got dataset custom-hallway with 100000 items. output.cfg.to_fname() = 'custom-hallway-g8-n100K-a_dfs-h31024'\n"
     ]
    }
   ],
   "source": [
    "# get just the dataset, generating it if needed. \n",
    "# This step can be skipped if you set `do_generate_dataset=True` when calling `train_model`\n",
    "# or if the dataset in question already exists\n",
    "\n",
    "# load the dataset\n",
    "DATASET: MazeDataset = MazeDataset.from_config(\n",
    "    CFG.dataset_cfg, \n",
    "    verbose=True, \n",
    "    load_local=False,\n",
    "    local_base_path=PATH_DATA,\n",
    ").filter_by.collect_generation_meta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET.save(PATH_DATA / DATASET.cfg.to_fname())\n",
    "CFG.dataset_cfg = DATASET.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-15 03:51:26 ERROR Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmiv\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\KNC\\maze-transformer\\notebooks\\wandb\\run-20230615_035128-65sb0m33</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/miv/demo-notebooks/runs/65sb0m33' target=\"_blank\">curious-serenity-35</a></strong> to <a href='https://wandb.ai/miv/demo-notebooks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/miv/demo-notebooks' target=\"_blank\">https://wandb.ai/miv/demo-notebooks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/miv/demo-notebooks/runs/65sb0m33' target=\"_blank\">https://wandb.ai/miv/demo-notebooks/runs/65sb0m33</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-15 03:51:29 INFO config ={'__format__': 'ConfigHolder(SerializableDataclass)', 'dataset_cfg': {'__format__': 'MazeDatasetConfig(SerializableDataclass)', 'name': 'custom-hallway', 'seq_len_min': 1, 'seq_len_max': 512, 'seed': 42, 'applied_filters': [{'name': 'collect_generation_meta', 'args': (), 'kwargs': {}}], 'grid_n': 8, 'n_mazes': 100000, 'maze_ctor': {'__name__': 'gen_dfs', '__module__': 'maze_dataset.generation.generators', '__doc__': ['generate a lattice maze using depth first search, iterative', '', '        # Arguments', '        - `grid_shape: Coord`: the shape of the grid', '        - `lattice_dim: int`: the dimension of the lattice', '          (default: `2`)', '        - `n_accessible_cells: int | None`: the number of accessible cells in the maze. If `None`, defaults to the total number of cells in the grid.', '            (default: `None`)', '        - `max_tree_depth: int | None`: the maximum depth of the tree. If `None`, defaults to `2 * n_accessible_cells`.', '            (default: `None`)', '        - `do_forks: bool`: whether to allow forks in the maze. If `False`, the maze will be have no forks and will be a simple hallway.', '        - `start_coord: Coord | None`: the starting coordinate of the generation algorithm. If `None`, defaults to a random coordinate.', '', '', '        # algorithm', '        1. Choose the initial cell, mark it as visited and push it to the stack', '        2. While the stack is not empty', '                1. Pop a cell from the stack and make it a current cell', '                2. If the current cell has any neighbours which have not been visited', '                        1. Push the current cell to the stack', '                        2. Choose one of the unvisited neighbours', '                        3. Remove the wall between the current cell and the chosen cell', '                        4. Mark the chosen cell as visited and push it to the stack', '        '], 'source_code': ['    @staticmethod', '    def gen_dfs(', '        grid_shape: Coord,', '        lattice_dim: int = 2,', '        n_accessible_cells: int | None = None,', '        max_tree_depth: int | None = None,', '        do_forks: bool = True,', '        start_coord: Coord | None = None,', '    ) -> LatticeMaze:', '        \"\"\"generate a lattice maze using depth first search, iterative', '', '        # Arguments', '        - `grid_shape: Coord`: the shape of the grid', '        - `lattice_dim: int`: the dimension of the lattice', '          (default: `2`)', '        - `n_accessible_cells: int | None`: the number of accessible cells in the maze. If `None`, defaults to the total number of cells in the grid.', '            (default: `None`)', '        - `max_tree_depth: int | None`: the maximum depth of the tree. If `None`, defaults to `2 * n_accessible_cells`.', '            (default: `None`)', '        - `do_forks: bool`: whether to allow forks in the maze. If `False`, the maze will be have no forks and will be a simple hallway.', '        - `start_coord: Coord | None`: the starting coordinate of the generation algorithm. If `None`, defaults to a random coordinate.', '', '', '        # algorithm', '        1. Choose the initial cell, mark it as visited and push it to the stack', '        2. While the stack is not empty', '                1. Pop a cell from the stack and make it a current cell', '                2. If the current cell has any neighbours which have not been visited', '                        1. Push the current cell to the stack', '                        2. Choose one of the unvisited neighbours', '                        3. Remove the wall between the current cell and the chosen cell', '                        4. Mark the chosen cell as visited and push it to the stack', '        \"\"\"', '', '        # Default values if no constraints have been passed', '        grid_shape: Coord = np.array(grid_shape)', '        n_total_cells: int = int(np.prod(grid_shape))', '        if n_accessible_cells is None:', '            n_accessible_cells = n_total_cells', '        if max_tree_depth is None:', '            max_tree_depth = (', '                2 * n_total_cells', '            )  # We define max tree depth counting from the start coord in two directions. Therefore we divide by two in the if clause for neighboring sites later and multiply by two here.', '', '        start_coord = _random_start_coord(grid_shape, start_coord)', '', '        # initialize the maze with no connections', '        connection_list: ConnectionList = np.zeros(', '            (lattice_dim, grid_shape[0], grid_shape[1]), dtype=np.bool_', '        )', '', '        # initialize the stack with the target coord', '        visited_cells: set[tuple[int, int]] = set()', '        visited_cells.add(tuple(start_coord))', '        stack: list[Coord] = [start_coord]', '', '        # initialize tree_depth_counter', '        current_tree_depth: int = 1', '', '        # loop until the stack is empty or n_connected_cells is reached', '        while stack and (len(visited_cells) < n_accessible_cells):', '            # get the current coord from the stack', '            current_coord: Coord = stack.pop()', '', '            # filter neighbors by being within grid bounds and being unvisited', '            unvisited_neighbors_deltas: list[tuple[Coord, Coord]] = [', '                (neighbor, delta)', '                for neighbor, delta in zip(', '                    current_coord + NEIGHBORS_MASK, NEIGHBORS_MASK', '                )', '                if (', '                    (tuple(neighbor) not in visited_cells)', '                    and (0 <= neighbor[0] < grid_shape[0])', '                    and (0 <= neighbor[1] < grid_shape[1])', '                )', '            ]', '', \"            # don't continue if max_tree_depth/2 is already reached (divide by 2 because we can branch to multiple directions)\", '            if unvisited_neighbors_deltas and (', '                current_tree_depth <= max_tree_depth / 2', '            ):', \"                # if we want a maze without forks, simply don't add the current coord back to the stack\", '                if do_forks:', '                    stack.append(current_coord)', '', '                # choose one of the unvisited neighbors', '                chosen_neighbor, delta = random.choice(unvisited_neighbors_deltas)', '', '                # add connection', '                dim: int = np.argmax(np.abs(delta))', '                # if positive, down/right from current coord', '                # if negative, up/left from current coord (down/right from neighbor)', '                clist_node: Coord = (', '                    current_coord if (delta.sum() > 0) else chosen_neighbor', '                )', '                connection_list[dim, clist_node[0], clist_node[1]] = True', '', '                # add to visited cells and stack', '                visited_cells.add(tuple(chosen_neighbor))', '                stack.append(chosen_neighbor)', '', '                # Update current tree depth', '                current_tree_depth += 1', '            else:', '                current_tree_depth -= 1', '', '        return LatticeMaze(', '            connection_list=connection_list,', '            generation_meta=dict(', '                func_name=\"gen_dfs\",', '                grid_shape=grid_shape,', '                start_coord=start_coord,', '                n_accessible_cells=int(n_accessible_cells),', '                max_tree_depth=int(max_tree_depth),', '                fully_connected=bool(len(visited_cells) == n_accessible_cells),', '                visited_cells={tuple(int(x) for x in coord) for coord in visited_cells},', '            ),', '        )']}, 'maze_ctor_kwargs': {'do_forks': False}, 'padding_token_index': 10, 'token_arr': ['<ADJLIST_START>', '<ADJLIST_END>', '<TARGET_START>', '<TARGET_END>', '<ORIGIN_START>', '<ORIGIN_END>', '<PATH_START>', '<PATH_END>', '<-->', ';', '<PADDING>', '(0,0)', '(0,1)', '(1,0)', '(1,1)', '(0,2)', '(2,0)', '(1,2)', '(2,1)', '(2,2)', '(0,3)', '(3,0)', '(3,1)', '(2,3)', '(3,2)', '(1,3)', '(3,3)', '(0,4)', '(2,4)', '(4,0)', '(1,4)', '(4,1)', '(4,2)', '(3,4)', '(4,3)', '(4,4)', '(0,5)', '(5,0)', '(5,1)', '(2,5)', '(5,2)', '(5,3)', '(4,5)', '(5,4)', '(1,5)', '(3,5)', '(5,5)', '(0,6)', '(2,6)', '(4,6)', '(6,0)', '(1,6)', '(6,1)', '(6,2)', '(3,6)', '(6,3)', '(6,4)', '(5,6)', '(6,5)', '(6,6)', '(0,7)', '(7,0)', '(7,1)', '(2,7)', '(7,2)', '(7,3)', '(4,7)', '(7,4)', '(7,5)', '(6,7)', '(7,6)', '(1,7)', '(3,7)', '(5,7)', '(7,7)'], 'tokenizer_map': {'<ADJLIST_START>': 0, '<ADJLIST_END>': 1, '<TARGET_START>': 2, '<TARGET_END>': 3, '<ORIGIN_START>': 4, '<ORIGIN_END>': 5, '<PATH_START>': 6, '<PATH_END>': 7, '<-->': 8, ';': 9, '<PADDING>': 10, '(0,0)': 11, '(0,1)': 12, '(1,0)': 13, '(1,1)': 14, '(0,2)': 15, '(2,0)': 16, '(1,2)': 17, '(2,1)': 18, '(2,2)': 19, '(0,3)': 20, '(3,0)': 21, '(3,1)': 22, '(2,3)': 23, '(3,2)': 24, '(1,3)': 25, '(3,3)': 26, '(0,4)': 27, '(2,4)': 28, '(4,0)': 29, '(1,4)': 30, '(4,1)': 31, '(4,2)': 32, '(3,4)': 33, '(4,3)': 34, '(4,4)': 35, '(0,5)': 36, '(5,0)': 37, '(5,1)': 38, '(2,5)': 39, '(5,2)': 40, '(5,3)': 41, '(4,5)': 42, '(5,4)': 43, '(1,5)': 44, '(3,5)': 45, '(5,5)': 46, '(0,6)': 47, '(2,6)': 48, '(4,6)': 49, '(6,0)': 50, '(1,6)': 51, '(6,1)': 52, '(6,2)': 53, '(3,6)': 54, '(6,3)': 55, '(6,4)': 56, '(5,6)': 57, '(6,5)': 58, '(6,6)': 59, '(0,7)': 60, '(7,0)': 61, '(7,1)': 62, '(2,7)': 63, '(7,2)': 64, '(7,3)': 65, '(4,7)': 66, '(7,4)': 67, '(7,5)': 68, '(6,7)': 69, '(7,6)': 70, '(1,7)': 71, '(3,7)': 72, '(5,7)': 73, '(7,7)': 74}, 'grid_shape': (8, 8), 'token_node_map': {'(0,0)': (0, 0), '(0,1)': (0, 1), '(1,0)': (1, 0), '(1,1)': (1, 1), '(0,2)': (0, 2), '(2,0)': (2, 0), '(1,2)': (1, 2), '(2,1)': (2, 1), '(2,2)': (2, 2), '(0,3)': (0, 3), '(3,0)': (3, 0), '(3,1)': (3, 1), '(2,3)': (2, 3), '(3,2)': (3, 2), '(1,3)': (1, 3), '(3,3)': (3, 3), '(0,4)': (0, 4), '(2,4)': (2, 4), '(4,0)': (4, 0), '(1,4)': (1, 4), '(4,1)': (4, 1), '(4,2)': (4, 2), '(3,4)': (3, 4), '(4,3)': (4, 3), '(4,4)': (4, 4), '(0,5)': (0, 5), '(5,0)': (5, 0), '(5,1)': (5, 1), '(2,5)': (2, 5), '(5,2)': (5, 2), '(5,3)': (5, 3), '(4,5)': (4, 5), '(5,4)': (5, 4), '(1,5)': (1, 5), '(3,5)': (3, 5), '(5,5)': (5, 5), '(0,6)': (0, 6), '(2,6)': (2, 6), '(4,6)': (4, 6), '(6,0)': (6, 0), '(1,6)': (1, 6), '(6,1)': (6, 1), '(6,2)': (6, 2), '(3,6)': (3, 6), '(6,3)': (6, 3), '(6,4)': (6, 4), '(5,6)': (5, 6), '(6,5)': (6, 5), '(6,6)': (6, 6), '(0,7)': (0, 7), '(7,0)': (7, 0), '(7,1)': (7, 1), '(2,7)': (2, 7), '(7,2)': (7, 2), '(7,3)': (7, 3), '(4,7)': (4, 7), '(7,4)': (7, 4), '(7,5)': (7, 5), '(6,7)': (6, 7), '(7,6)': (7, 6), '(1,7)': (1, 7), '(3,7)': (3, 7), '(5,7)': (5, 7), '(7,7)': (7, 7)}, 'n_tokens': 75}, 'model_cfg': {'__format__': 'BaseGPTConfig(SerializableDataclass)', 'name': 'custom-model', 'act_fn': 'gelu', 'd_model': 128, 'd_head': 32, 'n_layers': 6, 'weight_processing': {'are_layernorms_folded': False, 'are_weights_processed': False}, 'n_heads': 4}, 'train_cfg': {'__format__': 'TrainConfig(SerializableDataclass)', 'name': 'custom-train', 'evals_max_new_tokens': 8, 'validation_dataset_cfg': 10, 'optimizer': 'AdamW', 'optimizer_kwargs': {'lr': 1e-05}, 'batch_size': 32, 'dataloader_cfg': {'shuffle': True, 'num_workers': 4, 'drop_last': False}, 'intervals': None, 'intervals_count': {'print_loss': 1000, 'checkpoint': 20, 'eval_fast': 100, 'eval_slow': 50}}, 'name': 'hallway-medium', 'pretrainedtokenizer_kwargs': None}\n",
      "2023-06-15 03:51:29 INFO Initialized logger\n",
      "2023-06-15 03:51:29 INFO Summary logged, getting dataset\n",
      "loading dataset from ../data/custom-hallway-g8-n100K-a_dfs-h98637.zanj\n",
      "Got dataset custom-hallway with 100000 items. output.cfg.to_fname() = 'custom-hallway-g8-n100K-a_dfs-h98637'\n",
      "2023-06-15 03:51:51 INFO finished getting training dataset with 100000 samples\n",
      "2023-06-15 03:51:51 INFO got validation dataset by splitting training dataset into 99990 train and 10 validation samples\n",
      "2023-06-15 03:51:51 INFO Loaded 99990 sequences\n",
      "2023-06-15 03:51:51 INFO Creating dataloader\n",
      "2023-06-15 03:51:51 INFO finished dataloader, passing to train()\n",
      "2023-06-15 03:51:51 INFO Initializing model\n",
      "Moving model to device:  cuda\n",
      "2023-06-15 03:51:51 INFO Initializing optimizer\n",
      "2023-06-15 03:51:51 INFO will train for 3125 batches, evals_enabled=True, with intervals: {'print_loss': 3, 'checkpoint': 156, 'eval_fast': 31, 'eval_slow': 62}\n",
      "2023-06-15 03:51:51 INFO Starting training\n",
      "2023-06-15 03:52:13 INFO Running evals: eval_fast\n",
      "2023-06-15 03:52:14 INFO Running evals: eval_slow\n",
      "2023-06-15 03:52:16 INFO iteration 0/3125: loss=4.743\n",
      "2023-06-15 03:52:16 INFO Saving model checkpoint to ../data/hallway-medium_2023-06-15-03-51-25/checkpoints/model.iter_0.zanj\n",
      "2023-06-15 03:52:17 INFO iteration 3/3125: loss=4.556\n",
      "2023-06-15 03:52:17 INFO iteration 6/3125: loss=4.279\n",
      "2023-06-15 03:52:18 INFO iteration 9/3125: loss=4.037\n",
      "2023-06-15 03:52:18 INFO iteration 12/3125: loss=3.790\n",
      "2023-06-15 03:52:19 INFO iteration 15/3125: loss=3.520\n",
      "2023-06-15 03:52:19 INFO iteration 18/3125: loss=3.480\n",
      "2023-06-15 03:52:20 INFO iteration 21/3125: loss=3.313\n",
      "2023-06-15 03:52:20 INFO iteration 24/3125: loss=3.151\n",
      "2023-06-15 03:52:21 INFO iteration 27/3125: loss=3.021\n",
      "2023-06-15 03:52:21 INFO iteration 30/3125: loss=3.026\n",
      "2023-06-15 03:52:22 INFO Running evals: eval_fast\n",
      "2023-06-15 03:52:23 INFO iteration 33/3125: loss=2.911\n",
      "2023-06-15 03:52:24 INFO iteration 36/3125: loss=2.880\n",
      "2023-06-15 03:52:24 INFO iteration 39/3125: loss=2.949\n",
      "2023-06-15 03:52:25 INFO iteration 42/3125: loss=2.650\n",
      "2023-06-15 03:52:25 INFO iteration 45/3125: loss=2.498\n",
      "2023-06-15 03:52:26 INFO iteration 48/3125: loss=2.759\n",
      "2023-06-15 03:52:27 INFO iteration 51/3125: loss=2.432\n",
      "2023-06-15 03:52:27 INFO iteration 54/3125: loss=2.341\n",
      "2023-06-15 03:52:28 INFO iteration 57/3125: loss=2.490\n",
      "2023-06-15 03:52:28 INFO iteration 60/3125: loss=2.430\n",
      "2023-06-15 03:52:29 INFO Running evals: eval_fast\n",
      "2023-06-15 03:52:30 INFO Running evals: eval_slow\n",
      "2023-06-15 03:52:31 INFO iteration 63/3125: loss=2.329\n",
      "2023-06-15 03:52:32 INFO iteration 66/3125: loss=2.461\n",
      "2023-06-15 03:52:33 INFO iteration 69/3125: loss=2.454\n",
      "2023-06-15 03:52:33 INFO iteration 72/3125: loss=2.720\n",
      "2023-06-15 03:52:34 INFO iteration 75/3125: loss=2.486\n",
      "2023-06-15 03:52:34 INFO iteration 78/3125: loss=2.152\n",
      "2023-06-15 03:52:35 INFO iteration 81/3125: loss=2.384\n",
      "2023-06-15 03:52:35 INFO iteration 84/3125: loss=2.371\n",
      "2023-06-15 03:52:36 INFO iteration 87/3125: loss=2.233\n",
      "2023-06-15 03:52:36 INFO iteration 90/3125: loss=2.400\n",
      "2023-06-15 03:52:37 INFO Running evals: eval_fast\n",
      "2023-06-15 03:52:38 INFO iteration 93/3125: loss=2.130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\KNC\\maze-transformer\\maze_transformer\\evaluation\\path_evals.py:91: RuntimeWarning:\n",
      "\n",
      "fraction_connections_adjacent_lattice called on path of length less than 2, retuning NaN\n",
      "prediction = array([[3, 1]])\n",
      "\n",
      "F:\\KNC\\maze-transformer\\maze_transformer\\evaluation\\path_evals.py:91: RuntimeWarning:\n",
      "\n",
      "fraction_connections_adjacent_lattice called on path of length less than 2, retuning NaN\n",
      "prediction = array([[5, 6]])\n",
      "\n",
      "F:\\KNC\\maze-transformer\\maze_transformer\\evaluation\\path_evals.py:91: RuntimeWarning:\n",
      "\n",
      "fraction_connections_adjacent_lattice called on path of length less than 2, retuning NaN\n",
      "prediction = array([[7, 0]])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-15 03:52:39 INFO iteration 96/3125: loss=2.114\n",
      "2023-06-15 03:52:39 INFO iteration 99/3125: loss=2.200\n",
      "2023-06-15 03:52:40 INFO iteration 102/3125: loss=2.350\n",
      "2023-06-15 03:52:40 INFO iteration 105/3125: loss=2.254\n",
      "2023-06-15 03:52:41 INFO iteration 108/3125: loss=2.067\n",
      "2023-06-15 03:52:41 INFO iteration 111/3125: loss=2.306\n",
      "2023-06-15 03:52:42 INFO iteration 114/3125: loss=1.934\n",
      "2023-06-15 03:52:43 INFO iteration 117/3125: loss=2.222\n",
      "2023-06-15 03:52:43 INFO iteration 120/3125: loss=2.202\n",
      "2023-06-15 03:52:44 INFO iteration 123/3125: loss=2.323\n",
      "2023-06-15 03:52:44 INFO Running evals: eval_fast\n",
      "2023-06-15 03:52:45 INFO Running evals: eval_slow\n",
      "2023-06-15 03:52:47 INFO iteration 126/3125: loss=2.056\n",
      "2023-06-15 03:52:48 INFO iteration 129/3125: loss=2.239\n",
      "2023-06-15 03:52:48 INFO iteration 132/3125: loss=2.183\n",
      "2023-06-15 03:52:49 INFO iteration 135/3125: loss=2.259\n",
      "2023-06-15 03:52:49 INFO iteration 138/3125: loss=1.942\n",
      "2023-06-15 03:52:50 INFO iteration 141/3125: loss=2.238\n",
      "2023-06-15 03:52:50 INFO iteration 144/3125: loss=2.016\n",
      "2023-06-15 03:52:51 INFO iteration 147/3125: loss=2.195\n",
      "2023-06-15 03:52:51 INFO iteration 150/3125: loss=2.182\n",
      "2023-06-15 03:52:52 INFO iteration 153/3125: loss=1.725\n",
      "2023-06-15 03:52:52 INFO Running evals: eval_fast\n",
      "2023-06-15 03:52:54 INFO iteration 156/3125: loss=1.860\n",
      "2023-06-15 03:52:54 INFO Saving model checkpoint to ../data/hallway-medium_2023-06-15-03-51-25/checkpoints/model.iter_156.zanj\n",
      "2023-06-15 03:52:55 INFO iteration 159/3125: loss=2.048\n",
      "2023-06-15 03:52:55 INFO iteration 162/3125: loss=1.721\n",
      "2023-06-15 03:52:56 INFO iteration 165/3125: loss=1.756\n",
      "2023-06-15 03:52:56 INFO iteration 168/3125: loss=1.680\n",
      "2023-06-15 03:52:57 INFO iteration 171/3125: loss=1.708\n",
      "2023-06-15 03:52:57 INFO iteration 174/3125: loss=1.925\n",
      "2023-06-15 03:52:58 INFO iteration 177/3125: loss=1.914\n",
      "2023-06-15 03:52:58 INFO iteration 180/3125: loss=1.701\n",
      "2023-06-15 03:52:59 INFO iteration 183/3125: loss=2.038\n",
      "2023-06-15 03:53:00 INFO Running evals: eval_fast\n",
      "2023-06-15 03:53:01 INFO Running evals: eval_slow\n",
      "2023-06-15 03:53:03 INFO iteration 186/3125: loss=2.046\n",
      "2023-06-15 03:53:03 INFO iteration 189/3125: loss=1.989\n",
      "2023-06-15 03:53:04 INFO iteration 192/3125: loss=1.750\n",
      "2023-06-15 03:53:04 INFO iteration 195/3125: loss=1.744\n",
      "2023-06-15 03:53:05 INFO iteration 198/3125: loss=2.116\n",
      "2023-06-15 03:53:05 INFO iteration 201/3125: loss=2.196\n",
      "2023-06-15 03:53:06 INFO iteration 204/3125: loss=1.876\n",
      "2023-06-15 03:53:07 INFO iteration 207/3125: loss=1.772\n",
      "2023-06-15 03:53:07 INFO iteration 210/3125: loss=1.727\n",
      "2023-06-15 03:53:08 INFO iteration 213/3125: loss=1.938\n",
      "2023-06-15 03:53:08 INFO iteration 216/3125: loss=1.641\n",
      "2023-06-15 03:53:08 INFO Running evals: eval_fast\n",
      "2023-06-15 03:53:11 INFO iteration 219/3125: loss=1.989\n",
      "2023-06-15 03:53:12 INFO iteration 222/3125: loss=1.952\n",
      "2023-06-15 03:53:12 INFO iteration 225/3125: loss=1.594\n",
      "2023-06-15 03:53:13 INFO iteration 228/3125: loss=1.939\n",
      "2023-06-15 03:53:14 INFO iteration 231/3125: loss=2.122\n",
      "2023-06-15 03:53:14 INFO iteration 234/3125: loss=1.745\n",
      "2023-06-15 03:53:15 INFO iteration 237/3125: loss=2.014\n",
      "2023-06-15 03:53:15 INFO iteration 240/3125: loss=1.924\n",
      "2023-06-15 03:53:16 INFO iteration 243/3125: loss=1.825\n",
      "2023-06-15 03:53:16 INFO iteration 246/3125: loss=1.651\n",
      "2023-06-15 03:53:17 INFO Running evals: eval_fast\n",
      "2023-06-15 03:53:18 INFO Running evals: eval_slow\n",
      "2023-06-15 03:53:20 INFO iteration 249/3125: loss=1.906\n",
      "2023-06-15 03:53:20 INFO iteration 252/3125: loss=2.093\n",
      "2023-06-15 03:53:21 INFO iteration 255/3125: loss=1.695\n",
      "2023-06-15 03:53:21 INFO iteration 258/3125: loss=1.928\n",
      "2023-06-15 03:53:22 INFO iteration 261/3125: loss=1.628\n",
      "2023-06-15 03:53:23 INFO iteration 264/3125: loss=1.973\n",
      "2023-06-15 03:53:23 INFO iteration 267/3125: loss=1.766\n",
      "2023-06-15 03:53:24 INFO iteration 270/3125: loss=1.860\n",
      "2023-06-15 03:53:24 INFO iteration 273/3125: loss=1.492\n",
      "2023-06-15 03:53:25 INFO iteration 276/3125: loss=1.849\n",
      "2023-06-15 03:53:25 INFO Running evals: eval_fast\n",
      "2023-06-15 03:53:27 INFO iteration 279/3125: loss=1.763\n",
      "2023-06-15 03:53:27 INFO iteration 282/3125: loss=1.831\n",
      "2023-06-15 03:53:28 INFO iteration 285/3125: loss=1.689\n",
      "2023-06-15 03:53:28 INFO iteration 288/3125: loss=1.618\n",
      "2023-06-15 03:53:29 INFO iteration 291/3125: loss=1.720\n",
      "2023-06-15 03:53:29 INFO iteration 294/3125: loss=1.894\n",
      "2023-06-15 03:53:30 INFO iteration 297/3125: loss=1.888\n",
      "2023-06-15 03:53:31 INFO iteration 300/3125: loss=1.682\n",
      "2023-06-15 03:53:31 INFO iteration 303/3125: loss=1.835\n",
      "2023-06-15 03:53:32 INFO iteration 306/3125: loss=1.721\n",
      "2023-06-15 03:53:32 INFO iteration 309/3125: loss=1.524\n",
      "2023-06-15 03:53:33 INFO Running evals: eval_fast\n",
      "2023-06-15 03:53:34 INFO Running evals: eval_slow\n",
      "2023-06-15 03:53:36 INFO iteration 312/3125: loss=1.765\n",
      "2023-06-15 03:53:36 INFO Saving model checkpoint to ../data/hallway-medium_2023-06-15-03-51-25/checkpoints/model.iter_312.zanj\n",
      "2023-06-15 03:53:37 INFO iteration 315/3125: loss=1.600\n",
      "2023-06-15 03:53:37 INFO iteration 318/3125: loss=2.045\n",
      "2023-06-15 03:53:38 INFO iteration 321/3125: loss=1.952\n",
      "2023-06-15 03:53:38 INFO iteration 324/3125: loss=1.976\n",
      "2023-06-15 03:53:39 INFO iteration 327/3125: loss=1.708\n",
      "2023-06-15 03:53:40 INFO iteration 330/3125: loss=1.958\n",
      "2023-06-15 03:53:40 INFO iteration 333/3125: loss=1.760\n",
      "2023-06-15 03:53:41 INFO iteration 336/3125: loss=1.425\n",
      "2023-06-15 03:53:41 INFO iteration 339/3125: loss=1.707\n",
      "2023-06-15 03:53:42 INFO Running evals: eval_fast\n",
      "2023-06-15 03:53:44 INFO iteration 342/3125: loss=1.724\n",
      "2023-06-15 03:53:44 INFO iteration 345/3125: loss=1.576\n",
      "2023-06-15 03:53:45 INFO iteration 348/3125: loss=1.719\n",
      "2023-06-15 03:53:46 INFO iteration 351/3125: loss=1.849\n",
      "2023-06-15 03:53:46 INFO iteration 354/3125: loss=1.607\n",
      "2023-06-15 03:53:47 INFO iteration 357/3125: loss=1.519\n",
      "2023-06-15 03:53:47 INFO iteration 360/3125: loss=1.883\n",
      "2023-06-15 03:53:48 INFO iteration 363/3125: loss=1.622\n",
      "2023-06-15 03:53:48 INFO iteration 366/3125: loss=1.932\n",
      "2023-06-15 03:53:49 INFO iteration 369/3125: loss=2.076\n",
      "2023-06-15 03:53:50 INFO Running evals: eval_fast\n",
      "2023-06-15 03:53:51 INFO Running evals: eval_slow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\KNC\\maze-transformer\\maze_transformer\\evaluation\\path_evals.py:91: RuntimeWarning:\n",
      "\n",
      "fraction_connections_adjacent_lattice called on path of length less than 2, retuning NaN\n",
      "prediction = array([[2, 3]])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-15 03:53:53 INFO iteration 372/3125: loss=1.362\n",
      "2023-06-15 03:53:54 INFO iteration 375/3125: loss=1.427\n",
      "2023-06-15 03:53:54 INFO iteration 378/3125: loss=1.687\n",
      "2023-06-15 03:53:55 INFO iteration 381/3125: loss=1.659\n",
      "2023-06-15 03:53:55 INFO iteration 384/3125: loss=1.505\n",
      "2023-06-15 03:53:56 INFO iteration 387/3125: loss=1.603\n",
      "2023-06-15 03:53:57 INFO iteration 390/3125: loss=1.798\n",
      "2023-06-15 03:53:57 INFO iteration 393/3125: loss=1.650\n",
      "2023-06-15 03:53:58 INFO iteration 396/3125: loss=1.591\n",
      "2023-06-15 03:53:58 INFO iteration 399/3125: loss=1.922\n",
      "2023-06-15 03:53:59 INFO iteration 402/3125: loss=1.772\n",
      "2023-06-15 03:53:59 INFO Running evals: eval_fast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\KNC\\maze-transformer\\maze_transformer\\evaluation\\path_evals.py:91: RuntimeWarning:\n",
      "\n",
      "fraction_connections_adjacent_lattice called on path of length less than 2, retuning NaN\n",
      "prediction = array([[1, 7]])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-15 03:54:01 INFO iteration 405/3125: loss=1.671\n",
      "2023-06-15 03:54:01 INFO iteration 408/3125: loss=1.619\n",
      "2023-06-15 03:54:02 INFO iteration 411/3125: loss=1.607\n",
      "2023-06-15 03:54:03 INFO iteration 414/3125: loss=1.910\n",
      "2023-06-15 03:54:03 INFO iteration 417/3125: loss=1.596\n",
      "2023-06-15 03:54:04 INFO iteration 420/3125: loss=1.796\n",
      "2023-06-15 03:54:04 INFO iteration 423/3125: loss=1.733\n",
      "2023-06-15 03:54:05 INFO iteration 426/3125: loss=1.561\n",
      "2023-06-15 03:54:05 INFO iteration 429/3125: loss=1.784\n",
      "2023-06-15 03:54:06 INFO iteration 432/3125: loss=1.731\n",
      "2023-06-15 03:54:06 INFO Running evals: eval_fast\n",
      "2023-06-15 03:54:08 INFO Running evals: eval_slow\n",
      "2023-06-15 03:54:10 INFO iteration 435/3125: loss=1.788\n",
      "2023-06-15 03:54:10 INFO iteration 438/3125: loss=1.669\n",
      "2023-06-15 03:54:11 INFO iteration 441/3125: loss=1.593\n",
      "2023-06-15 03:54:11 INFO iteration 444/3125: loss=1.630\n",
      "2023-06-15 03:54:12 INFO iteration 447/3125: loss=1.704\n",
      "2023-06-15 03:54:13 INFO iteration 450/3125: loss=1.863\n",
      "2023-06-15 03:54:13 INFO iteration 453/3125: loss=1.699\n",
      "2023-06-15 03:54:14 INFO iteration 456/3125: loss=1.903\n",
      "2023-06-15 03:54:14 INFO iteration 459/3125: loss=1.671\n",
      "2023-06-15 03:54:15 INFO iteration 462/3125: loss=1.496\n",
      "2023-06-15 03:54:15 INFO Running evals: eval_fast\n",
      "2023-06-15 03:54:17 INFO iteration 465/3125: loss=1.612\n",
      "2023-06-15 03:54:17 INFO iteration 468/3125: loss=1.949\n",
      "2023-06-15 03:54:17 INFO Saving model checkpoint to ../data/hallway-medium_2023-06-15-03-51-25/checkpoints/model.iter_468.zanj\n",
      "2023-06-15 03:54:19 INFO iteration 471/3125: loss=1.572\n",
      "2023-06-15 03:54:19 INFO iteration 474/3125: loss=1.612\n",
      "2023-06-15 03:54:20 INFO iteration 477/3125: loss=1.662\n",
      "2023-06-15 03:54:20 INFO iteration 480/3125: loss=1.655\n",
      "2023-06-15 03:54:21 INFO iteration 483/3125: loss=1.613\n",
      "2023-06-15 03:54:22 INFO iteration 486/3125: loss=1.914\n",
      "2023-06-15 03:54:22 INFO iteration 489/3125: loss=1.634\n",
      "2023-06-15 03:54:23 INFO iteration 492/3125: loss=1.703\n",
      "2023-06-15 03:54:23 INFO iteration 495/3125: loss=1.721\n",
      "2023-06-15 03:54:23 INFO Running evals: eval_fast\n",
      "2023-06-15 03:54:25 INFO Running evals: eval_slow\n",
      "2023-06-15 03:54:27 INFO iteration 498/3125: loss=1.650\n",
      "2023-06-15 03:54:27 INFO iteration 501/3125: loss=1.530\n",
      "2023-06-15 03:54:28 INFO iteration 504/3125: loss=1.588\n",
      "2023-06-15 03:54:28 INFO iteration 507/3125: loss=1.514\n",
      "2023-06-15 03:54:29 INFO iteration 510/3125: loss=1.517\n",
      "2023-06-15 03:54:30 INFO iteration 513/3125: loss=1.743\n",
      "2023-06-15 03:54:30 INFO iteration 516/3125: loss=1.701\n",
      "2023-06-15 03:54:31 INFO iteration 519/3125: loss=1.707\n",
      "2023-06-15 03:54:31 INFO iteration 522/3125: loss=1.764\n",
      "2023-06-15 03:54:32 INFO iteration 525/3125: loss=1.525\n",
      "2023-06-15 03:54:32 INFO Running evals: eval_fast\n",
      "2023-06-15 03:54:34 INFO iteration 528/3125: loss=1.755\n",
      "2023-06-15 03:54:35 INFO iteration 531/3125: loss=1.427\n",
      "2023-06-15 03:54:35 INFO iteration 534/3125: loss=1.634\n",
      "2023-06-15 03:54:36 INFO iteration 537/3125: loss=1.441\n",
      "2023-06-15 03:54:36 INFO iteration 540/3125: loss=1.634\n",
      "2023-06-15 03:54:37 INFO iteration 543/3125: loss=1.657\n",
      "2023-06-15 03:54:38 INFO iteration 546/3125: loss=1.289\n",
      "2023-06-15 03:54:38 INFO iteration 549/3125: loss=1.568\n",
      "2023-06-15 03:54:39 INFO iteration 552/3125: loss=1.605\n",
      "2023-06-15 03:54:40 INFO iteration 555/3125: loss=1.504\n",
      "2023-06-15 03:54:40 INFO Running evals: eval_fast\n",
      "2023-06-15 03:54:42 INFO Running evals: eval_slow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\KNC\\maze-transformer\\maze_transformer\\evaluation\\path_evals.py:91: RuntimeWarning:\n",
      "\n",
      "fraction_connections_adjacent_lattice called on path of length less than 2, retuning NaN\n",
      "prediction = array([[2, 6]])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-15 03:54:43 INFO iteration 558/3125: loss=1.488\n",
      "2023-06-15 03:54:44 INFO iteration 561/3125: loss=1.411\n",
      "2023-06-15 03:54:44 INFO iteration 564/3125: loss=1.527\n",
      "2023-06-15 03:54:45 INFO iteration 567/3125: loss=1.703\n",
      "2023-06-15 03:54:46 INFO iteration 570/3125: loss=1.965\n",
      "2023-06-15 03:54:46 INFO iteration 573/3125: loss=1.832\n",
      "2023-06-15 03:54:47 INFO iteration 576/3125: loss=1.567\n",
      "2023-06-15 03:54:47 INFO iteration 579/3125: loss=1.714\n",
      "2023-06-15 03:54:48 INFO iteration 582/3125: loss=1.573\n",
      "2023-06-15 03:54:48 INFO iteration 585/3125: loss=1.546\n",
      "2023-06-15 03:54:49 INFO iteration 588/3125: loss=1.604\n",
      "2023-06-15 03:54:49 INFO Running evals: eval_fast\n",
      "2023-06-15 03:54:51 INFO iteration 591/3125: loss=1.703\n",
      "2023-06-15 03:54:52 INFO iteration 594/3125: loss=1.419\n",
      "2023-06-15 03:54:52 INFO iteration 597/3125: loss=1.491\n",
      "2023-06-15 03:54:53 INFO iteration 600/3125: loss=1.616\n",
      "2023-06-15 03:54:54 INFO iteration 603/3125: loss=1.414\n",
      "2023-06-15 03:54:54 INFO iteration 606/3125: loss=1.552\n",
      "2023-06-15 03:54:55 INFO iteration 609/3125: loss=1.615\n",
      "2023-06-15 03:54:55 INFO iteration 612/3125: loss=1.526\n",
      "2023-06-15 03:54:56 INFO iteration 615/3125: loss=1.729\n",
      "2023-06-15 03:54:56 INFO iteration 618/3125: loss=1.648\n",
      "2023-06-15 03:54:57 INFO Running evals: eval_fast\n",
      "2023-06-15 03:54:58 INFO Running evals: eval_slow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\KNC\\maze-transformer\\maze_transformer\\evaluation\\path_evals.py:91: RuntimeWarning:\n",
      "\n",
      "fraction_connections_adjacent_lattice called on path of length less than 2, retuning NaN\n",
      "prediction = array([[4, 7]])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-15 03:55:00 INFO iteration 621/3125: loss=1.401\n",
      "2023-06-15 03:55:01 INFO iteration 624/3125: loss=1.865\n",
      "2023-06-15 03:55:01 INFO Saving model checkpoint to ../data/hallway-medium_2023-06-15-03-51-25/checkpoints/model.iter_624.zanj\n",
      "2023-06-15 03:55:02 INFO iteration 627/3125: loss=1.515\n",
      "2023-06-15 03:55:03 INFO iteration 630/3125: loss=1.533\n",
      "2023-06-15 03:55:03 INFO iteration 633/3125: loss=1.736\n",
      "2023-06-15 03:55:04 INFO iteration 636/3125: loss=1.680\n",
      "2023-06-15 03:55:04 INFO iteration 639/3125: loss=1.473\n",
      "2023-06-15 03:55:05 INFO iteration 642/3125: loss=1.554\n",
      "2023-06-15 03:55:05 INFO iteration 645/3125: loss=1.734\n",
      "2023-06-15 03:55:06 INFO iteration 648/3125: loss=1.515\n",
      "2023-06-15 03:55:07 INFO Running evals: eval_fast\n",
      "2023-06-15 03:55:08 INFO iteration 651/3125: loss=1.416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\KNC\\maze-transformer\\maze_transformer\\evaluation\\path_evals.py:91: RuntimeWarning:\n",
      "\n",
      "fraction_connections_adjacent_lattice called on path of length less than 2, retuning NaN\n",
      "prediction = array([[5, 5]])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-15 03:55:09 INFO iteration 654/3125: loss=1.815\n",
      "2023-06-15 03:55:09 INFO iteration 657/3125: loss=1.551\n",
      "2023-06-15 03:55:10 INFO iteration 660/3125: loss=1.556\n",
      "2023-06-15 03:55:10 INFO iteration 663/3125: loss=1.453\n",
      "2023-06-15 03:55:11 INFO iteration 666/3125: loss=1.482\n",
      "2023-06-15 03:55:12 INFO iteration 669/3125: loss=1.584\n",
      "2023-06-15 03:55:12 INFO iteration 672/3125: loss=1.512\n",
      "2023-06-15 03:55:13 INFO iteration 675/3125: loss=1.475\n",
      "2023-06-15 03:55:13 INFO iteration 678/3125: loss=1.744\n",
      "2023-06-15 03:55:14 INFO iteration 681/3125: loss=1.845\n",
      "2023-06-15 03:55:14 INFO Running evals: eval_fast\n",
      "2023-06-15 03:55:15 INFO Running evals: eval_slow\n",
      "2023-06-15 03:55:17 INFO iteration 684/3125: loss=1.744\n",
      "2023-06-15 03:55:18 INFO iteration 687/3125: loss=1.411\n",
      "2023-06-15 03:55:18 INFO iteration 690/3125: loss=1.736\n",
      "2023-06-15 03:55:19 INFO iteration 693/3125: loss=1.321\n",
      "2023-06-15 03:55:19 INFO iteration 696/3125: loss=1.649\n",
      "2023-06-15 03:55:20 INFO iteration 699/3125: loss=1.510\n",
      "2023-06-15 03:55:21 INFO iteration 702/3125: loss=1.428\n",
      "2023-06-15 03:55:21 INFO iteration 705/3125: loss=1.481\n",
      "2023-06-15 03:55:22 INFO iteration 708/3125: loss=1.411\n",
      "2023-06-15 03:55:22 INFO iteration 711/3125: loss=1.435\n",
      "2023-06-15 03:55:23 INFO Running evals: eval_fast\n",
      "2023-06-15 03:55:24 INFO iteration 714/3125: loss=1.555\n",
      "2023-06-15 03:55:25 INFO iteration 717/3125: loss=1.802\n",
      "2023-06-15 03:55:26 INFO iteration 720/3125: loss=1.545\n",
      "2023-06-15 03:55:26 INFO iteration 723/3125: loss=1.531\n",
      "2023-06-15 03:55:27 INFO iteration 726/3125: loss=1.731\n",
      "2023-06-15 03:55:27 INFO iteration 729/3125: loss=1.873\n",
      "2023-06-15 03:55:28 INFO iteration 732/3125: loss=1.523\n",
      "2023-06-15 03:55:28 INFO iteration 735/3125: loss=1.476\n",
      "2023-06-15 03:55:29 INFO iteration 738/3125: loss=1.592\n",
      "2023-06-15 03:55:30 INFO iteration 741/3125: loss=1.179\n",
      "2023-06-15 03:55:30 INFO Running evals: eval_fast\n",
      "2023-06-15 03:55:32 INFO Running evals: eval_slow\n",
      "2023-06-15 03:55:33 INFO iteration 744/3125: loss=1.580\n",
      "2023-06-15 03:55:34 INFO iteration 747/3125: loss=1.518\n",
      "2023-06-15 03:55:34 INFO iteration 750/3125: loss=1.538\n",
      "2023-06-15 03:55:35 INFO iteration 753/3125: loss=1.457\n",
      "2023-06-15 03:55:35 INFO iteration 756/3125: loss=1.691\n",
      "2023-06-15 03:55:36 INFO iteration 759/3125: loss=1.468\n",
      "2023-06-15 03:55:37 INFO iteration 762/3125: loss=1.547\n",
      "2023-06-15 03:55:37 INFO iteration 765/3125: loss=1.625\n",
      "2023-06-15 03:55:38 INFO iteration 768/3125: loss=1.535\n",
      "2023-06-15 03:55:38 INFO iteration 771/3125: loss=1.421\n",
      "2023-06-15 03:55:39 INFO iteration 774/3125: loss=1.700\n",
      "2023-06-15 03:55:39 INFO Running evals: eval_fast\n",
      "2023-06-15 03:55:41 INFO iteration 777/3125: loss=1.511\n",
      "2023-06-15 03:55:41 INFO iteration 780/3125: loss=1.470\n",
      "2023-06-15 03:55:41 INFO Saving model checkpoint to ../data/hallway-medium_2023-06-15-03-51-25/checkpoints/model.iter_780.zanj\n",
      "2023-06-15 03:55:43 INFO iteration 783/3125: loss=1.626\n",
      "2023-06-15 03:55:43 INFO iteration 786/3125: loss=1.739\n",
      "2023-06-15 03:55:44 INFO iteration 789/3125: loss=1.840\n",
      "2023-06-15 03:55:44 INFO iteration 792/3125: loss=1.595\n",
      "2023-06-15 03:55:45 INFO iteration 795/3125: loss=1.627\n",
      "2023-06-15 03:55:45 INFO iteration 798/3125: loss=1.579\n",
      "2023-06-15 03:55:46 INFO iteration 801/3125: loss=1.612\n",
      "2023-06-15 03:55:47 INFO iteration 804/3125: loss=1.442\n",
      "2023-06-15 03:55:47 INFO Running evals: eval_fast\n",
      "2023-06-15 03:55:48 INFO Running evals: eval_slow\n",
      "2023-06-15 03:55:50 INFO iteration 807/3125: loss=1.617\n",
      "2023-06-15 03:55:50 INFO iteration 810/3125: loss=1.480\n",
      "2023-06-15 03:55:51 INFO iteration 813/3125: loss=1.288\n",
      "2023-06-15 03:55:51 INFO iteration 816/3125: loss=1.353\n",
      "2023-06-15 03:55:52 INFO iteration 819/3125: loss=1.452\n",
      "2023-06-15 03:55:53 INFO iteration 822/3125: loss=1.651\n",
      "2023-06-15 03:55:53 INFO iteration 825/3125: loss=1.220\n",
      "2023-06-15 03:55:54 INFO iteration 828/3125: loss=1.200\n",
      "2023-06-15 03:55:54 INFO iteration 831/3125: loss=1.543\n",
      "2023-06-15 03:55:55 INFO iteration 834/3125: loss=1.538\n",
      "2023-06-15 03:55:56 INFO Running evals: eval_fast\n",
      "2023-06-15 03:55:57 INFO iteration 837/3125: loss=1.471\n",
      "2023-06-15 03:55:58 INFO iteration 840/3125: loss=1.409\n",
      "2023-06-15 03:55:58 INFO iteration 843/3125: loss=1.723\n",
      "2023-06-15 03:55:59 INFO iteration 846/3125: loss=1.463\n",
      "2023-06-15 03:55:59 INFO iteration 849/3125: loss=1.551\n",
      "2023-06-15 03:56:00 INFO iteration 852/3125: loss=1.403\n",
      "2023-06-15 03:56:00 INFO iteration 855/3125: loss=1.698\n",
      "2023-06-15 03:56:01 INFO iteration 858/3125: loss=1.607\n",
      "2023-06-15 03:56:01 INFO iteration 861/3125: loss=1.541\n",
      "2023-06-15 03:56:02 INFO iteration 864/3125: loss=1.417\n",
      "2023-06-15 03:56:03 INFO iteration 867/3125: loss=1.507\n",
      "2023-06-15 03:56:03 INFO Running evals: eval_fast\n",
      "2023-06-15 03:56:04 INFO Running evals: eval_slow\n",
      "2023-06-15 03:56:06 INFO iteration 870/3125: loss=1.910\n",
      "2023-06-15 03:56:07 INFO iteration 873/3125: loss=1.416\n",
      "2023-06-15 03:56:07 INFO iteration 876/3125: loss=1.487\n",
      "2023-06-15 03:56:08 INFO iteration 879/3125: loss=1.661\n",
      "2023-06-15 03:56:08 INFO iteration 882/3125: loss=1.846\n",
      "2023-06-15 03:56:09 INFO iteration 885/3125: loss=1.716\n",
      "2023-06-15 03:56:09 INFO iteration 888/3125: loss=1.418\n",
      "2023-06-15 03:56:10 INFO iteration 891/3125: loss=1.645\n",
      "2023-06-15 03:56:11 INFO iteration 894/3125: loss=1.506\n",
      "2023-06-15 03:56:11 INFO iteration 897/3125: loss=1.664\n",
      "2023-06-15 03:56:12 INFO Running evals: eval_fast\n",
      "2023-06-15 03:56:13 INFO iteration 900/3125: loss=1.422\n",
      "2023-06-15 03:56:14 INFO iteration 903/3125: loss=1.356\n",
      "2023-06-15 03:56:14 INFO iteration 906/3125: loss=1.265\n",
      "2023-06-15 03:56:15 INFO iteration 909/3125: loss=1.473\n",
      "2023-06-15 03:56:16 INFO iteration 912/3125: loss=1.557\n",
      "2023-06-15 03:56:16 INFO iteration 915/3125: loss=1.536\n",
      "2023-06-15 03:56:17 INFO iteration 918/3125: loss=1.657\n",
      "2023-06-15 03:56:17 INFO iteration 921/3125: loss=1.476\n",
      "2023-06-15 03:56:18 INFO iteration 924/3125: loss=1.666\n",
      "2023-06-15 03:56:19 INFO iteration 927/3125: loss=1.539\n",
      "2023-06-15 03:56:19 INFO Running evals: eval_fast\n",
      "2023-06-15 03:56:21 INFO Running evals: eval_slow\n",
      "2023-06-15 03:56:24 INFO iteration 930/3125: loss=1.675\n",
      "2023-06-15 03:56:25 INFO iteration 933/3125: loss=1.431\n",
      "2023-06-15 03:56:26 INFO iteration 936/3125: loss=1.548\n",
      "2023-06-15 03:56:26 INFO Saving model checkpoint to ../data/hallway-medium_2023-06-15-03-51-25/checkpoints/model.iter_936.zanj\n",
      "2023-06-15 03:56:28 INFO iteration 939/3125: loss=1.543\n",
      "2023-06-15 03:56:30 INFO iteration 942/3125: loss=1.432\n",
      "2023-06-15 03:56:31 INFO iteration 945/3125: loss=1.727\n",
      "2023-06-15 03:56:32 INFO iteration 948/3125: loss=1.562\n",
      "2023-06-15 03:56:34 INFO iteration 951/3125: loss=1.506\n",
      "2023-06-15 03:56:35 INFO iteration 954/3125: loss=1.668\n",
      "2023-06-15 03:56:36 INFO iteration 957/3125: loss=1.291\n",
      "2023-06-15 03:56:38 INFO iteration 960/3125: loss=1.428\n",
      "2023-06-15 03:56:38 INFO Running evals: eval_fast\n",
      "2023-06-15 03:56:45 INFO iteration 963/3125: loss=1.350\n",
      "2023-06-15 03:56:46 INFO iteration 966/3125: loss=1.559\n",
      "2023-06-15 03:56:48 INFO iteration 969/3125: loss=1.826\n",
      "2023-06-15 03:56:49 INFO iteration 972/3125: loss=1.518\n",
      "2023-06-15 03:56:50 INFO iteration 975/3125: loss=1.496\n",
      "2023-06-15 03:56:52 INFO iteration 978/3125: loss=1.611\n",
      "2023-06-15 03:56:53 INFO iteration 981/3125: loss=1.493\n",
      "2023-06-15 03:56:55 INFO iteration 984/3125: loss=1.309\n",
      "2023-06-15 03:56:56 INFO iteration 987/3125: loss=1.742\n",
      "2023-06-15 03:56:57 INFO iteration 990/3125: loss=1.621\n",
      "2023-06-15 03:56:58 INFO Running evals: eval_fast\n",
      "2023-06-15 03:57:04 INFO Running evals: eval_slow\n",
      "2023-06-15 03:57:10 INFO iteration 993/3125: loss=1.284\n",
      "2023-06-15 03:57:12 INFO iteration 996/3125: loss=1.466\n",
      "2023-06-15 03:57:13 INFO iteration 999/3125: loss=1.361\n",
      "2023-06-15 03:57:14 INFO iteration 1002/3125: loss=1.721\n",
      "2023-06-15 03:57:16 INFO iteration 1005/3125: loss=1.304\n",
      "2023-06-15 03:57:17 INFO iteration 1008/3125: loss=1.745\n",
      "2023-06-15 03:57:18 INFO iteration 1011/3125: loss=1.398\n",
      "2023-06-15 03:57:20 INFO iteration 1014/3125: loss=1.709\n",
      "2023-06-15 03:57:21 INFO iteration 1017/3125: loss=1.890\n",
      "2023-06-15 03:57:23 INFO iteration 1020/3125: loss=1.517\n",
      "2023-06-15 03:57:24 INFO Running evals: eval_fast\n",
      "2023-06-15 03:57:30 INFO iteration 1023/3125: loss=1.507\n",
      "2023-06-15 03:57:31 INFO iteration 1026/3125: loss=1.509\n",
      "2023-06-15 03:57:33 INFO iteration 1029/3125: loss=1.801\n",
      "2023-06-15 03:57:34 INFO iteration 1032/3125: loss=1.622\n",
      "2023-06-15 03:57:35 INFO iteration 1035/3125: loss=1.582\n",
      "2023-06-15 03:57:37 INFO iteration 1038/3125: loss=1.377\n",
      "2023-06-15 03:57:38 INFO iteration 1041/3125: loss=1.603\n",
      "2023-06-15 03:57:39 INFO iteration 1044/3125: loss=1.740\n",
      "2023-06-15 03:57:41 INFO iteration 1047/3125: loss=1.484\n",
      "2023-06-15 03:57:42 INFO iteration 1050/3125: loss=1.559\n",
      "2023-06-15 03:57:43 INFO iteration 1053/3125: loss=1.434\n",
      "2023-06-15 03:57:44 INFO Running evals: eval_fast\n",
      "2023-06-15 03:57:50 INFO Running evals: eval_slow\n",
      "2023-06-15 03:57:57 INFO iteration 1056/3125: loss=1.448\n",
      "2023-06-15 03:57:58 INFO iteration 1059/3125: loss=1.465\n",
      "2023-06-15 03:57:59 INFO iteration 1062/3125: loss=1.422\n",
      "2023-06-15 03:58:01 INFO iteration 1065/3125: loss=1.569\n",
      "2023-06-15 03:58:02 INFO iteration 1068/3125: loss=1.513\n",
      "2023-06-15 03:58:03 INFO iteration 1071/3125: loss=1.559\n",
      "2023-06-15 03:58:05 INFO iteration 1074/3125: loss=1.428\n",
      "2023-06-15 03:58:06 INFO iteration 1077/3125: loss=1.614\n",
      "2023-06-15 03:58:07 INFO iteration 1080/3125: loss=1.425\n",
      "2023-06-15 03:58:09 INFO iteration 1083/3125: loss=1.780\n",
      "2023-06-15 03:58:10 INFO Running evals: eval_fast\n",
      "2023-06-15 03:58:16 INFO iteration 1086/3125: loss=1.288\n",
      "2023-06-15 03:58:17 INFO iteration 1089/3125: loss=1.485\n",
      "2023-06-15 03:58:19 INFO iteration 1092/3125: loss=1.608\n",
      "2023-06-15 03:58:19 INFO Saving model checkpoint to ../data/hallway-medium_2023-06-15-03-51-25/checkpoints/model.iter_1092.zanj\n",
      "2023-06-15 03:58:22 INFO iteration 1095/3125: loss=1.486\n",
      "2023-06-15 03:58:23 INFO iteration 1098/3125: loss=1.293\n",
      "2023-06-15 03:58:25 INFO iteration 1101/3125: loss=1.297\n",
      "2023-06-15 03:58:26 INFO iteration 1104/3125: loss=1.551\n",
      "2023-06-15 03:58:27 INFO iteration 1107/3125: loss=1.391\n",
      "2023-06-15 03:58:29 INFO iteration 1110/3125: loss=1.677\n",
      "2023-06-15 03:58:30 INFO iteration 1113/3125: loss=1.305\n",
      "2023-06-15 03:58:32 INFO Running evals: eval_fast\n",
      "2023-06-15 03:58:38 INFO Running evals: eval_slow\n",
      "2023-06-15 03:58:44 INFO iteration 1116/3125: loss=1.504\n",
      "2023-06-15 03:58:45 INFO iteration 1119/3125: loss=1.588\n",
      "2023-06-15 03:58:47 INFO iteration 1122/3125: loss=1.382\n",
      "2023-06-15 03:58:48 INFO iteration 1125/3125: loss=1.414\n",
      "2023-06-15 03:58:49 INFO iteration 1128/3125: loss=1.570\n",
      "2023-06-15 03:58:51 INFO iteration 1131/3125: loss=1.341\n",
      "2023-06-15 03:58:52 INFO iteration 1134/3125: loss=1.606\n",
      "2023-06-15 03:58:54 INFO iteration 1137/3125: loss=1.435\n",
      "2023-06-15 03:58:55 INFO iteration 1140/3125: loss=1.524\n",
      "2023-06-15 03:58:57 INFO iteration 1143/3125: loss=1.356\n",
      "2023-06-15 03:58:58 INFO iteration 1146/3125: loss=1.304\n",
      "2023-06-15 03:58:59 INFO Running evals: eval_fast\n",
      "2023-06-15 03:59:06 INFO iteration 1149/3125: loss=1.423\n",
      "2023-06-15 03:59:07 INFO iteration 1152/3125: loss=1.406\n",
      "2023-06-15 03:59:09 INFO iteration 1155/3125: loss=1.397\n",
      "2023-06-15 03:59:10 INFO iteration 1158/3125: loss=1.539\n",
      "2023-06-15 03:59:12 INFO iteration 1161/3125: loss=1.573\n",
      "2023-06-15 03:59:14 INFO iteration 1164/3125: loss=1.655\n",
      "2023-06-15 03:59:15 INFO iteration 1167/3125: loss=1.342\n",
      "2023-06-15 03:59:16 INFO iteration 1170/3125: loss=1.350\n",
      "2023-06-15 03:59:18 INFO iteration 1173/3125: loss=1.462\n",
      "2023-06-15 03:59:20 INFO iteration 1176/3125: loss=1.551\n",
      "2023-06-15 03:59:21 INFO Running evals: eval_fast\n",
      "2023-06-15 03:59:28 INFO Running evals: eval_slow\n",
      "2023-06-15 03:59:35 INFO iteration 1179/3125: loss=1.392\n",
      "2023-06-15 03:59:37 INFO iteration 1182/3125: loss=1.572\n",
      "2023-06-15 03:59:38 INFO iteration 1185/3125: loss=1.596\n",
      "2023-06-15 03:59:39 INFO iteration 1188/3125: loss=1.443\n",
      "2023-06-15 03:59:41 INFO iteration 1191/3125: loss=1.436\n",
      "2023-06-15 03:59:42 INFO iteration 1194/3125: loss=1.481\n",
      "2023-06-15 03:59:44 INFO iteration 1197/3125: loss=1.424\n",
      "2023-06-15 03:59:45 INFO iteration 1200/3125: loss=1.436\n",
      "2023-06-15 03:59:46 INFO iteration 1203/3125: loss=1.537\n",
      "2023-06-15 03:59:48 INFO iteration 1206/3125: loss=1.538\n",
      "2023-06-15 03:59:49 INFO Running evals: eval_fast\n",
      "2023-06-15 03:59:55 INFO iteration 1209/3125: loss=1.222\n",
      "2023-06-15 03:59:57 INFO iteration 1212/3125: loss=1.578\n",
      "2023-06-15 03:59:58 INFO iteration 1215/3125: loss=1.491\n",
      "2023-06-15 03:59:59 INFO iteration 1218/3125: loss=1.489\n",
      "2023-06-15 04:00:01 INFO iteration 1221/3125: loss=1.475\n",
      "2023-06-15 04:00:02 INFO iteration 1224/3125: loss=1.686\n",
      "2023-06-15 04:00:03 INFO iteration 1227/3125: loss=1.375\n",
      "2023-06-15 04:00:05 INFO iteration 1230/3125: loss=1.483\n",
      "2023-06-15 04:00:06 INFO iteration 1233/3125: loss=1.635\n",
      "2023-06-15 04:00:07 INFO iteration 1236/3125: loss=1.453\n",
      "2023-06-15 04:00:09 INFO iteration 1239/3125: loss=1.512\n",
      "2023-06-15 04:00:09 INFO Running evals: eval_fast\n",
      "2023-06-15 04:00:16 INFO Running evals: eval_slow\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result: TrainingResult \u001b[39m=\u001b[39m train_model(\n\u001b[0;32m      2\u001b[0m \tbase_path\u001b[39m=\u001b[39;49mPATH_DATA,\n\u001b[0;32m      3\u001b[0m     cfg\u001b[39m=\u001b[39;49mCFG,\n\u001b[0;32m      4\u001b[0m \twandb_project\u001b[39m=\u001b[39;49mWandbProject\u001b[39m.\u001b[39;49mDEMO_NOTEBOOKS, \u001b[39m# change this to WandbProject.DEMO_NOTEBOOKS! INTEGRATION_TESTS is for CI testing\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \tdo_generate_dataset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m      6\u001b[0m \tdataset_verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      7\u001b[0m )\n",
      "File \u001b[1;32mF:\\KNC\\maze-transformer\\maze_transformer\\training\\train_model.py:149\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(base_path, wandb_project, cfg, cfg_file, cfg_names, do_generate_dataset, dataset_verbose, device, help, **kwargs)\u001b[0m\n\u001b[0;32m    146\u001b[0m dataloader: DataLoader \u001b[39m=\u001b[39m get_dataloader(dataset, cfg, logger)\n\u001b[0;32m    148\u001b[0m logger\u001b[39m.\u001b[39mprogress(\u001b[39m\"\u001b[39m\u001b[39mfinished dataloader, passing to train()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 149\u001b[0m trained_model: ZanjHookedTransformer \u001b[39m=\u001b[39m train(\n\u001b[0;32m    150\u001b[0m     cfg\u001b[39m=\u001b[39;49mcfg,\n\u001b[0;32m    151\u001b[0m     dataloader\u001b[39m=\u001b[39;49mdataloader,\n\u001b[0;32m    152\u001b[0m     logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[0;32m    153\u001b[0m     output_dir\u001b[39m=\u001b[39;49moutput_path,\n\u001b[0;32m    154\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[0;32m    155\u001b[0m     val_dataset\u001b[39m=\u001b[39;49mval_dataset,\n\u001b[0;32m    156\u001b[0m )\n\u001b[0;32m    158\u001b[0m \u001b[39mreturn\u001b[39;00m TrainingResult(\n\u001b[0;32m    159\u001b[0m     output_path\u001b[39m=\u001b[39moutput_path,\n\u001b[0;32m    160\u001b[0m     model\u001b[39m=\u001b[39mtrained_model,\n\u001b[0;32m    161\u001b[0m )\n",
      "File \u001b[1;32mF:\\KNC\\maze-transformer\\maze_transformer\\training\\training.py:143\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(cfg, dataloader, logger, output_dir, device, val_dataset, zanj, model)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[39mif\u001b[39;00m iteration \u001b[39m%\u001b[39m intervals[interval_key] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    142\u001b[0m             logger\u001b[39m.\u001b[39mprogress(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRunning evals: \u001b[39m\u001b[39m{\u001b[39;00minterval_key\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 143\u001b[0m             scores: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, StatCounter] \u001b[39m=\u001b[39m evaluate_model(\n\u001b[0;32m    144\u001b[0m                 model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m    145\u001b[0m                 dataset\u001b[39m=\u001b[39;49mval_dataset,\n\u001b[0;32m    146\u001b[0m                 dataset_tokens\u001b[39m=\u001b[39;49mval_dataset_tokens,\n\u001b[0;32m    147\u001b[0m                 eval_functions\u001b[39m=\u001b[39;49mevals_dict,\n\u001b[0;32m    148\u001b[0m                 batch_size\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mtrain_cfg\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[0;32m    149\u001b[0m                 max_new_tokens\u001b[39m=\u001b[39;49mcfg\u001b[39m.\u001b[39;49mtrain_cfg\u001b[39m.\u001b[39;49mevals_max_new_tokens,\n\u001b[0;32m    150\u001b[0m             )\n\u001b[0;32m    151\u001b[0m             metrics\u001b[39m.\u001b[39mupdate(scores)\n\u001b[0;32m    152\u001b[0m logger\u001b[39m.\u001b[39mlog_metric_hist(metrics)\n",
      "File \u001b[1;32mF:\\KNC\\maze-transformer\\maze_transformer\\evaluation\\eval_model.py:220\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, dataset, dataset_tokens, eval_functions, max_new_tokens, batch_size, verbose)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m chunks(\u001b[39mzip\u001b[39m(dataset, dataset_tokens), batch_size):\n\u001b[0;32m    219\u001b[0m     maze_batch, tokens_batch \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)\n\u001b[1;32m--> 220\u001b[0m     predictions: \u001b[39mlist\u001b[39m[\u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mlist\u001b[39m[\u001b[39mtuple\u001b[39m[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m]]] \u001b[39m=\u001b[39m predict_maze_paths(\n\u001b[0;32m    221\u001b[0m         tokens_batch\u001b[39m=\u001b[39;49mtokens_batch,\n\u001b[0;32m    222\u001b[0m         data_cfg\u001b[39m=\u001b[39;49mdataset\u001b[39m.\u001b[39;49mcfg,\n\u001b[0;32m    223\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m    224\u001b[0m         max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens,\n\u001b[0;32m    225\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    226\u001b[0m     )\n\u001b[0;32m    228\u001b[0m     \u001b[39mfor\u001b[39;00m name, func \u001b[39min\u001b[39;00m eval_functions\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    229\u001b[0m         score_counters[name]\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m    230\u001b[0m             func(\n\u001b[0;32m    231\u001b[0m                 maze\u001b[39m=\u001b[39msolved_maze,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[39mfor\u001b[39;00m solved_maze, prediction \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(maze_batch, predictions)\n\u001b[0;32m    237\u001b[0m         )\n",
      "File \u001b[1;32mF:\\KNC\\maze-transformer\\maze_transformer\\evaluation\\eval_model.py:137\u001b[0m, in \u001b[0;36mpredict_maze_paths\u001b[1;34m(tokens_batch, data_cfg, model, max_new_tokens, verbose, when_noncoord, temperature)\u001b[0m\n\u001b[0;32m    135\u001b[0m context: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(get_context_tokens(tokens))\n\u001b[0;32m    136\u001b[0m \u001b[39m# predict tokens\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m prediction: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[0;32m    138\u001b[0m     context,\n\u001b[0;32m    139\u001b[0m     eos_token_id\u001b[39m=\u001b[39;49mdata_cfg\u001b[39m.\u001b[39;49mtokenizer_map[SPECIAL_TOKENS[\u001b[39m\"\u001b[39;49m\u001b[39mpath_end\u001b[39;49m\u001b[39m\"\u001b[39;49m]],\n\u001b[0;32m    140\u001b[0m     stop_at_eos\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    141\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens,\n\u001b[0;32m    142\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    143\u001b[0m     temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[0;32m    144\u001b[0m     \u001b[39m# use_past_kv_cache=False,\u001b[39;49;00m\n\u001b[0;32m    145\u001b[0m )\n\u001b[0;32m    146\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    147\u001b[0m     prediction, \u001b[39mstr\u001b[39m\n\u001b[0;32m    148\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprediction must be a string, got \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(prediction)\u001b[39m=}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mprediction\u001b[39m \u001b[39m\u001b[39m= }\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m \u001b[39m# convert to strings\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mivan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-transformer-2cGx2R0F-py3.10\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mivan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-transformer-2cGx2R0F-py3.10\\lib\\site-packages\\transformer_lens\\HookedTransformer.py:1325\u001b[0m, in \u001b[0;36mHookedTransformer.generate\u001b[1;34m(self, input, max_new_tokens, stop_at_eos, eos_token_id, do_sample, top_k, top_p, temperature, freq_penalty, num_return_sequences, use_past_kv_cache, prepend_bos, return_type, verbose)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[39mif\u001b[39;00m use_past_kv_cache:\n\u001b[0;32m   1323\u001b[0m     \u001b[39m# We just take the final tokens, as a [batch, 1] tensor\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1325\u001b[0m         logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\n\u001b[0;32m   1326\u001b[0m             tokens[:, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m:],\n\u001b[0;32m   1327\u001b[0m             return_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlogits\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1328\u001b[0m             past_kv_cache\u001b[39m=\u001b[39;49mpast_kv_cache,\n\u001b[0;32m   1329\u001b[0m         )\n\u001b[0;32m   1330\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1331\u001b[0m         logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\n\u001b[0;32m   1332\u001b[0m             tokens, return_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m, past_kv_cache\u001b[39m=\u001b[39mpast_kv_cache\n\u001b[0;32m   1333\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\mivan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-transformer-2cGx2R0F-py3.10\\lib\\site-packages\\transformer_lens\\HookedTransformer.py:325\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[1;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    321\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[0;32m    322\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[0;32m    323\u001b[0m         )\n\u001b[1;32m--> 325\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[0;32m    326\u001b[0m         residual,\n\u001b[0;32m    327\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[0;32m    328\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m    329\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[0;32m    330\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[0;32m    331\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    334\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[0;32m    335\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mivan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-transformer-2cGx2R0F-py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mivan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-transformer-2cGx2R0F-py3.10\\lib\\site-packages\\transformer_lens\\components.py:776\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mattn_only \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mparallel_attn_mlp:\n\u001b[0;32m    773\u001b[0m     resid_mid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_resid_mid(\n\u001b[0;32m    774\u001b[0m         resid_pre \u001b[39m+\u001b[39m attn_out\n\u001b[0;32m    775\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m--> 776\u001b[0m     normalized_resid_mid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln2(resid_mid)\n\u001b[0;32m    777\u001b[0m     mlp_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_mlp_out(\n\u001b[0;32m    778\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(normalized_resid_mid)\n\u001b[0;32m    779\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[0;32m    780\u001b[0m     resid_post \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_resid_post(\n\u001b[0;32m    781\u001b[0m         resid_mid \u001b[39m+\u001b[39m mlp_out\n\u001b[0;32m    782\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mivan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-transformer-2cGx2R0F-py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mivan\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\maze-transformer-2cGx2R0F-py3.10\\lib\\site-packages\\transformer_lens\\components.py:158\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    154\u001b[0m     \u001b[39mself\u001b[39m, x: Union[Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m], Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos head_index d_model\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m    155\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m], Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos head_index d_model\u001b[39m\u001b[39m\"\u001b[39m]]:\n\u001b[0;32m    156\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m-\u001b[39m x\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# [batch, pos, length]\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     scale: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos 1\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_scale(\n\u001b[1;32m--> 158\u001b[0m         (x\u001b[39m.\u001b[39;49mpow(\u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49mmean(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, keepdim\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\u001b[39m.\u001b[39;49msqrt()\n\u001b[0;32m    159\u001b[0m     )\n\u001b[0;32m    160\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m/\u001b[39m scale  \u001b[39m# [batch, pos, length]\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_normalized(x \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result: TrainingResult = train_model(\n",
    "\tbase_path=PATH_DATA,\n",
    "    cfg=CFG,\n",
    "\twandb_project=WandbProject.DEMO_NOTEBOOKS, # change this to WandbProject.DEMO_NOTEBOOKS! INTEGRATION_TESTS is for CI testing\n",
    "\tdo_generate_dataset=False,\n",
    "\tdataset_verbose=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maze-transformer",
   "language": "python",
   "name": "maze-transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
