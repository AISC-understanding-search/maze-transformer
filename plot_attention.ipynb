{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import OpenAIGPTLMHeadModel, OpenAIGPTConfig\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from muutils.logger import Logger, TimerContext\n",
    "from muutils.json_serialize import json_serialize, dataclass_serializer_factory\n",
    "from muutils.tensor_utils import ATensor, NDArray\n",
    "from muutils.statcounter import StatCounter\n",
    "from muutils.misc import shorten_numerical_to_str\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from maze_transformer.generation.latticemaze import LatticeMaze\n",
    "from maze_transformer.generation.generators import LatticeMazeGenerators\n",
    "from maze_transformer.training.tokenizer import MazeTokenizer, SPECIAL_TOKENS\n",
    "from maze_transformer.training.mazedataset import MazeDatasetConfig, MazeDataset\n",
    "from maze_transformer.evaluation.plot_maze import plot_multi_paths, PathFormat\n",
    "from maze_transformer.training.dataset import GPTDatasetConfig\n",
    "from maze_transformer.training.config import TrainConfig\n",
    "from maze_transformer.training.training import TRAIN_SAVE_FILES\n",
    "from maze_transformer.evaluation.eval_model import generate_plot_predicted_path, MazePath, load_model_with_configs, LoadedModelConfigs, predict_maze_path\n",
    "from maze_transformer.evaluation.pathdist import MazeEvalFunction, ArrMazeEvalFunction, MazeEvalFuncs, ArrMazeEvalFuncs\n",
    "from maze_transformer.evaluation.eval_model import decode_maze_tokens_to_coords, predict_tokens\n",
    "from maze_transformer.evaluation.plot_attention import colorize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and configs\n",
    "model_path: str = \"data/g4-n4K/g4-n4K_tiny-v1_2022-10-05-02-03-44/model.final.pt\"\n",
    "loaded_model_and_configs: LoadedModelConfigs = load_model_with_configs(model_path, MazeDatasetConfig)\n",
    "data_cfg: MazeDatasetConfig; train_cfg: TrainConfig\n",
    "model_cfg: OpenAIGPTConfig; model: OpenAIGPTLMHeadModel\n",
    "data_cfg, train_cfg, model_cfg, model = loaded_model_and_configs\n",
    "\n",
    "model.config.max_length = model_cfg.n_positions + 1\n",
    "model.config.output_attentions = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate a maze\n",
    "from maze_transformer.evaluation.eval_model import pad_sequence\n",
    "\n",
    "\n",
    "grid_n: int = data_cfg.grid_n\n",
    "maze: LatticeMaze = LatticeMazeGenerators.gen_dfs((grid_n, grid_n))\n",
    "c_start = (0, 0)\n",
    "c_end = (grid_n - 1, grid_n - 1)\n",
    "\n",
    "# solve the maze explicitly\n",
    "path_true = np.array(maze.find_shortest_path(\n",
    "\tc_start = c_start,\n",
    "\tc_end = c_end,\n",
    "))\n",
    "\n",
    "solved_maze: MazeTokenizer = MazeTokenizer(\n",
    "\tmaze=maze,\n",
    "\tsolution=np.array(maze.find_shortest_path(\n",
    "\t\tc_start=c_start,\n",
    "\t\tc_end=c_end,\n",
    "\t)),\n",
    ")\n",
    "\n",
    "# tokenize the maze\n",
    "maze_only_tokens: list[str] = solved_maze.as_tokens(data_cfg.node_token_map , solution = False) + [ SPECIAL_TOKENS[\"start_path\"] ]\n",
    "\n",
    "print(\"maze tokens:\", maze_only_tokens)\n",
    "\n",
    "array_nopad = torch.tensor(\n",
    "\t[ data_cfg.tokenizer_map[t] for t in maze_only_tokens ], \n",
    "\tdtype=torch.int32,\n",
    "\tdevice=\"cpu\",\n",
    ")\n",
    "\n",
    "array: ATensor = pad_sequence(array_nopad, model_cfg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have the model predict some tokens\n",
    "\n",
    "context_str: list[str] = maze_only_tokens[-model.config.n_positions:]\n",
    "context_tokenized: list[int] = array[-model.config.n_positions:]\n",
    "\n",
    "with torch.no_grad():\n",
    "\toutput = model(array[-model.config.n_positions:])\n",
    "\n",
    "print(model.config)\n",
    "\n",
    "print(f\"{output.logits.shape = }\\n{len(output.attentions) = }\\n{[x.shape for x in output.attentions] = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)\n",
    "\n",
    "# decode the tokens\n",
    "predicted_tokens = [ data_cfg.token_arr[t] for t in predictions[0] ]\n",
    "\n",
    "print(predicted_tokens)\n",
    "\n",
    "path_predicted: list[tuple[int,int]] = decode_maze_tokens_to_coords(\n",
    "\tpredicted_tokens[len(maze_only_tokens):],\n",
    "\tmazedata_cfg = data_cfg, \n",
    "\twhen_noncoord = \"skip\",\n",
    ")\n",
    "\n",
    "# plot the maze and both solutions\n",
    "# for label, fmt, color, path in paths\n",
    "plot_multi_paths(\n",
    "\tmaze = maze,\n",
    "\tpaths = [\n",
    "\t\tPathFormat(path_true, \"true\", \"-\", \"red\", {'width': 0.015}),\n",
    "\t\tPathFormat(np.array(path_predicted), \"predicted\", \":\", \"blue\", {}),\n",
    "\t],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "826cdf1d6a1d995932fcc4b02bd7049699ce423053098b308e34496c9b855014"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
